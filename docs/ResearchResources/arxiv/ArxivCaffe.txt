If there are any errors
please Abort, and run `arxiv_required` for required package installation, and start again
Please wait while we phrase the requested information from global arxiv[arxiv.org] servers 
------------>
---------------------------->
------------------------------------------------------>
 
DLA: Compiler and FPGA Overlay for Neural Network Inference Acceleration (Mohamed S. Abdelfattah - 13 July, 2018)
Additionally, we implement a sophisticated domain specific graph compiler that compiles deep learning languages such as Caffe or Tensorflow to easily target our overlay. We show how our graph compiler performs architecture-driven software optimizations to significantly boost performance of both convolutional and recurrent neural networks (CNNs/RNNs) - we demonstrate a 3x improvement on ResNet-101 and a 12x improvement for long short-term memory (LSTM) cells, compared to naive implementations. Finally, we describe how we can tailor our hardware overlay, and use our graph compiler to achieve ~900 fps on GoogLeNet on an Intel Arria 10 1150 - the fastest ever reported on comparable FPGAs.
Link: https://arxiv.org/abs/1807.06434
====================================================
Restructuring Batch Normalization to Accelerate CNN Training (Daejin Jung - 3 July, 2018)
The proposed solution can significantly reduce main-memory accesses while training the latest CNN models, and the experiments on a chip multiprocessor with our modified Caffe implementation show that the proposed BN restructuring can improve the performance of DenseNet with 121 convolutional layers by 28.4%.
Link: https://arxiv.org/abs/1807.01702
====================================================
Deploy Large-Scale Deep Neural Networks in Resource Constrained IoT Devices with Local Quantization Region (Yi Yang - 23 May, 2018)
We use models in Caffe model zoo as our example tasks to evaluate the effect of our low precision data representation scheme. For example, our 8-bit scheme has no drops on top-1 and top-5 accuracy with 2x speedup on Intel Edison IoT platform. For example, the drop on our task is only 0.7% when using 2-bit scheme, a scheme which could largely save transistors
Link: https://arxiv.org/abs/1805.09473
====================================================
A DAG Model of Synchronous Stochastic Gradient Descent in Distributed Deep Learning (Shaohuai Shi - 25 September, 2018)
To understand the practical impact of data communications on training performance, we conduct extensive empirical studies on four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet and TensorFlow) over multi-GPU and multi-node environments with different data communication techniques, including PCIe, NVLink, 10GbE, and InfiniBand
Link: https://arxiv.org/abs/1805.03812
====================================================
Î¼-cuDNN: Accelerating Deep Learning Frameworks with Micro-Batching (Yosuke Oyama - 13 April, 2018)
We demonstrate the effectiveness of Î¼-cuDNN over two frameworks, Caffe and TensorFlow, achieving speedups of 1.63x for AlexNet and 1.21x for ResNet-18 on P100-SXM2 GPU
Link: https://arxiv.org/abs/1804.04806
====================================================
Fine-Grained Energy and Performance Profiling framework for Deep Convolutional Neural Networks (Crefeda Faviola Rodrigues - 14 May, 2018)
However, current benchmarks studies in existing deep learning frameworks (for example, Caffe, Tensorflow, Torch and others) are based on performance of these applications on high-end CPUs and GPUs. In this work, we introduce a benchmarking framework called "SyNERGY" to measure the energy and time of 11 representative Deep Convolutional Neural Networks on embedded platforms such as NVidia Jetson TX1. In addition, we build an initial multi-variable linear regression model to predict energy consumption of unseen neural network models based on the number of SIMD instructions executed and main memory accesses of the CPU cores of the TX1 with an average relative test error rate of 8.04 +/- 5.96 %. Our predicted results demonstrate 7.08 +/- 6.0 % average relative error over actual energy measurements of all 11 networks tested, except MobileNet. By including MobileNet the average relative test error increases to 17.33 +/- 12.2 %.
Link: https://arxiv.org/abs/1803.11151
====================================================
SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks (Linnan Wang - 12 January, 2018)
Evaluations against Caffe, Torch, MXNet and TensorFlow have demonstrated that SuperNeurons trains at least 3.2432 deeper network than current ones with the leading performance
Link: https://arxiv.org/abs/1801.04380
====================================================
SparCE: Sparsity aware General Purpose Core Extensions to Accelerate Deep Neural Networks (Sanchari Sen - 29 November, 2017)
We model SparCE using the gem5 architectural simulator, and evaluate our approach on 6 image-recognition DNNs in the context of both training and inference using the Caffe framework. On a scalar microprocessor, SparCE achieves 19%-31% reduction in application-level. We also evaluate SparCE on a 4-way SIMD ARMv8 processor using the OpenBLAS library, and demonstrate that SparCE achieves 8%-15% reduction in the application-level execution time.
Link: https://arxiv.org/abs/1711.06315
====================================================
Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs (Shaohuai Shi - 20 August, 2018)
In this study, we evaluate the running performance of four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet, and TensorFlow) over single-GPU, multi-GPU, and multi-node environments
Link: https://arxiv.org/abs/1711.05979
====================================================
ADaPTION: Toolbox and Benchmark for Training Convolutional Neural Networks with Reduced Numerical Precision Weights and Activation (Moritz B. Milde - 13 November, 2017)
Here we present a method and present the ADaPTION toolbox to extend the popular deep learning library Caffe to support training of deep CNNs with reduced numerical precision of weights and activations using fixed point notation. Using the ADaPTION tools, we quantized several CNNs including VGG16 down to 16-bit weights and activations with only 0.8% drop in Top-1 accuracy. The quantization, especially of the activations, leads to increase of up to 50% of sparsity especially in early and intermediate layers, which we exploit to skip multiplications with zero, thus performing faster and computationally cheaper inference.
Link: https://arxiv.org/abs/1711.04713
====================================================
DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers (Akshay Sethi - 9 November, 2017)
The extracted computational graph is then converted into execution ready source code in both Keras and Caffe, in real-time. Experiments on the simulated dataset show that the proposed framework provide more than $93\%$ accuracy in flow diagram content extraction.
Link: https://arxiv.org/abs/1711.03543
====================================================
PowerAI DDL (Minsik Cho - 7 August, 2017)
This library has been integrated into Tensorflow, Caffe, and Torch. We train Resnet-101 on Imagenet 22K with 64 IBM Power8 S822LC servers (256 GPUs) in about 7 hours to an accuracy of 33.8 % validation accuracy. Microsoft's ADAM and Google's DistBelief results did not reach 30 % validation accuracy for Imagenet 22K. Compared to Facebook AI Research's recent paper on 256 GPU training, we use a different communication algorithm, and our combined software and hardware system offers better communication overhead for Resnet-50. A PowerAI DDL enabled version of Torch completed 90 epochs of training on Resnet 50 for 1K classes in 50 minutes using 64 IBM Power8 S822LC servers (256 GPUs).
Link: https://arxiv.org/abs/1708.02188
====================================================
Optimized Broadcast for Deep Learning Workloads on Dense-GPU InfiniBand Clusters: MPI or NCCL? (Ammar Ahmad Awan - 28 July, 2017)
This coupled with new application workloads brought forward by Deep Learning frameworks like Caffe and Microsoft CNTK pose additional design constraints due to very large message communication of GPU buffers during the training phase. In addition, the proposed designs provide up to 7% improvement over NCCL-based solutions for data parallel training of the VGG network on 128 GPUs using Microsoft CNTK.
Link: https://arxiv.org/abs/1707.09414
====================================================
Benchmarking Data Analysis and Machine Learning Applications on the Intel KNL Many-Core Processor (Chansup Byun - 11 July, 2017)
Also a performance comparison of a machine learning application, Caffe, between the two different Intel CPUs, Xeon E5 v3 and Xeon Phi 7210, demonstrated a 2.7x improvement on a KNL node.
Link: https://arxiv.org/abs/1707.03515
====================================================
Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters (Hao Zhang - 10 June, 2017)
We show that Poseidon enables Caffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even with limited bandwidth (10GbE) and the challenging VGG19-22K network for image classification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up with 32 single-GPU machines on Inception-V3, a 50% improvement over the open-source TensorFlow (20x speed-up).
Link: https://arxiv.org/abs/1706.03292
====================================================
Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox to Speed up Deep Neural Network Training (Martin Kuehn - 18 August, 2017)
To reduce training times to a bearable amount at reasonable cost we extend the popular Caffe toolbox for DNN with an efficient distributed memory communication pattern. To im- plement these communication patterns we rely on the the Global address space Programming Interface version 2 (GPI-2) communication library
Link: https://arxiv.org/abs/1706.00095
====================================================
User-transparent Distributed TensorFlow (Abhinav Vishnu - 14 April, 2017)
Several DL implementations -- primarily limited to a single compute node -- such as Caffe, TensorFlow, Theano and Torch have become readily available. Yet, the adoption of distributed DL implementations faces significant impediments: 1) most implementations require DL analysts to modify their code significantly -- which is a show-stopper, 2) several distributed DL implementations are geared towards cloud computing systems -- which is inadequate for execution on massively parallel systems such as supercomputers.
Link: https://arxiv.org/abs/1704.04560
====================================================
DyVEDeep: Dynamic Variable Effort Deep Neural Networks (Sanjay Ganapathy - 4 April, 2017)
Across all benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar operations, which translates to 1.8x-2.3x performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.
Link: https://arxiv.org/abs/1704.01137
====================================================
Using Deep Learning Method for Classification: A Proposed Algorithm for the ISIC 2017 Skin Lesion Classification Challenge (Wenhao Zhang - 10 March, 2017)
This algorithm is comprised of three steps: (1) original images preprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4] framework, (3) predicting the test images and calculating the scores that represent the likelihood of corresponding classification. The models are built on the source images are using the Caffe [4] framework
Link: https://arxiv.org/abs/1703.02182
====================================================
DR2-Net: Deep Residual Reconstruction Network for Image Compressive Sensing (Hantao Yao - 16 November, 2017)
The code of DR$^{2}$-Net has been released on: https://github.com/coldrainyht/caffe\_dr2
Link: https://arxiv.org/abs/1702.05743
====================================================
Supervised Learning Based Algorithm Selection for Deep Neural Networks (Shaohuai Shi - 16 March, 2017)
Our experimental results show that the revised Caffe can outperform the original one by an average of 28\%
Link: https://arxiv.org/abs/1702.03192
====================================================
An Implementation of Faster RCNN with Study for Region Sampling (Xinlei Chen - 8 February, 2017)
We adapted the join-training scheme of Faster RCNN framework from Caffe to TensorFlow as a baseline implementation for object detection. This report documents the simplifications made to the original pipeline, with justifications from ablation analysis on both PASCAL VOC 2007 and COCO 2014
Link: https://arxiv.org/abs/1702.02138
====================================================
ImageNet pre-trained models with batch normalization (Marcel Simon - 6 December, 2016)
In this paper, we present a new set of pre-trained models with popular state-of-the-art architectures for the Caffe framework
Link: https://arxiv.org/abs/1612.01452
====================================================
Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks (Roberto DiCecco - 30 September, 2016)
In this work we present a modified version of the popular CNN framework Caffe, with FPGA support. The results show that our framework achieves 50 GFLOPS across 3x3 convolutions in the benchmarks
Link: https://arxiv.org/abs/1609.09671
====================================================
Benchmarking State-of-the-Art Deep Learning Software Tools (Shaohuai Shi - 17 February, 2017)
In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch
Link: https://arxiv.org/abs/1608.07249
====================================================
Deep Learning for Saliency Prediction in Natural Video (Souad Chaabouni - 27 April, 2016)
We design the deep architecture on the basis of CaffeNet implemented with Caffe toolkit. We show that changing the way of data selection for optimisation of network parameters, we can save computation cost up to 12 times. The first is IRCCYN video database containing 31 videos with an overall amount of 7300 frames and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided 2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the accuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of saliency of patches show the improvement up to 2% with regard to RGB use only. The resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of predicted saliency maps with visual fixation maps shows the increase up to 16% on a sample of video clips from this dataset.
Link: https://arxiv.org/abs/1604.08010
====================================================
Convolutional Architecture Exploration for Action Recognition and Image Classification (J. T. Turner - 23 December, 2015)
Convolutional Architecture for Fast Feature Encoding (CAFFE) [11] is a software package for the training, classifying, and feature extraction of images. The UCF Sports Action dataset is a widely used machine learning dataset that has 200 videos taken in 720x480 resolution of 9 different sporting activities: diving, golf, swinging, kicking, lifting, horseback riding, running, skateboarding, swinging (various gymnastics), and walking
Link: https://arxiv.org/abs/1512.07502
====================================================
Towards Open Set Deep Networks (Abhijit Bendale - 19 November, 2015)
We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images
Link: https://arxiv.org/abs/1511.06233
====================================================
Exemplar Based Deep Discriminative and Shareable Feature Learning for Scene Image Classification (Zhen Zuo - 21 August, 2015)
Based on the experimental results, DDSFL can achieve very promising performance, and it also shows great complementary effect to the state-of-the-art Caffe features.
Link: https://arxiv.org/abs/1508.05306
====================================================
Caffe con Troll: Shallow Ideas to Speed Up Deep Learning (Stefan Hadjis - 26 May, 2015)
We find that, by employing standard batching optimizations for CPU training, we achieve a 4.5x throughput improvement over Caffe on popular networks like CaffeNet
Link: https://arxiv.org/abs/1504.04343
====================================================
Theano-based Large-Scale Visual Recognition with Multiple GPUs (Weiguang Ding - 6 April, 2015)
Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU
Link: https://arxiv.org/abs/1412.2302
====================================================
DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks (Tao Chen - 30 October, 2014)
Our deep CNNs model is trained based on Caffe, a newly developed deep learning framework. Performance evaluation shows the newly trained deep CNNs model SentiBank 2.0 (or called DeepSentiBank) is significantly improved in both annotation accuracy and retrieval performance, compared to its predecessors which mainly use binary SVM classification models.
Link: https://arxiv.org/abs/1410.8586
====================================================
cuDNN: Efficient Primitives for Deep Learning (Sharan Chetlur - 17 December, 2014)
For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.
Link: https://arxiv.org/abs/1410.0759
====================================================
Caffe: Convolutional Architecture for Fast Feature Embedding (Yangqing Jia - 20 June, 2014)
Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\approx$ 2.5 ms per image)
Link: https://arxiv.org/abs/1408.5093
====================================================
MyCaffe: A Complete C# Re-Write of Caffe with Reinforcement Learning (David W. Brown - 4 October, 2018)
This article describes the general architecture of MyCaffe including the newly added MyCaffeTrainerRL for Reinforcement Learning. In addition, this article discusses how MyCaffe closely follows the C++ Caffe, while talking efficiently to the low level NVIDIA CUDA hardware to offer a high performance, highly programmable deep learning system for Windows .NET programmers.
Link: https://arxiv.org/abs/1810.02272
====================================================
Generalized Capsule Networks with Trainable Routing Procedure (Zhenhua Chen - 27 August, 2018)
The code is shared on \hyperlink{https://github.com/chenzhenhua986/CAFFE-CapsNet}{CAFFE-CapsNet}.
Link: https://arxiv.org/abs/1808.08692
====================================================
Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe (Jiong Gong - 4 May, 2018)
We show that the inference throughput and latency with ResNet-50, Inception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with neglectable accuracy loss from IntelCaffe FP32 baseline and by 56X-75X and 26X-37X from BVLC Caffe. All these techniques have been open-sourced on IntelCaffe GitHub1, and the artifact is provided to reproduce the result on Amazon AWS Cloud.
Link: https://arxiv.org/abs/1805.08691
====================================================
Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions (Zheng Qin - 27 March, 2018)
Evaluation results show that our proposed method gains $15.4\times$ training speedup on Darknet, $8.4\times$ on Caffe, $5.4\times$ on PyTorch, $3.5\times$ on MXNet, and $1.4\times$ on TensorFlow, compared to their original implementations of depthwise convolutions.
Link: https://arxiv.org/abs/1803.09926
====================================================
CuLDA_CGS: Solving Large-scale LDA Problems on GPUs (Xiaolong Xie - 13 March, 2018)
Ten- sorFlow, Caffe, CNTK, support to use GPUs for accelerating the popular machine learning data-intensive algorithms
Link: https://arxiv.org/abs/1803.04631
====================================================
Barista - a Graphical Tool for Designing and Training Deep Neural Networks (Soeren Klemm - 13 February, 2018)
In this paper, we introduce Barista, an open-source graphical high-level interface for the Caffe deep learning framework. While Caffe is one of the most popular frameworks for training DNNs, editing prototext files in order to specify the net architecture and hyper parameters can become a cumbersome and error-prone task
Link: https://arxiv.org/abs/1802.04626
====================================================
A Systematic Analysis for State-of-the-Art 3D Lung Nodule Proposals Generation (Hui Wu - 8 January, 2018)
We implement the 3D CNN model on CPU platform and propose an Intel Extended-Caffe framework which supports many highly-efficient 3D computations, which is opened source at https://github.com/extendedcaffe/extended-caffe.
Link: https://arxiv.org/abs/1802.02179
====================================================
Transfer learning for diagnosis of congenital abnormalities of the kidney and urinary tract in children based on Ultrasound imaging data (Qiang Zheng - 30 December, 2017)
Particularly, a pre-trained deep learning model (imagenet-caffe-alex) is adopted for transfer learning-based feature extraction from 3-channel feature maps computed from US images, including original images, gradient features, and distanced transform features
Link: https://arxiv.org/abs/1801.00224
====================================================
Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning Applications (Qixue Xiao - 21 December, 2017)
To allow deep learning applications to handle a wide range of input data, popular frameworks, such as Caffe, TensorFlow, and Torch, all provide data scaling functions to resize input to the dimensions used by deep learning models
Link: https://arxiv.org/abs/1712.07805
====================================================
Security Risks in Deep Learning Implementations (Qixue Xiao - 29 November, 2017)
This paper discloses a set of vulnerabilities in popular deep learning frameworks including Caffe, TensorFlow, and Torch
Link: https://arxiv.org/abs/1711.11008
====================================================
Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks (Ziming Zhang - 20 November, 2017)
In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.
Link: https://arxiv.org/abs/1711.07354
====================================================
Feed Forward and Backward Run in Deep Convolution Neural Network (Pushparaja Murugan - 9 November, 2017)
This has led to the major development in Deep learning frameworks such as Tensorflow, caffe, keras, theno
Link: https://arxiv.org/abs/1711.03278
====================================================
IBM Deep Learning Service (Bishwaranjan Bhattacharjee - 18 September, 2017)
DLaaS provides developers the flexibility to use popular deep learning libraries such as Caffe, Torch and TensorFlow, in the cloud in a scalable and resilient manner with minimal effort
Link: https://arxiv.org/abs/1709.05871
====================================================
What does fault tolerant Deep Learning need from MPI? (Vinay Amatya - 11 September, 2017)
We leverage a distributed memory implementation of Caffe, currently available under the Machine Learning Toolkit for Extreme Scale (MaTEx). We implement our approaches by ex- tending MaTEx-Caffe for using ULFM-based implementation
Link: https://arxiv.org/abs/1709.03316
====================================================
DARVIZ: Deep Abstract Representation, Visualization, and Verification of Deep Learning Models (Anush Sankaran - 16 August, 2017)
Further, for deep learning development there are many libraries in multiple programming languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch (Lua), and Deeplearning4j (Java), driving a huge need for interoperability across libraries.
Link: https://arxiv.org/abs/1708.04915
====================================================
Fast Learning and Prediction for Object Detection using Whitened CNN Features (BjÃ¶rn Barz - 12 April, 2017)
The Adaptive Real-Time Object Detection System (ARTOS) has been refactored broadly to be used in combination with Caffe for the experimental studies reported in this work.
Link: https://arxiv.org/abs/1704.02930
====================================================
Coordinating Filters for Faster Deep Neural Networks (Wei Wen - 25 July, 2017)
Source code is available in https://github.com/wenwei202/caffe
Link: https://arxiv.org/abs/1703.09746
====================================================
A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe (Volodymyr Turchenko - 18 January, 2017)
This paper presents the development of several models of a deep convolutional auto-encoder in the Caffe deep learning framework and their experimental evaluation on the example of MNIST dataset. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework
Link: https://arxiv.org/abs/1701.04949
====================================================
DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning (Tian Zhao - 9 January, 2017)
The state-of-the art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment
Link: https://arxiv.org/abs/1701.02284
====================================================
Deep Convolutional Neural Network Design Patterns (Leslie N. Smith - 14 November, 2016)
In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns)
Link: https://arxiv.org/abs/1611.00847
====================================================
A Tour of TensorFlow (Peter Goldsborough - 1 October, 2016)
We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.
Link: https://arxiv.org/abs/1610.01178
====================================================
Reducing Drift in Visual Odometry by Inferring Sun Direction Using a Bayesian Convolutional Neural Network (Valentin Peretroukhin - 27 July, 2017)
An open source implementation of our Bayesian CNN sun estimator (Sun-BCNN) using Caffe is available at https://github
Link: https://arxiv.org/abs/1609.05993
====================================================
UnrealCV: Connecting Computer Vision to Unreal Engine (Weichao Qiu - 5 September, 2016)
We show two applications: (i) a proof of concept image dataset, and (ii) linking Caffe with the virtual world to test deep network algorithms.
Link: https://arxiv.org/abs/1609.01326
====================================================
Learning Structured Sparsity in Deep Neural Networks (Wei Wen - 18 October, 2016)
Open source code is in https://github.com/wenwei202/caffe/tree/scnn
Link: https://arxiv.org/abs/1608.03665
====================================================
DeepLearningKit - an GPU Optimized Deep Learning Framework for Apple's iOS, OS X and tvOS developed in Metal and Swift (Amund Tveit - 15 May, 2016)
The goal is to support using deep learning models trained with popular frameworks such as Caffe, Torch, TensorFlow, Theano, Pylearn, Deeplearning4J and Mocha
Link: https://arxiv.org/abs/1605.04614
====================================================
DeepSpark: A Spark-Based Distributed Deep Learning Framework for Commodity Clusters (Hanjoo Kim - 30 September, 2016)
To support parallel operations, DeepSpark automatically distributes workloads and parameters to Caffe/Tensorflow-running nodes using Spark, and iteratively aggregates training results by a novel lock-free asynchronous variant of the popular elastic averaging stochastic gradient descent based update scheme, effectively complementing the synchronized processing capabilities of Spark
Link: https://arxiv.org/abs/1602.08191
====================================================
The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection (Pascal Mettes - 23 February, 2016)
The reorganized hierarchies and their derived Caffe models are publicly available at http://tinyurl.com/imagenetshuffle.
Link: https://arxiv.org/abs/1602.07119
====================================================
Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines (Hao Zhang - 19 December, 2015)
A number of frameworks have been developed to expedite the process of designing and training deep neural networks (DNNs), such as Caffe, Torch and Theano
Link: https://arxiv.org/abs/1512.06216
====================================================
SSD: Single Shot MultiBox Detector (Wei Liu - 29 December, 2016)
Code is available at https://github.com/weiliu89/caffe/tree/ssd .
Link: https://arxiv.org/abs/1512.02325
====================================================
Creation of a Deep Convolutional Auto-Encoder in Caffe (Volodymyr Turchenko - 21 April, 2016)
The development of a deep (stacked) convolutional auto-encoder in the Caffe deep learning framework is presented in this paper. We describe simple principles which we used to create this model in Caffe
Link: https://arxiv.org/abs/1512.01596
====================================================
Design of Kernels in Convolutional Neural Networks for Image Classification (Zhun Sun - 28 November, 2016)
The code is available in github.com/minogame/caffe-qhconv.
Link: https://arxiv.org/abs/1511.09231
====================================================
Comparative Study of Deep Learning Software Frameworks (Soheil Bahrampour - 29 March, 2016)
This paper presents a comparative study of five deep learning frameworks, namely Caffe, Neon, TensorFlow, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. Caffe is the easiest for evaluating the performance of standard deep architectures
Link: https://arxiv.org/abs/1511.06435
====================================================
SparkNet: Training Deep Networks in Spark (Philipp Moritz - 28 February, 2016)
Our implementation includes a convenient interface for reading data from Spark RDDs, a Scala interface to the Caffe deep learning framework, and a lightweight multi-dimensional tensor library. Furthermore, it is easy to deploy and use with no parameter tuning, and it is compatible with existing Caffe models
Link: https://arxiv.org/abs/1511.06051
====================================================
Learning Dense Convolutional Embeddings for Semantic Segmentation (Adam W. Harley - 7 January, 2016)
Our contributions are integrated in the popular Caffe deep learning framework, and consist in straightforward modifications to convolution routines
Link: https://arxiv.org/abs/1511.04377
====================================================
Places205-VGGNet Models for Scene Recognition (Limin Wang - 7 August, 2015)
Specifically, we train three VGGNet models, namely VGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe toolbox with high computational efficiency. Our trained models achieve the state-of-the-art performance on these datasets and are made public available.
Link: https://arxiv.org/abs/1508.01667
====================================================
ParseNet: Looking Wider to See Better (Wei Liu - 19 November, 2015)
Code is available at https://github.com/weiliu89/caffe/tree/fcn .
Link: https://arxiv.org/abs/1506.04579
====================================================
Expresso : A user-friendly GUI for Designing, Training and Exploring Convolutional Neural Networks (Ravi Kiran Sarvadevabhatla - 18 October, 2015)
Expresso is built atop Caffe, the open-source, prize-winning framework popularly used to develop Convolutional Neural Networks
Link: https://arxiv.org/abs/1505.06605
====================================================
U-Net: Convolutional Networks for Biomedical Image Segmentation (Olaf Ronneberger - 18 May, 2015)
The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
Link: https://arxiv.org/abs/1505.04597
====================================================
Fast R-CNN (Ross Girshick - 27 September, 2015)
Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.
Link: https://arxiv.org/abs/1504.08083
====================================================
A continuum-mechanical model for the flow of anisotropic polar ice (Ralf Greve - 24 March, 2010)
The CAFFE model fulfills all the fundamental principles of classical continuum mechanics, is sufficiently simple to allow numerical implementations in ice-flow models and contains only a limited number of free parameters. The applicability of the CAFFE model is demonstrated by a case study for the site of the EPICA (European Project for Ice Coring in Antarctica) ice core in Dronning Maud Land, East Antarctica.
Link: https://arxiv.org/abs/0903.3078
====================================================
Continuum-mechanical, Anisotropic Flow model for polar ice masses, based on an anisotropic Flow Enhancement factor (Luca Placidi - 18 March, 2010)
The CAFFE model is an application of the theory of mixtures with continuous diversity for the case of large polar ice masses in which induced anisotropy occurs. It is proven that the flow law of the CAFFE model is truly anisotropic despite the collinearity between the stress deviator and stretching tensors.
Link: https://arxiv.org/abs/0903.0688
====================================================
