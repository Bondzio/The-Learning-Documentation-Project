If there are any errors
please Abort, and run `arxiv_required` for required package installation, and start again
Please wait while we phrase the requested information from global arxiv[arxiv.org] servers 
------------>
---------------------------->
------------------------------------------------------>
 
Triple Attention Mixed Link Network for Single Image Super Resolution (Xi Cheng - 7 October, 2018)
In this work, to significantly enhance the feature representation, we proposed Triple Attention mixed link Network (TAN) which consists of 1) three different aspects (i.e., kernel, spatial and channel) of attention mechanisms and 2) fu-sion of both powerful residual and dense connections (i.e., mixed link)
Link: https://arxiv.org/abs/1810.03254
====================================================
Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing (Jayashree Mohan - 5 October, 2018)
Our tools also revealed 10 new crash-consistency bugs in widely-used, mature Linux file systems, seven of which existed in the kernel since 2014
Link: https://arxiv.org/abs/1810.02904
====================================================
Towards Fast and Energy-Efficient Binarized Neural Network Inference on FPGA (Cheng Fu - 4 October, 2018)
By analyzing local properties of images and the learned BNN kernel weights, we observe an average of $\sim$78% input similarity and $\sim$59% weight similarity among weight kernels, measured by our proposed metric in common network architectures
Link: https://arxiv.org/abs/1810.02068
====================================================
Performance Evaluation of SIFT Descriptor against Common Image Deformations on Iban Plaited Mat Motifs (Silvia Joseph - 2 October, 2018)
However, it did not performed well with Image blur test sequences with an average of 1.61 percents retained pairwise matching after blurring with a Gaussian kernel of 8.0 radius.
Link: https://arxiv.org/abs/1810.01562
====================================================
A deterministic polynomial kernel for Odd Cycle Transversal and Vertex Multiway Cut in planar graphs (Bart M. P. Jansen - 2 October, 2018)
We show that Odd Cycle Transversal and Vertex Multiway Cut admit deterministic polynomial kernels when restricted to planar graphs and parameterized by the solution size. On the way to these results, we provide an efficient sparsification routine in the flavor of the sparsification routine used for the Steiner Tree problem in planar graphs (FOCS 2014)
Link: https://arxiv.org/abs/1810.01136
====================================================
GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration (Jacob R. Gardner - 1 October, 2018)
Adapting this algorithm to complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In experiments we show that BBMM effectively uses GPU acceleration - exact GP inference and scalable approximations are up to 20 times faster than existing methods
Link: https://arxiv.org/abs/1809.11165
====================================================
Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS (Negin Hayatbini - 27 September, 2018)
A varying scale-kernel is implemented to reduce the sensitivity of image segmentation to noise and capture objects with various finenesses of the edges in remote-sensing images. Evaluation of event-based images indicates that the proposed algorithm has potential to improve rain detection and estimation skills with an average of more than 45% gain comparing to the segmentation technique used in PERSIANN-CCS and identifying cloud regions as objects with accuracy rates up to 98%.
Link: https://arxiv.org/abs/1809.10801
====================================================
Cursive Scene Text Analysis by Deep Convolutional Linear Pyramids (Saad Bin Ahmed - 27 September, 2018)
Each pyramid image process through various empirically selected kernels. The error rate of 0.17% was reported on Arabic scene text recognition.
Link: https://arxiv.org/abs/1809.10792
====================================================
Consistency and Variation in Kernel Neural Ranking Model (Mary Arpita Pyreddy - 27 September, 2018)
This paper studies the consistency of the kernel-based neural ranking model K-NRM, a recent state-of-the-art neural IR model, which is important for reproducible research and deployment in the industry
Link: https://arxiv.org/abs/1809.10522
====================================================
AlSub: Fully Parallel Subdivision for Modeling and Rendering (Daniel Mlakar - 1 October, 2018)
To further increase performance, we automatically identify critical matrix operations and replace them by specialized, heavily tuned GPU kernels. To substantiate the versatility of our approach we apply it to $\sqrt{3}$, Loop and Catmull-Clark subdivision schemes and show support for adaptive subdivision, semi-sharp creases, and a split evaluation scheme that separates topology and topological changes from positional updates
Link: https://arxiv.org/abs/1809.06047
====================================================
Multi-Kernel Diffusion CNNs for Graph-Based Learning on Point Clouds (Lasse Hansen - 14 September, 2018)
We validated our approach for learning point descriptors as well as semantic classification on real 3D point clouds of human poses and demonstrate an improvement from 85% to 95% in Dice overlap with our multi-kernel approach.
Link: https://arxiv.org/abs/1809.05370
====================================================
Random Warping Series: A Random Features Method for Time-Series Embedding (Lingfei Wu - 14 September, 2018)
The proposed kernel does not suffer from the issue of diagonal dominance while naturally enjoys a \emph{Random Features} (RF) approximation, which reduces the computational complexity of existing DTW-based techniques from quadratic to linear in terms of both the number and the length of time-series. Our extensive experiments on 16 benchmark datasets demonstrate that RWS outperforms or matches state-of-the-art classification and clustering methods in both accuracy and computational time
Link: https://arxiv.org/abs/1809.05259
====================================================
Normalization in Training Deep Convolutional Neural Networks for 2D Bio-medical Semantic Segmentation (Xiao-Yun Zhou - 11 September, 2018)
One common issue in training DCNN is the internal covariate shift, where the convolutional kernels are trained to fit the distribution change of input feature, hence both the training speed and performance are decreased. 37 RVs from both asymptomatic and Hypertrophic Cardiomyopathy (HCM) subjects and 20 aortas from asymptomatic subjects were used for the validation
Link: https://arxiv.org/abs/1809.03783
====================================================
Sparse Kernel PCA for Outlier Detection (Rudrajit Das - 13 September, 2018)
We consider outlier detection (where KPCA is employed) as an application for SKPCA, using the RBF kernel. We test it on 5 real-world datasets and show that by using just 4% (or even less) of the principal components (PCs), where each PC has on average less than 12% non-zero elements in the worst case among all 5 datasets, we are able to nearly match and in 3 datasets even outperform KPCA
Link: https://arxiv.org/abs/1809.02497
====================================================
Geometric Operator Convolutional Neural Network (Yangling Ma - 4 September, 2018)
Although the geometric operator convolution kernels have fewer trainable parameters than common convolution kernels, the experimental results indicate that GO-CNN performs more accurately than common CNN on CIFAR-10/100. In the practical task of medically diagnosing bone fractures, GO-CNN obtains 3% improvement in terms of the recall.
Link: https://arxiv.org/abs/1809.01016
====================================================
A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation (Madhu Babu V - 10 September, 2018)
These losses are defined using bi-linear sampling kernel and penalized using the Charbonnier penalty function. Another novelty lies in the fact that we combine a disparity-based depth estimation network with a pose estimation network to obtain absolute scale-aware 6 DOF Camera pose and superior depth map
Link: https://arxiv.org/abs/1809.00969
====================================================
Automated Instruction Stream Throughput Prediction for Intel and AMD Microarchitectures (Jan Laukemann - 10 October, 2018)
An accurate prediction of scheduling and execution of instruction streams is a necessary prerequisite for predicting the in-core performance behavior of throughput-bound loop kernels on out-of-order processor architectures. We present the Open Source Architecture Code Analyzer (OSACA), a static analysis tool for predicting the execution time of sequential loops comprising x86 instructions under the assumption of an infinite first-level cache and perfect out-of-order scheduling
Link: https://arxiv.org/abs/1809.00912
====================================================
Deductive Verification of Unmodified Linux Kernel Library Functions (Denis Efremov - 3 September, 2018)
This paper presents results from the development and evaluation of a deductive verification benchmark consisting of 26 unmodified Linux kernel library functions implementing conventional memory and string operations. The correctness of 23 functions was completely proved using AstraVer toolset, although success for 11 functions was achieved using 2 new specification language constructs. Another 2 functions were proved after a minor modification of their source code, while the final one cannot be completely proved using the existing memory model
Link: https://arxiv.org/abs/1809.00626
====================================================
On overcoming the Curse of Dimensionality in Neural Networks (Karen Yeressian - 10 September, 2018)
Let $H$ be a reproducing Kernel Hilbert space. Let $f^*\in H$ be the unique global minimiser of the functional \begin{equation*} J(f) = \frac{1}{2}\Vert f\Vert_{H}^{2} + \frac{1}{N}\sum_{i=1}^{N}\frac{1}{2}\vert f(x_i)-y_i\vert^{2}
Link: https://arxiv.org/abs/1809.00368
====================================================
Autonomous Configuration of Network Parameters in Operating Systems using Evolutionary Algorithms (Bartosz Gembala - 31 August, 2018)
This can be done for a standard Linux kernel using sysctl or /proc. Different scenarios for network parameter configurations are thoroughly tested, and an increase of up to 65% throughput speed is achieved compared to the default Linux configuration.
Link: https://arxiv.org/abs/1808.10733
====================================================
Hashing-Based-Estimators for Kernel Density in High Dimensions (Moses Charikar - 30 August, 2018)
Given a set of points $P\subset \mathbb{R}^{d}$ and a kernel $k$, the Kernel Density Estimate at a point $x\in\mathbb{R}^{d}$ is defined as $\mathrm{KDE}_{P}(x)=\frac{1}{|P|}\sum_{y\in P} k(x,y)$
Link: https://arxiv.org/abs/1808.10530
====================================================
Autoencoders, Kernels, and Multilayer Perceptrons for Electron Micrograph Restoration and Compression (Jeffrey M. Ede - 29 August, 2018)
We present 14 autoencoders, 15 kernels and 14 multilayer perceptrons for electron micrograph restoration and compression. TEM autoencoders have been trained for 1$\times$, 4$\times$, 16$\times$ and 64$\times$ compression, STEM autoencoders for 1$\times$, 4$\times$ and 16$\times$ compression and TEM+STEM autoencoders for 1$\times$, 2$\times$, 4$\times$, 8$\times$, 16$\times$, 32$\times$ and 64$\times$ compression. Kernels and multilayer perceptrons have been trained to approximate the denoising effect of the 4$\times$ compression autoencoders. Kernels for input sizes of 3, 5, 7, 11 and 15 have been fitted for TEM, STEM and TEM+STEM. TEM multilayer perceptrons have been trained with 1 hidden layer for input sizes of 3, 5 and 7 and with 2 hidden layers for input sizes of 5 and 7. STEM multilayer perceptrons have been trained with 1 hidden layer for input sizes of 3, 5 and 7. TEM+STEM multilayer perceptrons have been trained with 1 hidden layer for input sizes of 3, 5, 7 and 11 and with 2 hidden layers for input sizes of 3 and 7
Link: https://arxiv.org/abs/1808.09916
====================================================
Enumerating Top-k Quasi-Cliques (Seyed-Vahid Sanei-Mehri - 28 August, 2018)
Our method is based on identifying kernels of extremely dense subgraphs within a graph, following by growing subgraphs around these kernels, to arrive at quasi-cliques with the required densities (3) Experimental results show that our algorithm accurately enumerates quasi-cliques from a graph, is much faster than current state-of-the-art methods for quasi-clique enumeration (often more than three orders of magnitude faster), and can scale to larger graphs than current methods.
Link: https://arxiv.org/abs/1808.09531
====================================================
Removing out-of-focus blur from a single image (Guodong Xu - 28 August, 2018)
In this paper, we proposed a blind deconvolution method specifically designed for removing defocus blurring from an image, by providing effective solutions to two critical problems: 1) suppressing the artifacts caused by segmentation error by introducing an additional variable regularized by weighted $\ell_0$-norm; and 2) more accurate defocus kernel estimation using non-parametric symmetry and low-rank based constraints on the kernel
Link: https://arxiv.org/abs/1808.09166
====================================================
Rain Streak Removal for Single Image via Kernel Guided CNN (Ye-Tao Wang - 28 August, 2018)
In this paper, we propose a novel rain streak removal approach using a kernel guided convolutional neural network (KGCNN), achieving the state-of-the-art performance with simple network architectures
Link: https://arxiv.org/abs/1808.08545
====================================================
Nowcasting the Stance of Social Media Users in a Sudden Vote: The Case of the Greek Referendum (Adam Tsakalidis - 26 August, 2018)
We propose a semi-supervised multiple convolution kernel learning approach, leveraging temporally sensitive text and network information. Our evaluation under a real-time simulation framework demonstrates the effectiveness and robustness of our approach against competitive baselines, achieving a significant 20% increase in F-score compared to solely text-based models.
Link: https://arxiv.org/abs/1808.08538
====================================================
Improving the results of string kernels in sentiment analysis and Arabic dialect identification by adapting them to your test set (Radu Tudor Ionescu - 31 August, 2018)
Recently, string kernels have obtained state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification
Link: https://arxiv.org/abs/1808.08409
====================================================
Implementing Strassen's Algorithm with CUTLASS on NVIDIA Volta GPUs (Jianyu Huang - 23 August, 2018)
We further exploit intra- and inter-kernel parallelism by batching, streaming, and employing atomic operations. Overall, our 1-level Strassen can achieve up to 1.11x speedup with a crossover point as small as 1,536 compared to cublasSgemm on a NVIDIA Tesla V100 GPU. With additional workspace, our 2-level Strassen can achieve 1.19x speedup with a crossover point at 7,680.
Link: https://arxiv.org/abs/1808.07984
====================================================
Left ventricle quantification through spatio-temporal CNNs (Alejandro Debus - 23 August, 2018)
Instead of analysing slices independently, we process stacks of temporally adjacent slices by means of 3D convolutional kernels which fuse the spatio-temporal information, incorporating the temporal dynamics of the heart to the learned model. We show that incorporating such information by means of spatio-temporal convolutions into standard LV quantification architectures improves the accuracy of the predictions when compared with single-slice models, achieving competitive results for all cardiac indices and significantly breaking the state of the art (Xue et al., 2018, MedIA) for cardiac phase estimation.
Link: https://arxiv.org/abs/1808.07967
====================================================
Towards Fine Grained Network Flow Prediction (Patrick Jahnke - 20 August, 2018)
Our FKKF relies on the well known Kalman Filter in combination with a kernel to support the prediction of non linear functions. Our approach predicts traffic on average across 17 out of 20 groups of flows with an average prediction error of 6.43% around 0.49 (average) seconds in advance, whilst existing coarse grained approaches exhibit prediction errors of 77% at best.
Link: https://arxiv.org/abs/1808.06453
====================================================
Anatomy Of High-Performance Deep Learning Convolutions On SIMD Architectures (Evangelos Georganas - 20 August, 2018)
In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and XeonPhi systems, which are implemented via a dynamic compilation approach
Link: https://arxiv.org/abs/1808.05567
====================================================
libhclooc: Software Library Facilitating Out-of-core Implementations of Accelerator Kernels on Hybrid Computing Platforms (Daniel Hanlon - 15 August, 2018)
We present a library called libhclooc, which provides a unifying interface facilitating out-of-core implementations for data parallel kernels on the three different mainstream accelerators (GPUs, Intel Xeon Phis, FPGAs). We show that it suffers from a maximum overhead of 10%, 4%, and 8% (due to abstraction) compared to the state-of-the-art optimised implementations for Nvidia K40c GPU, Nvidia P100 PCIe GPU, and Intel Xeon Phi 3120P respectively. We also show that using libhclooc API reduces the number of lines of code (LOC) by 75% thereby drastically improving programmer productivity.
Link: https://arxiv.org/abs/1808.05056
====================================================
FaceOff: Anonymizing Videos in the Operating Rooms (Evangello Flouty - 6 August, 2018)
We also propose a temporal regularisation kernel to improve recall rates. The fine-tuned model achieves a face detection recall of 88.05 % and 93.45 % before and after applying temporal-smoothing respectively.
Link: https://arxiv.org/abs/1808.04440
====================================================
A robust image-based cryptology scheme based on cellular non-linear network and local image descriptors (Mohammad Mahdi Dehshibi - 10 August, 2018)
This scheme consists of a cryptography and steganography sub-module in which a 3D CNN is designed to produce a chaotic map as the kernel of the system to preserve confidentiality and data integrity in cryptology. Conducted experiments on 25 standard images prove the effectiveness of the proposed cryptology scheme in terms of security, visual, and complexity analysis.
Link: https://arxiv.org/abs/1808.03702
====================================================
Accelerating wave-propagation algorithms with adaptive mesh refinement using the Graphics Processing Unit (GPU) (Xinsheng Qin - 8 August, 2018)
Some small GPU kernels are merged into bigger kernels, which greatly reduces kernel launching overhead. A speed-up between $2$ and $3$ for the total running time is observed in an acoustics benchmark problem.
Link: https://arxiv.org/abs/1808.02638
====================================================
Question-Guided Hybrid Convolution for Visual Question Answering (Peng Gao - 8 August, 2018)
Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features.To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage
Link: https://arxiv.org/abs/1808.02632
====================================================
Multi-Output Convolution Spectral Mixture for Gaussian Processes (Kai Chen - 18 September, 2018)
In particular, Multi-Output Spectral Mixture kernel (MOSM) is a recent, powerful state of the art method
Link: https://arxiv.org/abs/1808.02266
====================================================
Efficient Fusion of Sparse and Complementary Convolutions (Chun-Fu Chen - 10 September, 2018)
The core of our approach is an efficient network module that linearly combines sparse kernels to yield feature representations as strong as those from regular kernels. For object detection, our approach leads to a VGG-16-based Faster RCNN detector that is 12.4$\times$ smaller and about 3$\times$ faster than the baseline.
Link: https://arxiv.org/abs/1808.02167
====================================================
Attentive Semantic Alignment with Offset-Aware Correlation Kernels (Paul Hongsuck Seo - 6 August, 2018)
Experiments demonstrate the effectiveness of the attentive model and offset-aware kernel, and the proposed model combining both techniques achieves the state-of-the-art performance.
Link: https://arxiv.org/abs/1808.02128
====================================================
Beyond $1/2$-Approximation for Submodular Maximization on Massive Data Streams (Ashkan Norouzi-Fard - 6 August, 2018)
Many tasks in machine learning and data mining, such as data diversification, non-parametric learning, kernel machines, clustering etc., require extracting a small but representative summary from a massive dataset. (2014), always finds a $0.5$-approximate solution. It is the first low-memory, single-pass algorithm that improves the factor $0.5$, under the natural assumption that elements arrive in a random order. We also show that this assumption is necessary, i.e., that there is no such algorithm with better than $0.5$-approximation when elements arrive in arbitrary order
Link: https://arxiv.org/abs/1808.01842
====================================================
Geared Rotationally Identical and Invariant Convolutional Neural Network Systems (ShihChung B. Lo - 10 August, 2018)
Using an ordinary CNN structure as a base, requirements for constructing a GRI-CNN include the use of either symmetric input vector or kernels with an angle increment that can form a complete cycle as a "gearwheel". With a design of using an ultra-fine gear-tooth step angle (e.g., 1 degree or 0.1 degree), all four GRI-CNN systems can be constructed virtually isotropically.
Link: https://arxiv.org/abs/1808.01280
====================================================
Maximum Margin Metric Learning Over Discriminative Nullspace for Person Re-identification (T M Feroz Ali - 28 July, 2018)
In this paper we propose a novel metric learning framework called Nullspace Kernel Maximum Margin Metric Learning (NK3ML) which efficiently addresses the small sample size (SSS) problem inherent in person re-identification and offers a significant performance gain over existing state-of-the-art methods. We obtain 99.8% rank-1 accuracy on the most widely accepted and challenging dataset VIPeR, compared to the previous state of the art being only 63.92%.
Link: https://arxiv.org/abs/1807.10908
====================================================
Leveraging Support Vector Machine for Opcode Density Based Detection of Crypto-Ransomware (James Baldwin - 27 July, 2018)
Using the SMO classifier and PUK kernel in the WEKA machine learning toolset it demonstrates that this methodology can achieve 100% precision when differentiating between ransomware and goodware, and 96.5% when differentiating between 5 cryptoransomware families and goodware. Moreover, 8 different attribute selection methods are evaluated to achieve significant feature reduction. Using the CorrelationAttributeEval method close to 100% precision can be maintained with a feature reduction of 59.5%. The CFSSubset filter achieves the highest feature reduction of 97.7% however with a slightly lower precision at 94.2%.
Link: https://arxiv.org/abs/1807.10442
====================================================
Meta-Learning Priors for Efficient Online Bayesian Regression (James Harrison - 24 July, 2018)
We find our approach outperforms kernel-based GP regression, as well as state of the art meta-learning approaches, thereby providing a promising plug-in tool for many regression tasks in robotics where scalability and data-efficiency are important.
Link: https://arxiv.org/abs/1807.08912
====================================================
PCNNA: A Photonic Convolutional Neural Network Accelerator (Armin Mehrabian - 23 July, 2018)
Here, we aim to exploit the synergy between the inherent parallelism of photonics in the form of Wavelength Division Multiplexing (WDM) and sparsity of connections between input feature maps and kernels in CNNs. While our full system design offers up to more than 3 orders of magnitude speedup in execution time, its optical core potentially offers more than 5 order of magnitude speedup compared to state-of-the-art electronic counterparts.
Link: https://arxiv.org/abs/1807.08792
====================================================
An Empirical Approach For Probing the Definiteness of Kernels (Martin Zaefferer - 10 July, 2018)
Finally, models can be enabled to use indefinite kernels. We provide a proof-of-concept with 16 different distance measures for permutations
Link: https://arxiv.org/abs/1807.03555
====================================================
Multi-kernel unmixing and super-resolution using the Modified Matrix Pencil method (StÃ©phane ChrÃ©tien - 8 July, 2018)
This problem is a generalization of the usual super-resolution setup wherein $L = 1$; we call this the multi-kernel unmixing super-resolution problem. Our approach involves estimating the group parameters sequentially in the order of increasing scale parameters, i.e., from group $1$ to $L$. In particular, the estimation process at stage $1 \leq l \leq L$ involves (i) carefully sampling the tail of the Fourier transform of $y$, (ii) a \emph{deflation} step wherein we subtract the contribution of the groups processed thus far from the obtained Fourier samples, and (iii) applying Moitra's modified Matrix Pencil method on a deconvolved version of the samples in (ii).
Link: https://arxiv.org/abs/1807.02862
====================================================
Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery (Seyed Majid Azimi - 10 July, 2018)
In this work, we propose a new method consisting of a novel joint image cascade and feature pyramid network with multi-size convolution kernels to extract multi-scale strong and weak semantic features. Our method achieves 68.16\% mAP on horizontal and 72.45\% mAP on oriented bounding box detection tasks on the challenging new DOTA dataset, outperforming all published methods by a large margin (+6% and +12% absolute improvement, respectively)
Link: https://arxiv.org/abs/1807.02700
====================================================
Compiler Phase Ordering as an Orthogonal Approach for Reducing Energy Consumption (Ricardo Nobre - 2 July, 2018)
Our experiments with a set of representative kernels show that there we can reduce energy consumption by up to 24% and that some of these improvements can only be partially explained by improvements to execution time
Link: https://arxiv.org/abs/1807.00638
====================================================
Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays (Amogh Agrawal - 1 July, 2018)
This is beneficial for deep networks, where the kernel size grows and requires to be stored in multiple sub-banks. Moreover, an energy improvement of 2.5x, and a performance improvement of 4x was achieved by using the proposed sectioned-SRAM, compared to a non-sectioned SRAM design.
Link: https://arxiv.org/abs/1807.00343
====================================================
Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data (Yanru Qu - 1 July, 2018)
Generalizing the kernel product to a net-in-net architecture, we further propose Product-network In Network (PIN) which can generalize previous models. Extensive experiments on 4 industrial datasets and 1 contest dataset demonstrate that our models consistently outperform 8 baselines on both AUC and log loss. Besides, PIN makes great CTR improvement (relatively 34.67%) in online A/B test.
Link: https://arxiv.org/abs/1807.00311
====================================================
Practical Cryptographic Data Integrity Protection with Full Disk Encryption Extended Version (Milan Broz - 1 July, 2018)
Our implementation has been included in the Linux kernel since the version 4.12. This is extended version of our article that appears in IFIP SEC 2018 conference proceedings.
Link: https://arxiv.org/abs/1807.00309
====================================================
Learning from graphs with structural variation (Rune Kok Nielsen - 29 June, 2018)
Next, we investigate the performance of the state-of-the-art Weisfeiler-Lehman graph kernel under increasing synthetic structural errors and find that the effect of introducing errors depends strongly on the dataset.
Link: https://arxiv.org/abs/1806.11377
====================================================
Speeding Up Budgeted Stochastic Gradient Descent SVM Training with Precomputed Golden Section Search (Tobias Glasmachers - 26 June, 2018)
Limiting the model size of a kernel support vector machine to a pre-defined budget is a well-established technique that allows to scale SVM learning and prediction to large-scale data. We manage to reduce the merging time by up to 65% and the total training time by 44% without any loss of accuracy.
Link: https://arxiv.org/abs/1806.10180
====================================================
Multi-Merge Budget Maintenance for Stochastic Gradient Descent SVM Training (Sahar Qaadan - 26 June, 2018)
Budgeted Stochastic Gradient Descent (BSGD) is a state-of-the-art technique for training large-scale kernelized support vector machines. The process of finding suitable merge partners is costly; it can account for up to 45% of the total training time
Link: https://arxiv.org/abs/1806.10179
====================================================
Improving tasks throughput on accelerators using OpenCL command concurrency (A. J. LÃ¡zaro-MuÃ±oz - 1 July, 2018)
Moreover, employing the temporal execution model, a heuristic is proposed which is able to establish a near-optimal tasks execution ordering that significantly reduces the total execution time, including data transfers.The heuristic has been evaluated with five different benchmarks composed of dominant kernel and dominant transfer real tasks. Concretely, our heuristic obtains, on average for all the devices, between 84\% and 96\% of the improvement achieved by the best execution order.
Link: https://arxiv.org/abs/1806.10113
====================================================
Data Reduction for Maximum Matching on Real-World Graphs: Theory and Experiments (Viatcheslav Korenwein - 25 June, 2018)
We build on recent theoretical work focusing on linear-time data reduction rules for finding maximum-cardinality matchings and complement the theoretical results by presenting and analyzing (thereby employing the kernelization methodology of parameterized complexity analysis) linear-time data reduction rules for the positive-integer-weighted case. Moreover, we experimentally demonstrate that these data reduction rules provide significant speedups of the state-of-the art implementation for computing matchings in real-world graphs: the average speedup is 3800% in the unweighted case and "just" 30% in the weighted case.
Link: https://arxiv.org/abs/1806.09683
====================================================
BISMO: A Scalable Bit-Serial Matrix Multiplication Overlay for Reconfigurable Computing (Yaman Umuroglu - 22 June, 2018)
Matrix-matrix multiplication is a key computational kernel for numerous applications in science and engineering, with ample parallelism and data locality that lends itself well to high-performance implementations. We characterize the resource usage and performance of BISMO across a range of parameters to build a hardware cost model, and demonstrate a peak performance of 6.5 TOPS on the Xilinx PYNQ-Z1 board.
Link: https://arxiv.org/abs/1806.08862
====================================================
High Speed Kernelized Correlation Filters without Boundary Effect (Ming Tang - 10 July, 2018)
To ensure nBEKCF runs at high speed, we present two efficient algorithms, ACSII and CCIM, to significantly accelerate the evaluation of kernel matrix without employing FFT. Preliminary experimental results on two public datasets, OTB2013 and OTB2015, show that, without bells and whistles, nBEKCF outperforms representative trackers with hand-crafted features, in the meanwhile, runs at 70 fps on average.
Link: https://arxiv.org/abs/1806.06406
====================================================
SIMD Vectorization for the Lennard-Jones Potential with AVX2 and AVX-512 instructions (Hiroshi Watanabe - 13 June, 2018)
Since the force-calculation kernel of the molecular dynamics method involves indirect access to memory, the data layout is one of the most important factors in vectorization. The performance gains by vectorization are about 42\% on HSW compared with the code optimized without vectorization. On KNL, the hand-vectorized codes exhibit 34\% better performance than the codes vectorized automatically by the Intel compiler
Link: https://arxiv.org/abs/1806.05713
====================================================
Cardiac Motion Scoring with Segment- and Subject-level Non-Local Modeling (Wufeng Xue - 14 June, 2018)
Besides, Cardiac-MOS can effectively extract motion information from MR sequences of various lengths by interpolating the convolution kernel along the temporal dimension, therefore can be applied to MR sequences of multiple sources. Experiments on 1440 myocardium segments of 90 subjects from short axis MR sequences of multiple lengths prove that Cardiac-MOS achieves reliable performance, with correlation of 0.926 for motion score index estimation and accuracy of 77.4\% for motion scoring
Link: https://arxiv.org/abs/1806.05569
====================================================
Holographic Automata for Ambient Immersive A. I. via Reservoir Computing (Theophanes E. Raptis - 20 June, 2018)
We prove the existence of a semilinear representation of Cellular Automata (CA) with the introduction of multiple convolution kernels. Examples of the technique are presented for rules akin to the "edge-of-chaos" including the Turing universal rule 110 for further utilization in the area of reservoir computing
Link: https://arxiv.org/abs/1806.05108
====================================================
When Regression Verification Meets CEGAR (Fei He - 12 June, 2018)
We performed extensive experiments on a large number of industrial programs (534 revisions of 89 Linux kernel device drivers)
Link: https://arxiv.org/abs/1806.04829
====================================================
Circular-shift Linear Network Codes with Arbitrary Odd Block Lengths (Qifu Tyler Sun - 12 June, 2018)
Circular-shift linear network coding (LNC) is a special type of vector LNC with low encoding and decoding complexities, with local encoding kernels chosen from cyclic permutation matrices. When $L$ is a prime with primitive root $2$, it was recently shown that a scalar linear solution over GF($2^{L-1}$) can induce an $L$-dimensional circular-shift linear solution at rate $(L-1)/L$. In this work, we prove that for an arbitrary odd $L$, every scalar linear solution over GF($2^{m_L}$), where $m_L$ refers to the multiplicative order of $2$ modulo $L$, can induce an $L$-dimensional circular-shift linear solution at a certain rate
Link: https://arxiv.org/abs/1806.04635
====================================================
Learning Transferable UAV for Forest Visual Perception (Lyujie Chen - 10 June, 2018)
To transfer the learned strategy to the real world, we construct a ResNet-18 adaptation model via multi-kernel maximum mean discrepancies to leverage the relevant labelled data and alleviate the discrepancy between simulated and real environment. The ResNet-18 adaptation and its variant model achieve the best result of 84.08% accuracy in reality.
Link: https://arxiv.org/abs/1806.03626
====================================================
Shape Robust Text Detection with Progressive Scale Expansion Network (Xiang Li - 7 June, 2018)
Due to the fact that there are large geometrical margins among these minimal kernels, our method is effective to distinguish the adjacent text instances and is robust to arbitrary shapes. The state-of-the-art results on ICDAR 2015 and ICDAR 2017 MLT benchmarks further confirm the great effectiveness of PSENet. Notably, PSENet outperforms the previous best record by absolute 6.37\% on the curve text dataset SCUT-CTW1500
Link: https://arxiv.org/abs/1806.02559
====================================================
GP-RVM: Genetic Programing-based Symbolic Regression Using Relevance Vector Machine (Hossein Izadi Rad - 25 August, 2018)
RVM which is a sparse Bayesian kernel method selects suitable functions to constitute the basis. The computational complexity of GP-RVM scales in $O( M^{3})$, where $M$ is the number of functions in the basis set and is typically much smaller than the number $N$ of training patterns.
Link: https://arxiv.org/abs/1806.02502
====================================================
Elasticizing Linux via Joint Disaggregation of Memory and Computation (Ehab Ababneh - 3 June, 2018)
We have implemented these primitives in a Linux 2.6 kernel, collectively calling the extended operating system, ElasticOS. Our evaluation across a variety of algorithms shows up to 10x improvement in performance over standard network swap.
Link: https://arxiv.org/abs/1806.00885
====================================================
IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks (Ke Sun - 20 July, 2018)
We empirically demonstrate that the combination of low-rank and sparse kernels boosts the performance and the superiority of our proposed approach to the state-of-the-arts, IGCV2 and MobileNetV2 over image classification on CIFAR and ImageNet and object detection on COCO.
Link: https://arxiv.org/abs/1806.00178
====================================================
HOPF: Higher Order Propagation Framework for Deep Collective Classification (Priyesh Vijayan - 21 September, 2018)
Recent state-of-the-art models for CC learn end-to-end differentiable variations of Weisfeiler-Lehman (WL) kernels to aggregate multi-hop neighborhood information. We do an extensive evaluation across 11 datasets from different domains
Link: https://arxiv.org/abs/1805.12421
====================================================
Harmonic-summing Module of SKA on FPGA--Optimising the Irregular Memory Accesses (Haomiao Wang - 28 June, 2018)
While in raw performance a single FPGA board cannot compete with a GPU, in terms of energy dissipation, GPU costs up to 2.6x times more energy than that of FPGAs in executing the same NDRange kernels.
Link: https://arxiv.org/abs/1805.12258
====================================================
CuisineNet: Food Attributes Classification using Multi-scale Convolution Network (Md. Mostafa Kamal Sarker - 8 June, 2018)
The aggregation of multi-scale convolution layers with different kernel size is also used for weighting the features results from different scales. The experimental results show that our proposed method yields 65% and 62% average F1 score on validation and test set which outperforming the state-of-the-art models.
Link: https://arxiv.org/abs/1805.12081
====================================================
Hypervisor-Based Active Data Protection for Integrity and Confidentiality of Dynamically Allocated Memory in Windows Kernel (Igor Korkin - 30 May, 2018)
All this data can be tampered with by kernel-mode malware. AllMemPro prevents access to even 1 byte of allocated data, adapts for newly allocated memory in real time, and protects the driver without its source code. AllMemPro works well on newest Windows 10 1709 x64.
Link: https://arxiv.org/abs/1805.11847
====================================================
Hamiltonian Variational Auto-Encoder (Anthony L. Caterini - 29 May, 2018)
While the use of Markov chain Monte Carlo (MCMC) techniques such as Hamiltonian Monte Carlo (HMC) has been previously suggested to achieve this [23, 26], the proposed methods require specifying reverse kernels which have a large impact on performance. We show here how to optimally select reverse kernels in this setting and, by building upon Hamiltonian Importance Sampling (HIS) [17], we obtain a scheme that provides low-variance unbiased estimators of the ELBO and its gradients using the reparameterization trick. This method can be reinterpreted as a target-informed normalizing flow [20] which, within our context, only requires a few evaluations of the gradient of the sampled likelihood and trivial Jacobian calculations at each iteration.
Link: https://arxiv.org/abs/1805.11328
====================================================
Scalable Spectral Clustering Using Random Binning Features (Lingfei Wu - 11 August, 2018)
Specifically, we implicitly approximate the graph similarity (kernel) matrix by the inner product of a large sparse feature matrix generated by RB. Extensive experiments on 8 benchmarks show that the proposed method either outperforms or matches the state-of-the-art methods in both accuracy and runtime
Link: https://arxiv.org/abs/1805.11048
====================================================
Fast Dynamic Routing Based on Weighted Kernel Density Estimation (Suofei Zhang - 31 August, 2018)
To address this problem, we generalize existing routing methods within the framework of weighted kernel density estimation, and propose two fast routing methods with different optimization strategies. Our methods prompt the time efficiency of routing by nearly 40\% with negligible performance degradation. By stacking a hybrid of convolutional layers and capsule layers, we construct a network architecture to handle inputs at a resolution of $64\times{64}$ pixels
Link: https://arxiv.org/abs/1805.10807
====================================================
Understanding Generalization and Optimization Performance of Deep CNNs (Pan Zhou - 28 May, 2018)
Specifically, for a CNN model consisting of $l$ convolutional layers and one fully connected layer, we prove that its generalization error is bounded by $\mathcal{O}(\sqrt{\dt\widetilde{\varrho}/n})$ where $Î¸$ denotes freedom degree of the network parameters and $\widetilde{\varrho}=\mathcal{O}(\log(\prod_{i=1}^{l}\rwi{i} (\ki{i}-\si{i}+1)/p)+\log(\rf))$ encapsulates architecture parameters including the kernel size $\ki{i}$, stride $\si{i}$, pooling size $p$ and parameter magnitude $\rwi{i}$
Link: https://arxiv.org/abs/1805.10767
====================================================
EcoRNN: Fused LSTM RNN Implementation with Data Layout Optimization (Bojian Zheng - 22 May, 2018)
For example, default implementations in Tensorflow and MXNet invoke many tiny GPU kernels, leading to excessive overhead in launching GPU threads. Although cuDNN, NVIDIA's deep learning library, can accelerate performance by around 2x, it is closed-source and inflexible, hampering further research and performance improvements in frameworks, such as PyTorch, that use cuDNN as their backend. We show that (1) fusing tiny GPU kernels and (2) applying data layout optimization can give us a maximum performance boost of 3x over MXNet default and 1.5x over cuDNN implementations
Link: https://arxiv.org/abs/1805.08899
====================================================
One machine, one minute, three billion tetrahedra (CÃ©lestin Marot - 22 May, 2018)
Our second contribution is a multi-threaded version of the Delaunay kernel able to concurrently insert vertices. The performances of our implementation have been measured on three different processors, Intel core-i7, Intel Xeon Phi and AMD EPYC, on which we have been able to compute 3 billion tetrahedra in 53 seconds . This corresponds to a generation rate of over 55 million tetrahedra per second which is, to our best knowledge, three times the rate reached by the current fastest implementation
Link: https://arxiv.org/abs/1805.08831
====================================================
Localized Multiple Kernel Learning for Anomaly Detection: One-class Classification (Chandan Gautam - 17 July, 2018)
Proposed LMKAD approach adapts the weight for each kernel using a gating function. We present the empirical results of the performance of LMKAD on 25 benchmark datasets from various disciplines
Link: https://arxiv.org/abs/1805.07892
====================================================
Multi-layer Kernel Ridge Regression for One-class Classification (Chandan Gautam - 1 June, 2018)
In this paper, a multi-layer architecture (in a hierarchical fashion) by stacking various Kernel Ridge Regression (KRR) based Auto-Encoder for one-class classification is proposed and is referred as MKOC. The proposed MKOC is experimentally evaluated on 15 publicly available benchmark datasets. Experimental results verify the effectiveness of the proposed approach over 11 existing state-of-the-art kernel-based one-class classifiers
Link: https://arxiv.org/abs/1805.07808
====================================================
Dual parameterization of Weighted Coloring (JÃºlio AraÃºjo - 17 May, 2018)
On the other hand, we present a kernel with at most $(2^{k-1}+1) (k-1)$ vertices, and we rule out the existence of polynomial kernels unless ${\sf NP} \subseteq {\sf coNP} / {\sf poly}$, even on split graphs with only two different weights
Link: https://arxiv.org/abs/1805.06699
====================================================
Beyond 5G with UAVs: Foundations of a 3D Wireless Cellular Network (Mohammad Mozaffari - 16 May, 2018)
To this end, first, the spatial distribution of the drone-UEs is estimated using a kernel density estimation method, and the parameters of the estimator are obtained using a cross-validation method. In particular, the proposed approach yields a reduction of up to 46% in the average latency compared to the SINR-based association
Link: https://arxiv.org/abs/1805.06532
====================================================
Learning to Deblur Images with Exemplars (Jinshan Pan - 14 May, 2018)
The success of the state-of-the-art image deblurring algorithms stems mainly from implicit or explicit restoration of salient edges for kernel estimation
Link: https://arxiv.org/abs/1805.05503
====================================================
RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition (Albert Zeyer - 24 May, 2018)
We compare the fast training and decoding speed of RETURNN of attention models for translation, due to fast CUDA LSTM kernels, and a fast pure TensorFlow beam search decoder. We show that a layer-wise pretraining scheme for recurrent attention models gives over 1% BLEU improvement absolute and it allows to train deeper recurrent encoder networks. We are able to train state-of-the-art models for translation and end-to-end models for speech recognition and show results on WMT 2017 and Switchboard
Link: https://arxiv.org/abs/1805.05225
====================================================
UnibucKernel Reloaded: First Place in Arabic Dialect Identification for the Second Year in a Row (Andrei M. Butnaru - 28 July, 2018)
In the learning stage, we independently employ Kernel Discriminant Analysis (KDA) and Kernel Ridge Regression (KRR). Our approach is shallow and simple, but the empirical results obtained in the 2018 ADI Closed Shared Task prove that it achieves the best performance. Furthermore, our top macro-F1 score (58.92%) is significantly better than the second best score (57.59%) in the 2018 ADI Shared Task, according to the statistical significance test performed by the organizers. Nevertheless, we obtain even better post-competition results (a macro-F1 score of 62.28%) using the audio embeddings released by the organizers after the competition. With a very similar approach (that did not include phonetic features), we also ranked first in the ADI Closed Shared Tasks of the 2017 VarDial Evaluation Campaign, surpassing the second best method by 4.62%
Link: https://arxiv.org/abs/1805.04876
====================================================
Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic Segmentation (Yunchao Wei - 27 May, 2018)
Specifically, we find that varying dilation rates can effectively enlarge the receptive fields of convolutional kernels and more importantly transfer the surrounding discriminative information to non-discriminative object regions, promoting the emergence of these regions in the object localization maps. In particular, it achieves 60.8% and 67.6% mIoU scores on Pascal VOC 2012 test set in weakly- (only image-level labels are available) and semi- (1,464 segmentation masks are available) supervised settings, which are the new state-of-the-arts.
Link: https://arxiv.org/abs/1805.04574
====================================================
Retinal Vessel Segmentation Based on Conditional Deep Convolutional Generative Adversarial Networks (Yun Jiang - 10 May, 2018)
In order to reduce the number of parameters and calculations, using a small convolution to halve the number of channels in the input signature before using a large convolution kernel. By verifying the method on the DRIVE and STARE datasets, the segmentation accuracy rate is 96.08% and 97.71%, the sensitivity reaches 82.74% and 85.34% respectively, and the F-measure reaches 82.08% and 85.02% respectively. The sensitivity is 4.82% and 2.4% higher than that of R2U-Net.
Link: https://arxiv.org/abs/1805.04224
====================================================
FlashAbacus: A Self-Governing Flash-Based Accelerator for Low-Power Systems (Jie Zhang - 7 May, 2018)
The proposed accelerator can simultaneously process data from different applications with diverse types of operational functions, and it allows multiple kernels to directly access flash without the assistance of a host-level file system or an I/O runtime library. We prototype FlashAbacus on a multicore-based PCIe platform that connects to FPGA-based flash controllers with a 20 nm node process. The evaluation results show that FlashAbacus can improve the bandwidth of data processing by 127%, while reducing energy consumption by 78.4%, as compared to a conventional method of heterogeneous computing. \blfootnote{This paper is accepted by and will be published at 2018 EuroSys
Link: https://arxiv.org/abs/1805.02807
====================================================
Improving Network Intrusion Detection Classifiers by Non-payload-Based Exploit-Independent Obfuscations: An Adversarial Approach (Ivan Homoliak - 7 May, 2018)
We perform the evaluation of five classifiers: Gaussian Naive Bayes, Gaussian Naive Bayes with kernel density estimation, Logistic Regression, Decision Tree, and Support Vector Machines. Our experiments confirm the assumption that it is possible to evade the intrusion detection capability of all classifiers trained without prior knowledge about obfuscated attacks, causing an exacerbation of the TPR ranging from 7.8% to 66.8%. Further, when widening the training knowledge of the classifiers by a subset of obfuscated attacks, we achieve a significant improvement of the TPR by 4.21% - 73.3%, while the FPR is deteriorated only slightly (0.1% - 1.48%). Finally, we test the capability of an obfuscations-aware classifier to detect unknown obfuscated attacks, where we achieve over 90% detection rate on average for most of the obfuscations.
Link: https://arxiv.org/abs/1805.02684
====================================================
Bi-directional Graph Structure Information Model for Multi-Person Pose Estimation (Jing Wang - 19 August, 2018)
The first branch predicts the confidence maps of joints and uses a geometrical transform kernel to propagate information between neighboring joints at the confidence level. Based on the proposed network structure, we achieve an average precision of 62.9 on the COCO Keypoint Challenge dataset and 77.6 on the MPII (multi-person) dataset
Link: https://arxiv.org/abs/1805.00603
====================================================
Automated essay scoring with string kernels and word embeddings (MÄdÄlina Cozma - 6 July, 2018)
String kernels capture the similarity among strings based on counting common character n-grams, which are a low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification
Link: https://arxiv.org/abs/1804.07954
====================================================
Nonparametric Stochastic Compositional Gradient Descent for Q-Learning in Continuous Markov Decision Problems (Alec Koppel - 19 April, 2018)
To ameliorate this complexity explosion, we apply Kernel Orthogonal Matching Pursuit to the sequence of kernel weights and dictionaries, which yields a controllable error in the descent direction of the underlying optimization method. We prove that the resulting algorithm, called KQ-Learning, converges with probability 1 to a stationary point of this problem, yielding a fixed point of the Bellman optimality operator under the hypothesis that it belongs to the RKHS
Link: https://arxiv.org/abs/1804.07323
====================================================
Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy (Guanglu Song - 2 June, 2018)
Fully convolutional neural network (FCN) has been dominating the game of face detection task for a few years with its congenital capability of sliding-window-searching with shared kernels, which boiled down all the redundant calculation, and most recent state-of-the-art methods such as Faster-RCNN, SSD, YOLO and FPN use FCN as their backbone
Link: https://arxiv.org/abs/1804.05197
====================================================
Detection of Compromised Smart Grid Devices with Machine Learning and Convolution Techniques (Cengiz Kaygusuz - 13 April, 2018)
Our framework specifically utilizes system and library call lists at the kernel level of the operating system on both resource-limited and resource-rich smart grid devices such as RTUs, PLCs, PMUs, and IEDs. 91%) which reveals that the framework is effective to overcome compromised smart grid devices problem.
Link: https://arxiv.org/abs/1804.05106
====================================================
Î¼-cuDNN: Accelerating Deep Learning Frameworks with Micro-Batching (Yosuke Oyama - 13 April, 2018)
NVIDIA cuDNN is a low-level library that provides GPU kernels frequently used in deep learning. We demonstrate the effectiveness of Î¼-cuDNN over two frameworks, Caffe and TensorFlow, achieving speedups of 1.63x for AlexNet and 1.21x for ResNet-18 on P100-SXM2 GPU
Link: https://arxiv.org/abs/1804.04806
====================================================
The Generalized Matrix Chain Algorithm (Henrik Barthels - 10 April, 2018)
The challenge then shifts from finding an optimal parenthesization to finding an optimal mapping of the input expression to the available kernels. In our experiments, the generated code outperforms other libraries and languages on average by a factor of about 9
Link: https://arxiv.org/abs/1804.04021
====================================================
A neural network memory prefetcher using semantic locality (Leeor Peled - 26 July, 2018)
We show that the prefetcher can deliver an average speedup of 30% for SPEC2006 (up to 2.7x) and up to 4.6x over kernels
Link: https://arxiv.org/abs/1804.00478
====================================================
Multi-scale Location-aware Kernel Representation for Object Detection (Hao Wang - 2 April, 2018)
Our MLKP can be efficiently computed on a modified multi-scale feature map using a low-dimensional polynomial kernel approximation.Moreover, different from existing orderless global representations based on high-order statistics, our proposed MLKP is location retentive and sensitive so that it can be flexibly adopted to object detection. Through integrating into Faster R-CNN schema, the proposed MLKP achieves very competitive performance with state-of-the-art methods, and improves Faster R-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0% (AP at IOU=[0.5:0.05:0.95]) on PASCAL VOC 2007, VOC 2012 and MS COCO benchmarks, respectively
Link: https://arxiv.org/abs/1804.00428
====================================================
A Study of Clustering Techniques and Hierarchical Matrix Formats for Kernel Ridge Regression (Elizaveta Rebrova - 27 March, 2018)
These results confirm that --- with correct tuning of the hyperparameters --- classification using kernel ridge regression with the compressed matrix does not lose prediction accuracy compared to the exact --- not compressed --- kernel matrix and that our approach can be extended to $\mathcal{O}(1M)$ datasets, for which computation with the full kernel matrix becomes prohibitively expensive. We present numerical experiments in a distributed memory environment up to 1,024 processors of the NERSC's Cori supercomputer using well-known datasets to the machine learning community that range from dimension 8 up to 784.
Link: https://arxiv.org/abs/1803.10274
====================================================
Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave Scattering (Mustafa Abduljabbar - 27 March, 2018)
We extract the potential thread- and data-level parallelism of the key Helmholtz kernels of FMM. With shared memory optimizations, we achieve roughly 77% of peak single precision floating point performance of a 56-core Skylake processor, and on average 60% of peak single precision floating point performance of a 72-core KNL. These numbers represent nearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the baseline scalar code. In addition, we exhibit up to 85% efficiency in strong scaling. We compute in excess of 2 billion DoF on the full-scale of the Cray XC40 supercomputer.
Link: https://arxiv.org/abs/1803.09948
====================================================
Speeding-up Object Detection Training for Robotics with FALKON (Elisa Maiettini - 27 August, 2018)
Proposed approaches are based on end-to-end learning by back-propagation [22] or kernel methods trained with Hard Negatives Mining on top of deep features [8]. In this paper we propose a novel pipeline for object detection that overcomes this problem and provides comparable performance, with a 60x training speedup. Our pipeline combines (i) the Region Proposal Network and the deep feature extractor from [22] to efficiently select candidate RoIs and encode them into powerful representations, with (ii) the FALKON [23] algorithm, a novel kernel-based method that allows fast training on large scale problems (millions of points). We assess the effectiveness of the approach on a standard Computer Vision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a real robotic scenario with the iCubWorld Transformations [18] dataset.
Link: https://arxiv.org/abs/1803.08740
====================================================
Reservoir computing approaches for representation and classification of multivariate time series (Filippo Maria Bianchi - 21 March, 2018)
We compare with state-of-the-art recurrent networks, standard RC approaches and time series kernels on multiple classification tasks, showing that the proposed algorithms can achieve superior classification accuracy, while being vastly more efficient to train.
Link: https://arxiv.org/abs/1803.07870
====================================================
Dynamic Sampling Convolutional Neural Networks (Jialin Wu - 22 March, 2018)
While DSCNNs inherit the advantages of DFN, namely avoiding feature map blurring by position-specific kernels while keeping translation invariance, it also efficiently alleviates the overfitting issue caused by much more parameters than normal CNNs. Our results show that DSCNNs enjoy stronger recognition abilities and achieve 81.7% in VOC2012 detection dataset
Link: https://arxiv.org/abs/1803.07624
====================================================
UnibucKernel: A kernel-based learning method for complex word identification (Andrei M. Butnaru - 22 May, 2018)
In this paper, we present a kernel-based learning approach for the 2018 Complex Word Identification (CWI) Shared Task
Link: https://arxiv.org/abs/1803.07602
====================================================
Efficient Hardware Realization of Convolutional Neural Networks using Intra-Kernel Regular Pruning (Maurice Yang - 15 March, 2018)
Unlike other pruning methods such as Fine-Grained pruning, IKR pruning maintains regular kernel structures that are exploitable in a hardware accelerator. Experimental results demonstrate up to 10x parameter reduction and 7x computational reduction at a cost of less than 1% degradation in accuracy versus the un-pruned case.
Link: https://arxiv.org/abs/1803.05909
====================================================
ASAP: Accelerated Short-Read Alignment on Programmable Hardware (Subho S. Banerjee - 23 May, 2018)
We focus on the Levenshtein distance (edit-distance) computation kernel and propose the ASAP accelerator, which utilizes the intrinsic delay of circuits for edit-distance computation elements as a proxy for computation. Our design is implemented on an Xilinx Virtex 7 FPGA in an IBM POWER8 system that uses the CAPI interface for cache coherence across the CPU and FPGA
Link: https://arxiv.org/abs/1803.02657
====================================================
Local Distance Metric Learning for Nearest Neighbor Algorithm (Hossein Rajabzadeh - 15 March, 2018)
The quality as well as the efficiency of the proposed method assesses through a set of different experiments on various datasets and the obtained results show that LDML as well as the kernelized version is superior to the other related state-of-the-art methods.
Link: https://arxiv.org/abs/1803.01562
====================================================
House Price Modeling over Heterogeneous Regions with Hierarchical Spatial Functional Analysis (Bang Liu - 28 February, 2018)
However, by using uniformly damping kernels, they are unable to handle irregularly shaped regions or capture land value discontinuities within the same region due to the existence of implicit sub-communities, which are common in real-world scenarios. Extensive evaluations based on housing data in a major Canadian city show that our proposed approach can reduce the mean relative house price estimation error down to 6.60%.
Link: https://arxiv.org/abs/1803.00919
====================================================
CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes (Yuhong Li - 11 April, 2018)
The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.
Link: https://arxiv.org/abs/1802.10062
====================================================
VBALD - Variational Bayesian Approximation of Log Determinants (Diego Granziol - 21 February, 2018)
Applications thereof range from Gaussian processes, minimum-volume ellipsoids, metric learning, kernel learning, Bayesian neural networks, Determinental Point Processes, Markov random fields to partition functions of discrete graphical models. In order to avoid the canonical, yet prohibitive, Cholesky $\mathcal{O}(n^{3})$ computational cost, we propose a novel approach, with complexity $\mathcal{O}(n^{2})$, based on a constrained variational Bayes algorithm
Link: https://arxiv.org/abs/1802.08054
====================================================
AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning (Ahmed M. Alaa - 20 February, 2018)
This is achieved by modeling the pipelines performances as a black-box function with a Gaussian process prior, and modeling the similarities between the pipelines baseline algorithms via a sparse additive kernel with a Dirichlet prior. We demonstrate the utility of AUTOPROGNOSIS using 10 major patient cohorts representing various aspects of cardiovascular patient care.
Link: https://arxiv.org/abs/1802.07207
====================================================
Improved TDNNs using Deep Kernels and Frequency Dependent Grid-RNNs (Florian Kreyssig - 20 February, 2018)
Experiments using the multi-genre broadcast (MGB3) English data (275h) show that deep kernel TDNNs reduces the word error rate (WER) by 6% relative and when combined with the frequency dependent Grid-RNN gives a relative WER reduction of 9%.
Link: https://arxiv.org/abs/1802.06412
====================================================
Generalized kernels of polygons under rotation (David Orden - 16 February, 2018)
In particular, we design efficient algorithms for (i) computing and maintaining $\{0^{o}\}$-${\rm Kernel}_Î¸(P)$ while $Î¸$ varies in $[-\fracÏ{2},\fracÏ{2})$, obtaining the angular intervals where the $\{0^{o}\}$-${\rm Kernel}_Î¸(P)$ is not empty and (ii) for orthogonal polygons $P$, computing the orientation $Î¸\in[0, \fracÏ{2})$ such that the area and/or the perimeter of the $\{0^{o},90^{o}\}$-${\rm Kernel}_Î¸(P)$ are maximum or minimum
Link: https://arxiv.org/abs/1802.05995
====================================================
NtMalDetect: A Machine Learning Approach to Malware Detection Using Native API System Calls (Chan Woo Kim - 19 May, 2018)
Inspired by recent successes in Natural Language Processing (NLP), widely used document classification techniques were assessed in detecting malware by doing such analysis on system calls, which contain useful information about the operation of a program as requests that the program makes of the kernel. This paper shows that Linear Support Vector Machines (SVM) optimized by Stochastic Gradient Descent and the traditional Coordinate Descent on the Wolfe Dual form of the SVM are effective in this approach, achieving a highest of 96% accuracy with 95% recall score
Link: https://arxiv.org/abs/1802.05412
====================================================
Input-Aware Auto-Tuning of Compute-Bound HPC Kernels (Philippe Tillet - 14 February, 2018)
In this paper, we present an input-aware auto-tuning framework for matrix multiplications and convolutions, ISAAC, which uses predictive modeling techniques to drive highly parameterized PTX code templates towards not only hardware-, but also application-specific kernels. Numerical experiments on the NVIDIA Maxwell and Pascal architectures show up to 3x performance gains over both cuBLAS and cuDNN after only a few hours of auto-tuning.
Link: https://arxiv.org/abs/1802.05371
====================================================
GPU implementation of algorithm SIMPLE-TS for calculation of unsteady, viscous, compressible and heat-conductive gas flows (Kiril S. Shterev - 12 February, 2018)
The GPU algorithm was implemented in one kernel for the implicit scheme and two kernels for the explicit scheme. The tests show that overall speedup of AMD Radeon R9 280X is up to 102x compared to Intel Core i5-4690 core and up to 184x compared to Intel Core i7-920 core, while speedup of NVIDIA Tesla M2090 is up to 11x compared to Intel Core i5-4690 core and up to 20x compared to Intel Core i7-920 core. It requires 1[GB] global memory for 5.9 million finite volumes that are two times less compared to C++ CPU code
Link: https://arxiv.org/abs/1802.04243
====================================================
Learning Multiple Levels of Representations with Kernel Machines (Shiyu Duan - 1 April, 2018)
With a single fixed generic kernel for each layer and two layers in total, our model compares favorably with state-of-the-art multiple kernel learning algorithms using significantly more kernels and popular deep architectures on widely used classification benchmarks.
Link: https://arxiv.org/abs/1802.03774
====================================================
Disturbance Grassmann Kernels for Subspace-Based Learning (Junyuan Hong - 17 June, 2018)
Experiments on action data indicate that the proposed kernels perform better compared to state-of-the-art subspace-based methods, even in a worse environment.
Link: https://arxiv.org/abs/1802.03517
====================================================
A Multi-Kernel Multi-Code Polar Decoder Architecture (Gabriele Coppolino - 2 February, 2018)
Most research on polar codes has focused on codes constructed from a $2\times2$ polarization matrix, called binary kernel: codes constructed from binary kernels have code lengths that are bound to powers of $2$. A few recent works have proposed construction methods based on multiple kernels of different dimensions, not only binary ones, allowing code lengths different from powers of $2$. The decoder can achieve frequency of more than $1$ GHz in $65$ nm CMOS technology, and a throughput of $615$ Mb/s. The area occupation ranges between $0.11$ mm$^2$ for $N_{max}=256$ and $2.01$ mm$^2$ for $N_{max}=4096$. Implementation results show an unprecedented degree of flexibility: with $N_{max}=4096$, up to $55$ code lengths can be decoded with the same hardware, along with any kernel sequence and code rate.
Link: https://arxiv.org/abs/1802.00580
====================================================
Fusarium Damaged Kernels Detection Using Transfer Learning on Deep Neural Network Architecture (MÃ¡rcio Nicolau - 31 January, 2018)
The accuracy of the proposed methodology is equivalent to ones using HSI methodology $(81\%-91\%)$ used for the same task, but with the advantage of being independent on special equipment to classify wheat kernel for FHB symptoms.
Link: https://arxiv.org/abs/1802.00030
====================================================
A CNN-based Spatial Feature Fusion Algorithm for Hyperspectral Imagery Classification (Alan J. X. Guo - 7 August, 2018)
The spectral-spatial feature is obtained by a convolutional operation between the estimated kernel and the corresponding spectral features within a neighborhood. Without increasing the number of training samples or involving pixel patches at the training stage, the CSFF framework achieves the state-of-the-art by declining $20\%-50\%$ classification failures in experiments on three well-known hyperspectral images.
Link: https://arxiv.org/abs/1801.10355
====================================================
A Deep Ranking Model for Spatio-Temporal Highlight Detection from a 360 Video (Youngjae Yu - 31 January, 2018)
We propose a novel deep ranking model named as Composition View Score (CVS) model, which produces a spherical score map of composition per video segment, and determines which view is suitable for highlight via a sliding window kernel at inference. To evaluate the proposed framework, we perform experiments on the Pano2Vid benchmark dataset and our newly collected 360 degree video highlight dataset from YouTube and Vimeo. We also show that our model is 16 times faster at inference than AutoCam, which is one of the first summarization algorithms of 360 degree videos
Link: https://arxiv.org/abs/1801.10312
====================================================
A Theoretical Investigation of Graph Degree as an Unsupervised Normality Measure (Caglar Aytekin - 5 February, 2018)
We show that our analyses guide us to choose fully-connected graphs whose edge weights are calculated via universal kernels. We show that a simple graph degree based unsupervised anomaly detection method with the above properties, achieves higher accuracy compared to other unsupervised anomaly detection methods on average over 10 widely used datasets
Link: https://arxiv.org/abs/1801.07889
====================================================
Combinatorial framework for planning in geological exploration (Mark Sh. Levin - 22 January, 2018)
Aggregation of the obtained modular alternatives (stage 6) is based on detection of a alternatives 'kernel' and its extension by addition of elements (multiple choice model)
Link: https://arxiv.org/abs/1801.07229
====================================================
Time series kernel similarities for predicting Paroxysmal Atrial Fibrillation from ECGs (Filippo Maria Bianchi - 4 April, 2018)
We consider different approaches to perform classification in the original space of the multi-variate time series and in an embedding space, defined by the kernel similarity measure. We achieve a classification accuracy comparable with state of the art methods, with the additional advantage of detecting the PAF onset up to 15 minutes in advance.
Link: https://arxiv.org/abs/1801.06845
====================================================
EnKCF: Ensemble of Kernelized Correlation Filters for High-Speed Object Tracking (Burak Uzkent - 20 January, 2018)
To develop a single-target tracking algorithm with these properties, we propose an ensemble of the kernelized correlation filters (KCF), we call it EnKCF. Experimental results showed that the performance of ours is, on average, 70.10% for precision at 20 pixels, 53.00% for success rate for the OTB100 data, and 54.50% and 40.2% for the UAV123 data. Experimental results showed that our method is better than other high-speed trackers over 5% on precision on 20 pixels and 10-20% on AUC on average. Moreover, our implementation ran at 340 fps for the OTB100 and at 416 fps for the UAV123 dataset that is faster than DCF (292 fps) for the OTB100 and KCF (292 fps) for the UAV123
Link: https://arxiv.org/abs/1801.06729
====================================================
On the Kernel of $\mathbb{Z}_{2^s}$-Linear Hadamard Codes (Cristina FernÃ¡ndez-CÃ³rdoba - 16 January, 2018)
In this paper, the kernel of $\mathbb{Z}_{2^s}$-linear Hadamard codes and its dimension are established for $s > 2$. The exact amount of nonequivalent such codes are given up to $t=11$ for any $s\geq 2$, by using also the rank and, in some cases, further computations.
Link: https://arxiv.org/abs/1801.05189
====================================================
Enlarging Context with Low Cost: Efficient Arithmetic Coding with Trimmed Convolution (Mu Li - 3 July, 2018)
As for trimmed convolution, the convolutional kernels are specially trimmed to respect the compression order and context dependency of the input symbols. Furthermore, to speed up the decoding process, a slope TCAE model is presented to divide the codes from a 3D code map into several blocks and remove the dependency between the codes inner one block for parallel decoding, which can 60x speed up the decoding process
Link: https://arxiv.org/abs/1801.04662
====================================================
Effect of Meltdown and Spectre Patches on the Performance of HPC Applications (Nikolay A. Simakov - 16 January, 2018)
To study this we use the application kernel module of XDMoD to test the performance before and after the application of the vulnerability patches. The results show that although some specific functions can have performance decreased by as much as 74%, the majority of individual metrics indicates little to no decrease in performance
Link: https://arxiv.org/abs/1801.04329
====================================================
Speculose: Analyzing the Security Implications of Speculative Execution in CPUs (Giorgi Maisuradze - 12 January, 2018)
To demonstrate the practicality of such attacks, we show how a user-space adversary can probe for kernel pages to reliably break kernel-level ASLR in Linux in under three seconds and reduce the Windows 10 KASLR entropy by 18~bits in less than a second.
Link: https://arxiv.org/abs/1801.04084
====================================================
DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features (Abelino Jimenez - 8 January, 2018)
These features, typically of thousands of dimensions, are classified in state of the art approaches using kernel machines, such as the Support Vector Machines (SVM). We compared their performance using an SVM in the context of the DCASE Task 1 - Acoustic Scene Classification. Experiments show that both, input and random features outperformed the DCASE baseline by an absolute 4%. Hence, random features could be employed by state of the art approaches to compute low-storage features and perform faster kernel computations.
Link: https://arxiv.org/abs/1801.02690
====================================================
Graph Memory Networks for Molecular Activity Prediction (Trang Pham - 26 January, 2018)
Machine learning techniques such as kernel methods and random forests have been successful for this task. We demonstrate the effectiveness of the proposed model for separately and jointly training on more than 100K measurements, spanning across 9 BioAssay activity tests.
Link: https://arxiv.org/abs/1801.02622
====================================================
Optimizing TCP Loss Recovery Performance Over Mobile Data Networks (Ke Liu - 8 January, 2018)
We implemented and modularized the proposed algorithms in the Linux kernel thus they can plug-and-play with the existing TCP loss recovery algorithms easily. Using emulated experiments we showed that, compared to the existing TCP loss recovery algorithms, the proposed optimization algorithms improve the bandwidth efficiency by up to 133% and completely mitigate RTT spikes, i.e., over 50% RTT reduction, over the loss recovery phase.
Link: https://arxiv.org/abs/1801.02534
====================================================
Meltdown (Moritz Lipp - 3 January, 2018)
Meltdown exploits side effects of out-of-order execution on modern processors to read arbitrary kernel-memory locations including personal data and passwords. The attack works on different Intel microarchitectures since at least 2010 and potentially other processors are affected
Link: https://arxiv.org/abs/1801.01207
====================================================
Sketching for Kronecker Product Regression and P-splines (Huaian Diao - 26 December, 2017)
We take TensorSketch outside of the context of polynomials kernels, and show its utility in applications in which the underlying design matrix is a Kronecker product of smaller matrices. That is, TensorSketch only provides input sparsity time for Kronecker product regression with respect to the $2$-norm. We show how to solve Kronecker product regression with respect to the $1$-norm in time sublinear in the time required for computing the Kronecker product, as well as for more general $p$-norms.
Link: https://arxiv.org/abs/1712.09473
====================================================
Kernel Regression with Sparse Metric Learning (Rongqing Huang - 24 December, 2017)
Our work is the first to combine kernel regression with sparse metric learning. To verify the effectiveness of the proposed method, it is evaluated on 19 data sets for regression
Link: https://arxiv.org/abs/1712.09001
====================================================
Blind Image Deblurring via Reweighted Graph Total Variation (Yuanchao Bai - 24 December, 2017)
Experimental results show that our algorithm can robustly estimate the blur kernel with large kernel size, and the reconstructed sharp image is competitive against the state-of-the-art methods.
Link: https://arxiv.org/abs/1712.08877
====================================================
Detection and classification of masses in mammographic images in a multi-kernel approach (Sidney Marlon Lopes de Lima - 19 December, 2017)
Classification was performed by using SVM and ELM networks with modified kernels, in order to optimize accuracy rates, reaching 94.11%. Our proposal was 50 times higher than the ratio obtained using the best method of the state-of-the-art
Link: https://arxiv.org/abs/1712.07116
====================================================
Distributed learning of CNNs on heterogeneous CPU/GPU architectures (Jose Marques - 7 December, 2017)
For the CIFAR-10 dataset, using a CNN with two convolutional layers, and $500$ and $1500$ kernels, respectively, best speedups achieve $3.28\times$ using four CPUs and $2.45\times$ with three GPUs. Modern imaging datasets, larger and more complex than CIFAR-10 will certainly require more than $60$-$90$\% of processing time calculating convolutions, and speedups will tend to increase accordingly.
Link: https://arxiv.org/abs/1712.02546
====================================================
Deep learning analysis of breast MRIs for prediction of occult invasive disease in ductal carcinoma in situ (Zhe Zhu - 28 November, 2017)
Results: The best classification performance was obtained using the deep features approach with GoogleNet model pre-trained on ImageNet as the feature extractor and a polynomial kernel SVM used as the classifier (AUC = 0.70, 95% CI: 0.58- 0.79). For the transfer learning based approach, the highest AUC obtained was 0.53 (95% CI: 0.41-0.62)
Link: https://arxiv.org/abs/1711.10577
====================================================
Scalable and Compact 3D Action Recognition with Approximated RBF Kernel Machines (Jacopo Cavazza - 28 November, 2017)
Our approximation is directly inspired by the exact feature map that is induced by an RBF Gaussian kernel but, unlike the latter, it is finite dimensional and very compact. In a broad experimental validation, we assess the superiority of our approximation in terms of 1) ease and speed of training, 2) compactness of the model, and 3) improvements with respect to the state-of-the-art performance.
Link: https://arxiv.org/abs/1711.10290
====================================================
Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? (Kensho Hara - 1 April, 2018)
The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively
Link: https://arxiv.org/abs/1711.09577
====================================================
DeepDeblur: Fast one-step blurry face images restoration (Lingxiao Wang - 26 November, 2017)
Unlike previous deep learning involved methods that can only handle a single blur kernel at one time, our network is trained on totally random and numerous training sample pairs to deal with the variances due to different blur kernels in practice. Moreover, the proposed method shows significant improvement in face recognition accuracy along with increasing running speed by more than 100 times.
Link: https://arxiv.org/abs/1711.09515
====================================================
Invariance of Weight Distributions in Rectified MLPs (Russell Tsuchida - 31 May, 2018)
Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having $0$ mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights
Link: https://arxiv.org/abs/1711.09090
====================================================
Efficient and Invariant Convolutional Neural Networks for Dense Prediction (Hongyang Gao - 24 November, 2017)
The kernel rotation can be achieved on kernels of 3 $\times$ 3, while kernel flip can be applied on kernels of any size
Link: https://arxiv.org/abs/1711.09064
====================================================
Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions (Bichen Wu - 3 December, 2017)
However, spatial convolutions are expensive in terms of model size and computation, both of which grow quadratically with respect to kernel size. To demonstrate the operation's efficacy, we replace ResNet's 3x3 convolutions with shift-based modules for improved CIFAR10 and CIFAR100 accuracy using 60% fewer parameters; we additionally demonstrate the operation's resilience to parameter reduction on ImageNet, outperforming ResNet family members
Link: https://arxiv.org/abs/1711.08141
====================================================
Revisiting Connected Vertex Cover: FPT Algorithms and Lossy Kernels (R. Krithika - 21 November, 2017)
In a recent paper, Lokshtanov et al.[STOC 2017], have shown an $Î±$-approximate kernel for the problem for every $Î±> 1$, in the framework of approximate or lossy kernelization
Link: https://arxiv.org/abs/1711.07872
====================================================
Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data (Davide Bacciu - 21 November, 2017)
Experimental results show that the proposed approach can outperform state-of-the-art syntactic kernels as well as generative kernels built on the same probabilistic model as the HTN.
Link: https://arxiv.org/abs/1711.07784
====================================================
A Fusion-based Gender Recognition Method Using Facial Images (Benyamin Ghojogh - 17 November, 2017)
The third framework uses lower part of faces as input and classifies them using kernel SVM. This method obtains recognition rate of 94% for neutral faces of FEI face dataset, which is equal to state-of-the-art rate for this dataset.
Link: https://arxiv.org/abs/1711.06451
====================================================
Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints for Action Recognition (Jian Liu - 3 August, 2018)
We propose "Skepxels" a spatio-temporal representation for skeleton sequences to fully exploit the "local" correlations between joints using the 2D convolution kernels of CNN. We extend the Inception-ResNet CNN architecture with the proposed method and improve the state-of-the-art accuracy by 4.4% on the large scale NTU human activity dataset. On the medium-sized N-UCLA and UTH-MHAD datasets, our method outperforms the existing results by 5.7% and 9.3% respectively.
Link: https://arxiv.org/abs/1711.05941
====================================================
Practical Whole-System Provenance Capture (Thomas Pasquier - 14 November, 2017)
CamFlow addresses these shortcoming by: 1) leveraging the latest kernel design advances to achieve efficiency; 2) using a self-contained, easily maintainable implementation relying on a Linux Security Module, NetFilter, and other existing kernel facilities; 3) providing a mechanism to tailor the captured provenance data to the needs of the application; and 4) making it easy to integrate provenance across distributed systems
Link: https://arxiv.org/abs/1711.05296
====================================================
Smaller parameters for vertex cover kernelization (Eva-Maria C. Hols - 13 November, 2017)
Our starting point is a recent paper due to Fomin and StrÃ¸mme [WG 2016] who gave a kernel with $\mathcal{O}(|X|^{12})$ vertices when $X$ is a vertex set such that each connected component of $G-X$ contains at most one cycle, i.e., $X$ is a modulator to a pseudoforest
Link: https://arxiv.org/abs/1711.04604
====================================================
Domain-Specific Acceleration and Auto-Parallelization of Legacy Scientific Code in FORTRAN 77 using Source-to-Source Compilation (Wim Vanderbauwhede - 13 November, 2017)
In this paper we present a source to source compilation approach with whole-program analysis to automatically transform single-threaded FORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized kernels.
Link: https://arxiv.org/abs/1711.04471
====================================================
Kernelized Hashcode Representations for Biomedical Relation Extraction (Sahil Garg - 17 August, 2018)
Kernel methods have produced state-of-the-art results for a number of NLP tasks such as relation extraction, but suffer from poor scalability due to the high cost of computing kernel similarities between discrete natural language structures. state-of-the-art classifiers, along with drastic (orders-of-magnitude) speedup compared to conventional kernel methods.
Link: https://arxiv.org/abs/1711.04044
====================================================
Fast matrix-free evaluation of discontinuous Galerkin finite element operators (Martin Kronbichler - 9 November, 2017)
The sum factorization kernels are optimized by vectorization over several cells and faces and an even-odd decomposition of the one-dimensional compute kernels. In isolation our implementation then reaches up to 60\% of arithmetic peak on Intel Haswell and Broadwell processors and up to 50\% of arithmetic peak on Intel Knights Landing. Our performance analysis shows that the results are often within 10\% of the available memory bandwidth for the proposed implementation, with the exception of the Cartesian mesh case where the cost of gather operations and MPI communication are more substantial.
Link: https://arxiv.org/abs/1711.03590
====================================================
Fast Integral Histogram Computations on GPU for Real-Time Video Analytics (Mahdieh Poostchi - 6 November, 2017)
Our kernels perform cumulative sums on row and column histograms in a cross-weave or wavefront scan order, use different data organization and scheduling methods that is shown to critically affect utilization of GPU resources (cores and shared memory). The tiled integral histogram using a diagonal wavefront scan has the best performance of about 300.4 frames/sec for 640 x 480 images and 32 bins with a speedup factor of about 120 using GTX Titan X graphics card compared to a single threaded sequential CPU implementation. Mapping integral histogram bins computations onto multiple GPUs enables us to process 32 giga bytes integral histogram data (of 64MB Image and 128 bins) with a frame rate of 0.73 Hz and speedup factor of 153X over single-threaded CPU implementation and the speedup of 45X over 16-threaded CPU implementation.
Link: https://arxiv.org/abs/1711.01919
====================================================
Comparison of Parallelisation Approaches, Languages, and Compilers for Unstructured Mesh Algorithms on GPUs (G. D. Balogh - 6 November, 2017)
Results of this work show how clang's CUDA compiler frequently outperform NVIDIA's nvcc, performance issues with directive-based approaches on complex kernels, and OpenMP 4 support maturing in clang and XL; currently around 10% slower than CUDA.
Link: https://arxiv.org/abs/1711.01845
====================================================
Gaussian Kernel in Quantum Paradigm (Arit Kumar Bishwas - 4 November, 2017)
In this paper, we have demonstrated a quantum version of the Gaussian kernel and analyzed its complexity, which is O(Îµ^(-1)logN) with N-dimensional instances and an accuracy Îµ
Link: https://arxiv.org/abs/1711.01464
====================================================
Shift-Invariant Kernel Additive Modelling for Audio Source Separation (Delia Fano Yela - 16 February, 2018)
While most state-of-the-art approaches are supervised methods trained on large datasets, interest in non-data-driven approaches such as Kernel Additive Modelling (KAM) remains high due to their interpretability and adaptability
Link: https://arxiv.org/abs/1711.00351
====================================================
Kernel Graph Convolutional Neural Networks (Giannis Nikolentzos - 7 September, 2018)
We address this challenge by using graph kernels to embed meaningful local neighborhoods of the graphs in a continuous vector space. With limited parameter tuning, our approach outperforms strong baselines on 7 out of 10 benchmark datasets.
Link: https://arxiv.org/abs/1710.10689
====================================================
Convolutional neural networks on irregular domains through approximate translations on inferred graphs (Bastien Pasdeloup - 27 October, 2017)
In more details, we introduce a three-step methodology to create convolutional layers that are adapted to the signals to process: 1) From a training set of signals, infer a graph representing the topology on which they evolve; 2) Identify translation operators in the vertex domain; 3) Emulate a convolution operator by translating a localized kernel on the graph
Link: https://arxiv.org/abs/1710.10035
====================================================
Trace norm regularization and faster inference for embedded speech recognition RNNs (Markus Kliegl - 6 February, 2018)
For speedup, we enable faster inference on ARM processors through new open sourced kernels optimized for small batch sizes, resulting in 3x to 7x speed ups over the widely used gemmlowp library
Link: https://arxiv.org/abs/1710.09026
====================================================
On Parallel Solution of Sparse Triangular Linear Systems in CUDA (Ruipeng Li - 13 October, 2017)
Solving linear systems with sparse triangular structured matrices is another important sparse kernel as demanded by a variety of scientific and engineering applications such as sparse linear solvers. Numerical results have indicated that the CUDA implementations of the proposed algorithms can outperform the state-of-the-art solvers in cuSPARSE by a factor of up to $2.6$ for structured model problems and general sparse matrices.
Link: https://arxiv.org/abs/1710.04985
====================================================
What Would a Graph Look Like in This Layout? A Machine Learning Approach to Large Graph Visualization (Oh-Hyun Kwon - 11 October, 2017)
Also, our graph kernels outperform the state-of-the-art ones in both time and accuracy
Link: https://arxiv.org/abs/1710.04328
====================================================
On Data-Driven Saak Transform (C. -C. Jay Kuo - 14 October, 2017)
The Saak transform consists of three steps: 1) building the optimal linear subspace approximation with orthonormal bases using the second-order statistics of input vectors, 2) augmenting each transform kernel with its negative, 3) applying the rectified linear unit (ReLU) to the transform output. The integration of Steps 2 and 3 is powerful since they resolve the sign confusion problem, remove the rectification loss and allow a straightforward implementation of the inverse Saak transform at the same time
Link: https://arxiv.org/abs/1710.04176
====================================================
Deep Convolutional Neural Networks as Generic Feature Extractors (Lars Hertel - 6 October, 2017)
We then maintained the learned convolution kernels and only retrained the classification part on different datasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100, compared to the previous state-of-the-art result of 65.43 %
Link: https://arxiv.org/abs/1710.02286
====================================================
Remote Sensing Image Classification with Large Scale Gaussian Processes (Pablo Morales-Alvarez - 3 October, 2017)
A popular kernel classifier is the Gaussian process classifier (GPC), since it approaches the classification problem with a solid probabilistic treatment, thus yielding confidence intervals for the predictions as well as very competitive results to state-of-the-art neural networks and support vector machines
Link: https://arxiv.org/abs/1710.00575
====================================================
Improving Dermoscopic Image Segmentation with Enhanced Convolutional-Deconvolutional Networks (Yading Yuan - 27 September, 2017)
In this paper, we extended our previous work by developing a deeper network architecture with smaller kernels to enhance its discriminant capacity. We extensively evaluated our method on the ISBI 2017 skin lesion segmentation challenge. By training with the 2000 challenge training images, our method achieved an average Jaccard Index (JA) of 0.765 on the 600 challenge testing images, which ranked itself in the first place in the challenge
Link: https://arxiv.org/abs/1709.09780
====================================================
Ensemble Classifier for Eye State Classification using EEG Signals (Ali Al-Taei - 25 September, 2017)
The suggested (ensemble system) method based on a voting algorithm with two kernels: random forest (RF) and Kstar classification methods. For instance, the suggested method's performance was 97.27% accuracy and 0.13 MAE.
Link: https://arxiv.org/abs/1709.08590
====================================================
Learned Features are better for Ethnicity Classification (Inzamam Anwar - 31 October, 2017)
The proposed method makes use of a pre trained Convolutional Neural Network (CNN) to extract the features and then Support Vector Machine (SVM) with linear kernel is used as a classifier. Average classification accuracy over all databases is 98.28%, 99.66% and 99.05% for Asian, African-American and Caucasian respectively.
Link: https://arxiv.org/abs/1709.07429
====================================================
Cystoid macular edema segmentation of Optical Coherence Tomography images using fully convolutional neural networks and fully connected CRFs (Fangliang Bai - 15 September, 2017)
A segmentation accuracy of $0.61\pm 0.21$ (Dice coefficient) was achieved, with respect to the ground truth, which compares favourably with the previous state-of-the-art that used a kernel regression based method ($0.51\pm 0.34$)
Link: https://arxiv.org/abs/1709.05324
====================================================
Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection (Hongyang Li - 8 June, 2018)
One advantage of MAD is that the learned weights enforced on each feature channel is predicted on-the-fly based on the input context, which is more suitable than the fixed enforcement of a convolutional kernel. Experimental results on three datasets, including PASCAL VOC 2007, ImageNet DET, MS COCO, demonstrate the effectiveness of our proposed algorithm over other state-of-the-arts, in terms of average recall (AR) for region proposal and average precision (AP) for object detection.
Link: https://arxiv.org/abs/1709.04347
====================================================
Parallelizing Linear Recurrent Neural Nets Over Sequence Length (Eric Martin - 22 February, 2018)
We develop a parallel linear recurrence CUDA kernel and show that it can be applied to immediately speed up training and inference of several state of the art RNN architectures by up to 9x
Link: https://arxiv.org/abs/1709.04057
====================================================
Amplifying Inter-message Distance: On Information Divergence Measures in Big Data (Rui She - 12 September, 2017)
In addition, we design a M-I divergence estimation algorithm by means of the ensemble estimator of the proposed weight kernel estimators, which can improve the convergence of mean squared error from ${O(\varGamma^{-j/d})}$ to ${O(\varGamma^{-1})}$ $({j\in (0,d]})$
Link: https://arxiv.org/abs/1709.03690
====================================================
Gaussian Quadrature for Kernel Features (Tri Dao - 31 January, 2018)
The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(Îµ^{-2})$ samples are required to achieve an approximation error of at most $Îµ$. We show that deterministic feature maps can be constructed, for any $Î³> 0$, to achieve error $Îµ$ with $O(e^{e^Î³} + Îµ^{-1/Î³})$ samples as $Îµ$ goes to 0. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features.
Link: https://arxiv.org/abs/1709.02605
====================================================
Fastron: An Online Learning-Based Model and Active Learning Strategy for Proxy Collision Detection (Nikhil Das - 7 September, 2017)
The Fastron allows iterative updates to account for a changing environment through a combination of a novel formulation of the kernel perceptron learning algorithm and an active learning strategy. Our simulations on a 7 degree-of-freedom arm indicate that proxy collision checks may be performed at least 2 times faster than an efficient polyhedral collision checker and at least 8 times faster than an efficient high-precision collision checker
Link: https://arxiv.org/abs/1709.02316
====================================================
Rotational Subgroup Voting and Pose Clustering for Robust 3D Object Recognition (Anders Glent Buch - 7 September, 2017)
Kernel density estimation allows combining the set of votes efficiently to determine a full 6 DoF candidate pose between the models
Link: https://arxiv.org/abs/1709.02142
====================================================
A Compact Kernel Approximation for 3D Action Recognition (Jacopo Cavazza - 4 October, 2017)
A kernel machine feed with such feature is an effective paradigm for 3D action recognition, yielding state-of-the-art results
Link: https://arxiv.org/abs/1709.01695
====================================================
A Non-Convex Optimization Technique for Sparse Blind Deconvolution -- Initialization Aspects and Error Reduction Properties (Aniruddha Adiga - 11 October, 2017)
The columns of the linear convolution matrix form a Riesz basis with the tightness of the Riesz bounds determined by the autocorrelation of the blur kernel. Employing a Bayesian framework results in a non-convex, non-smooth cost function consisting of an $\ell_2$ data-fidelity term and a sparsity promoting $\ell_p$-norm ($0 \le p \le 1$) regularizer
Link: https://arxiv.org/abs/1708.07370
====================================================
D3NOC: Dynamic Data-Driven Network On Chip in Photonic Electronic Hybrids (Armin Mehrabian - 22 August, 2017)
We evaluate the performance and power of our system against kernels from NAS Parallel Benchmark (NPB) in addition to some synthetically generated traffic. In comparison to a 16x16 base electrical mesh, D3NOC shows up to 89% latency and 67% dynamic power net improvements beyond overhead-corrected performance
Link: https://arxiv.org/abs/1708.06721
====================================================
Pillar Networks++: Distributed non-parametric deep and wide networks (Biswa Sengupta - 18 August, 2017)
In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset)
Link: https://arxiv.org/abs/1708.06250
====================================================
Scalable Kernelization for Maximum Independent Sets (Demian Hespe - 21 August, 2017)
Finally, we show that our kernelization algorithm can be used to accelerate existing state-of-the-art heuristic algorithms, allowing us to find larger independent sets faster on large real-world networks and synthetic instances.
Link: https://arxiv.org/abs/1708.06151
====================================================
Shapelet-based Sparse Representation for Landcover Classification of Hyperspectral Images (Ribana Roscher - 20 August, 2017)
They illustrate that our proposed approach shows superior results in comparison to sparse representation-based classifiers that use only limited spatial information and behaves competitively with or better than state-of-the-art classifiers utilizing spatial information and kernelized sparse representation-based classifiers.
Link: https://arxiv.org/abs/1708.05974
====================================================
Learning spectro-temporal features with 3D CNNs for speech emotion recognition (Jaebok Kim - 14 August, 2017)
We found that 1) shallow temporal and moderately deep spectral kernels of a homogeneous architecture are optimal for the task; and 2) our 3D CNNs are more effective for spectro-temporal feature learning compared to other methods
Link: https://arxiv.org/abs/1708.05071
====================================================
Graph Classification with 2D Convolutional Neural Networks (Antoine Jean-Pierre Tixier - 12 February, 2018)
Experiments reveal that our method is more accurate than state-of-the-art graph kernels and graph CNNs on 4 out of 6 real-world datasets (with and without continuous node attributes), and close elsewhere
Link: https://arxiv.org/abs/1708.02218
====================================================
On the Effect of Semantically Enriched Context Models on Software Modularization (Amir Saeidi - 4 August, 2017)
We try to overcome this problem by introducing context models for source code identifiers to obtain a semantic kernel, which can be used for both deriving the topics that run through the system as well as their clustering. We have applied our approach to 10 medium-sized open source Java projects, and show that by introducing contexts for identifiers, the quality of the modularization of the software systems is improved. In some cases, the authoritativeness of decompositions is improved by 67%
Link: https://arxiv.org/abs/1708.01680
====================================================
The All-Paths and Cycles Graph Kernel (P. -L. Giscard - 4 August, 2017)
Extensive evaluations on a variety of graph datasets demonstrate that the all-paths and cycles kernel has superior performance to the shortest-path kernel and state-of-the-art performance overall.
Link: https://arxiv.org/abs/1708.01410
====================================================
When Kernel Methods meet Feature Learning: Log-Covariance Network for Action Recognition from Skeletal Data (Jacopo Cavazza - 3 August, 2017)
However, the current state-of-the-art is contended between to different paradigms: kernel-based methods and feature learning with (recurrent) neural networks. We validate this hypothesis in a broad experimental analysis over 6 publicly available datasets.
Link: https://arxiv.org/abs/1708.01022
====================================================
On the Parameterized Complexity of Contraction to Generalization of Trees (Akanksha Agrawal - 2 August, 2017)
Inspired by the negative result for the kernelization, we design a lossy kernel for $\mathbb{T}_\ell$-Contraction of size $ \mathcal{O}([k(k + 2\ell)] ^{(\lceil {\fracÎ±{Î±-1}\rceil + 1)}})$.
Link: https://arxiv.org/abs/1708.00622
====================================================
Multiscale Co-Design Analysis of Energy, Latency, Area, and Accuracy of a ReRAM Analog Neural Training Accelerator (Matthew J. Marinella - 16 February, 2018)
This work presents a detailed design using a state of the art 14/16 nm PDK for of an analog crossbar circuit block designed to process three key kernels required in training and inference of neural networks. It is shown that the analog accelerator has a 270x energy and 540x latency advantage over a similar block utilizing only digital ReRAM and takes only 11 fJ per multiply and accumulate (MAC)
Link: https://arxiv.org/abs/1707.09952
====================================================
Lossy kernels for connected distance-$r$ domination on nowhere dense graph classes (Sebastian Siebertz - 31 July, 2017)
For $Î±\colon\mathbb{N}\rightarrow\mathbb{R}$, an $Î±$-approximate bi-kernel is a polynomial-time algorithm that takes as input an instance $(I, k)$ of a problem $Q$ and outputs an instance $(I',k')$ of a problem $Q'$ of size bounded by a function of $k$ such that, for every $c\geq 1$, a $c$-approximate solution for the new instance can be turned into a $c\cdotÎ±(k)$-approximate solution of the original instance in polynomial time. We prove that for every nowhere dense class of graphs, every $Î±>1$ and $r\in\mathbb{N}$ there exists a polynomial $p$ (whose degree depends only on $r$ while its coefficients depend on $Î±$) such that the connected distance-$r$ dominating set problem with parameter $k$ admits an $Î±$-approximate bi-kernel of size $p(k)$
Link: https://arxiv.org/abs/1707.09819
====================================================
CUDAMPF++: A Proactive Resource Exhaustion Scheme for Accelerating Homologous Sequence Search on CUDA-enabled GPU (Hanyu Jiang - 30 July, 2017)
For MSV (SSV) kernels, the peak performance of the CUDAMPF++ is 283.9 (471.7) GCUPS on a single K40 GPU, and impressive speedups ranging from 1.x (1.7x) to 168.3x (160.7x) are achieved over the CPU-based implementation (16 cores, 32 threads).
Link: https://arxiv.org/abs/1707.09683
====================================================
Can string kernels pass the test of time in Native Language Identification? (Radu Tudor Ionescu - 4 August, 2017)
The goal of this paper is to demonstrate that our shallow and simple approach based on string kernels (with minor improvements) can pass the test of time and reach state-of-the-art performance in the 2017 NLI shared task, despite the recent advances in natural language processing. Using only the data provided by the organizers for training our models, we have reached a macro F1 score of 86.95% in the closed essay track, a macro F1 score of 87.55% in the closed speech track, and a macro F1 score of 93.19% in the closed fusion track
Link: https://arxiv.org/abs/1707.08349
====================================================
Multi-kernel learning of deep convolutional features for action recognition (Biswa Sengupta - 12 November, 2017)
We combine multi-kernels based support-vector-machines (SVM) with a multi-stream deep convolutional neural network to achieve close to state-of-the-art performance on a 51-class activity recognition problem (HMDB-51 dataset); this specific dataset has proved to be particularly challenging for deep neural networks due to the heterogeneity in camera viewpoints, video quality, etc
Link: https://arxiv.org/abs/1707.06923
====================================================
Improved Kernels and Algorithms for Claw and Diamond Free Edge Deletion Based on Refined Observations (Wenjun Li - 21 July, 2017)
Based on some refined observations, we propose a kernel of $O(k^3)$ vertices and $O(k^4)$ edges, significantly improving the previous kernel of $O(k^{12})$ vertices and $O(k^{24})$ edges
Link: https://arxiv.org/abs/1707.06779
====================================================
Reconfiguration on nowhere dense graph classes (Sebastian Siebertz - 10 September, 2018)
We prove that for every nowhere dense class of graphs and for every integer $r\geq 1$ there exists a polynomial $p_r$ such that the reconfiguration variants of the distance-$r$ independent set problem and the distance-$r$ dominating set problem admit kernels of size $p_r(k)$. If $k$ is equal to the size of a minimum distance-$r$ dominating set, then for any fixed $Îµ>0$ we even obtain a kernel of almost linear size $\mathcal{O}(k^{1+Îµ})$. We then prove that if a class $\mathcal{C}$ is somewhere dense and closed under taking subgraphs, then for some value of $r\geq 1$ the reconfiguration variants of the above problems on $\mathcal{C}$ are $\mathsf{W}[1]$-hard (and in particular we cannot expect the existence of kernelization algorithms)
Link: https://arxiv.org/abs/1707.06775
====================================================
A Novel Space-Time Representation on the Positive Semidefinite Con for Facial Expression Recognition (Anis Kacem - 20 July, 2017)
Specifically, our approach involves three steps: 1) facial landmarks are first mapped into the Riemannian manifold of positive semidefinite matrices of rank 2, to build time-parameterized trajectories; 2) a temporal alignment is performed on the trajectories, providing a geometry-aware (dis-)similarity measure between them; 3) finally, pairwise proximity function SVM (ppfSVM) is used to classify them, incorporating the latter (dis-)similarity measure into the kernel function
Link: https://arxiv.org/abs/1707.06440
====================================================
graph2vec: Learning Distributed Representations of Graphs (Annamalai Narayanan - 17 July, 2017)
Our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels.
Link: https://arxiv.org/abs/1707.05005
====================================================
Non-Linear Subspace Clustering with Learned Low-Rank Kernels (Pan Ji - 19 September, 2017)
We evaluate the proposed method extensively on both motion segmentation and image clustering benchmarks, and obtain superior results, outperforming the kernel subspace clustering method that uses standard kernels[Patel 2014] and other state-of-the-art linear subspace clustering methods.
Link: https://arxiv.org/abs/1707.04974
====================================================
Feedback Vertex Set Inspired Kernel for Chordal Vertex Deletion (Akanksha Agrawal - 16 July, 2017)
Recently, Jansen and Pilipczuk resolved this question affirmatively by designing a polynomial kernel for CVD of size $O(k^{161}\log^{58}k)$, and asked whether one can design a kernel of size $O(k^{10})$. While we do not completely resolve this question, we design a significantly smaller kernel of size $O(k^{12}\log^{10}k)$, inspired by the $O(k^2)$-size kernel for Feedback Vertex Set
Link: https://arxiv.org/abs/1707.04917
====================================================
Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration (Jeng-Hau Lin - 15 July, 2017)
In this paper, we push the boundaries of hardware-effective CNN design by proposing BCNN with Separable Filters (BCNNw/SF), which applies Singular Value Decomposition (SVD) on BCNN kernels to further reduce computational and storage complexity. Our BCNNw/SF accelerator realizes memory savings of 17% and execution time reduction of 31.3% compared to BCNN with only minor accuracy sacrifices.
Link: https://arxiv.org/abs/1707.04693
====================================================
Pushing the Limits of Online Auto-tuning: Machine Code Optimization in Short-Running Kernels (Fernando Endo - 14 July, 2017)
In a CPU-bound kernel, the average speedups achieved are 1.10 to 1.58 depending on the target micro-architecture, up to 2.53 in the most favourable conditions (all run-time overheads included). In a memory-bound kernel, less favourable to our runtime auto-tuning optimizations, the average speedups are 1.04 to 1.10, up to 1.30 in the best configuration. Despite the short execution times of our benchmarks, the overhead of our runtime auto-tuning is between 0.2 and 4.2% only of the total application execution times. By simulating the CPU-bound application in 11 different CPUs, we showed that, despite the clear hardware disadvantage of In-Order (io) cores vs. Out-of-Order (ooo) equivalent cores, online auto-tuning in io CPUs obtained an average speedup of 1.03 and an energy efficiency improvement of 39~\% over the SIMD reference in ooo CPUs.
Link: https://arxiv.org/abs/1707.04566
====================================================
Circular-shift Linear Network Coding (Hanqi Tang - 25 April, 2018)
Formulated as a special vector linear code over GF($2$), an $L$-dimensional circular-shift linear code of degree $Î´$ restricts its local encoding kernels to be the summation of at most $Î´$ cyclic permutation matrices of size $L$
Link: https://arxiv.org/abs/1707.02163
====================================================
Cooperative Kernels: GPU Multitasking for Blocking Algorithms (Extended Version) (Tyler Sorensen - 6 July, 2017)
We describe a prototype implementation of a cooperative kernel framework implemented in OpenCL 2.0 and evaluate our approach by porting a set of blocking GPU applications to cooperative kernels and examining their performance under multitasking
Link: https://arxiv.org/abs/1707.01989
====================================================
Indefinite Kernel Logistic Regression (Fanghui Liu - 6 July, 2017)
Systematical experiments on multi-modal datasets demonstrate the superiority of the proposed IKLR method over kernel logistic regression with positive definite kernels and other state-of-the-art indefinite learning based algorithms.
Link: https://arxiv.org/abs/1707.01826
====================================================
Adaptive Estimation for Nonlinear Systems using Reproducing Kernel Hilbert Spaces (Parag Bobade - 10 July, 2017)
The central feature of the theory introduced in this paper represents the unknown function as a member of a reproducing kernel Hilbert space (RKHS) and defines a distributed parameter system (DPS) that governs state estimates and estimates of the unknown function. This paper 1) derives sufficient conditions for the existence and stability of the infinite dimensional online estimation problem, 2) derives existence and stability of finite dimensional approximations of the infinite dimensional approximations, and 3) determines sufficient conditions for the convergence of finite dimensional approximations to the infinite dimensional online estimates
Link: https://arxiv.org/abs/1707.01567
====================================================
Copy-move Forgery Detection based on Convolutional Kernel Network (Yaqi Liu - 5 July, 2017)
Numerous experiments are conducted to demonstrate the effectiveness and robustness of the GPU version of Convolutional Kernel Network, and the state-of-the-art performance of the proposed copy-move forgery detection method based on Convolutional Kernel Network.
Link: https://arxiv.org/abs/1707.01221
====================================================
SeqORAM: A Locality-Preserving Write-Only Oblivious RAM (Anrin Chakraborti - 5 July, 2017)
A full-fledged Linux kernel-level implementation of SeqORAM is 100x faster than the state-of-the-art for standard workloads.
Link: https://arxiv.org/abs/1707.01211
====================================================
Lossy Kernels for Connected Dominating Set on Sparse Graphs (Eduard Eiben - 22 February, 2018)
For $Î±> 1$, an $Î±$-approximate (bi-)kernel is a polynomial-time algorithm that takes as input an instance $(I, k)$ of a problem $\mathcal{Q}$ and outputs an instance $(I',k')$ (of a problem $\mathcal{Q}'$) of size bounded by a function of $k$ such that, for every $c\geq 1$, a $c$-approximate solution for the new instance can be turned into a $(c\cdotÎ±)$-approximate solution of the original instance in polynomial time. We prove that for every $Î±>1$, Connected Dominating Set admits a polynomial-size $Î±$-approximate (bi-)kernel on all the aforementioned classes. Our results are in sharp contrast to the kernelization complexity of Connected Dominating Set, which is known to not admit a polynomial kernel even on $2$-degenerate graphs and graphs of bounded expansion, unless $\textsf{NP} \subseteq \textsf{coNP/poly}$. We show that if a class $\mathcal{C}$ is somewhere dense and closed under taking subgraphs, then for some value of $r\in\mathbb{N}$ there cannot exist an $Î±$-approximate bi-kernel for the (Connected) Distance-$r$ Dominating Set problem on $\mathcal{C}$ for any $Î±>1$ (assuming the Gap Exponential Time Hypothesis).
Link: https://arxiv.org/abs/1706.09339
====================================================
Kernelization of Constraint Satisfaction Problems: A Study through Universal Algebra (Victor Lagerkvist - 19 June, 2017)
We also study extensions of this towards SAT and CSP problems with kernels with O(n^c) constraints, c>1, based on embeddings into CSP problems preserved by a k-edge operation, k > c
Link: https://arxiv.org/abs/1706.05941
====================================================
An improved kernel for the cycle contraction problem (Bin Sheng - 18 June, 2017)
[IPEC 2013]} who obtained a linear kernel with at most $6k+6$ vertices
Link: https://arxiv.org/abs/1706.05628
====================================================
Multi-View Kernels for Low-Dimensional Modeling of Seismic Events (Ofir Lindenbaum - 6 June, 2017)
In this work, we propose to use a kernel-fusion based dimensionality reduction framework for generating meaningful seismic representations from raw data. The proposed method is tested on 2023 events that were recorded in Israel and in Jordan
Link: https://arxiv.org/abs/1706.01750
====================================================
Extended Sammon Projection and Wavelet Kernel Extreme Learning Machine for Gait-Based Legitimate User Identification on Smartphones (Muhammad Ahmad - 6 June, 2017)
Through extensive experimentations using overall and one subject cross-validation, we demonstrate that together, time and frequency domain features, are further optimized by applying the "Extended Sammon projection (ESP)" method to train the "wavelet kernel based extreme learning machine (KELM)", as an effective system to identify the legitimate user or an impostor. All experiments were carried out using MATLAB (2014b) on Intel Core i5 CPU 3.20 GHz with 8 GB of RAM with a 64-bit operating system machine.
Link: https://arxiv.org/abs/1706.01739
====================================================
Deep learning for extracting protein-protein interactions from biomedical literature (Yifan Peng - 6 June, 2017)
Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN compares favorably to the state-of-the-art rich-feature and single-kernel based methods. In addition, McDepCNN achieves 24.4% relative improvement in F1-score over the state-of-the-art methods on cross-corpus evaluation and 12% improvement in F1-score over kernel-based methods on "difficult" instances
Link: https://arxiv.org/abs/1706.01556
====================================================
NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps (Alessandro Aimar - 6 March, 2018)
The flexible architecture allows high utilization of available computing resources across kernel sizes ranging from 1x1 to 7x7. NullHop can process up to 128 input and 128 output feature maps per layer in a single pass. Post-synthesis simulations using Mentor Modelsim in a 28nm process with a clock frequency of 500 MHz show that the VGG19 network achieves over 450 GOp/s. By exploiting sparsity, NullHop achieves an efficiency of 368%, maintains over 98% utilization of the MAC units, and achieves a power efficiency of over 3TOp/s/W in a core area of 6.3mm$^2$
Link: https://arxiv.org/abs/1706.01406
====================================================
Context-aware, Adaptive and Scalable Android Malware Detection through Online Learning (extended version) (Annamalai Narayanan - 6 July, 2017)
In order to perform accurate detection, a novel graph kernel that facilitates capturing apps' security-sensitive behaviors along with their context information from dependency graphs is proposed. In a large-scale comparative analysis, CASANDRA outperforms two state-of-the-art techniques on a benchmark dataset achieving 99.23% F-measure. When evaluated with more than 87,000 apps collected in-the-wild, CASANDRA achieves 89.92% accuracy, outperforming existing techniques by more than 25% in their typical batch learning setting and more than 7% when they are continuously retained, while maintaining comparable efficiency.
Link: https://arxiv.org/abs/1706.00947
====================================================
Saving Critical Nodes with Firefighters is FPT (Jayesh Choudhari - 30 May, 2017)
We also demonstrate improved running times on trees and establish that the problem is unlikely to admit a polynomial kernelization (even when restricted to trees). Finally, we consider the spreading model of the firefighting game, a closely related problem, and show that the problem of saving a critical set parameterized by the number of firefighters is W[2]-hard, which contrasts our FPT result for the non-spreading model.
Link: https://arxiv.org/abs/1705.10923
====================================================
Optimizing Memory Efficiency for Convolution Kernels on Kepler GPUs (Xiaoming Chen - 29 May, 2017)
Experimental data based on implementations on Kepler GPUs show that our kernels achieve 5.16X and 35.5% average performance improvement over the latest cuDNN library, for the special case and the general case, respectively.
Link: https://arxiv.org/abs/1705.10591
====================================================
A Unified Optimization Approach for Sparse Tensor Operations on GPUs (Bangtian Liu - 28 May, 2017)
The performance of the proposed unified approach is demonstrated for tensor-based kernels such as the Sparse Matricized Tensor- Times-Khatri-Rao Product (SpMTTKRP) and the Sparse Tensor- Times-Matrix Multiply (SpTTM) and is used in tensor decomposition algorithms. Compared to state-of-the-art work we improve the performance of SpTTM and SpMTTKRP up to 3.7 and 30.6 times respectively on NVIDIA Titan-X GPUs. We implement a CANDECOMP/PARAFAC (CP) decomposition and achieve up to 14.9 times speedup using the unified method over state-of-the-art libraries on NVIDIA Titan-X GPUs.
Link: https://arxiv.org/abs/1705.09905
====================================================
MMD GAN: Towards Deeper Understanding of Moment Matching Network (Chun-Liang Li - 27 November, 2017)
In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.
Link: https://arxiv.org/abs/1705.08584
====================================================
Data-driven Random Fourier Features using Stein Effect (Wei-Cheng Chang - 23 May, 2017)
Approaches using random Fourier features have become increasingly popular [Rahimi and Recht, 2007], where kernel approximation is treated as empirical mean estimation via Monte Carlo (MC) or Quasi-Monte Carlo (QMC) integration [Yang et al., 2014]. A limitation of the current approaches is that all the features receive an equal weight summing to 1
Link: https://arxiv.org/abs/1705.08525
====================================================
Spectral-graph Based Classifications: Linear Regression for Classification and Normalized Radial Basis Function Network (Zhenfang Hu - 13 June, 2017)
It turns out that two classifiers are inherently related to the theory: linear regression for classification (LRC) and normalized radial basis function network (nRBFN), corresponding to linear and nonlinear kernel respectively. Experiments on 14 benchmark data sets show the performance of nRBFN is comparable to that of SVM, whereas the parameter tuning of nRBFN is much easier, leading to reduction of model selection time.
Link: https://arxiv.org/abs/1705.06922
====================================================
Kernel Truncated Regression Representation for Robust Subspace Clustering (Liangli Zhen - 23 May, 2017)
To achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps: 1) projecting the data into a hidden space in which the data can be linearly reconstructed from each other; 2) calculating the globally linear reconstruction coefficients in the kernel space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality, and then achieving clustering by solving a graph Laplacian problem
Link: https://arxiv.org/abs/1705.05108
====================================================
Machine learning methods for multimedia information retrieval (BÃ¡lint ZoltÃ¡n DarÃ³czy - 14 May, 2017)
We reread briefly some theoretical results of learning in Section 2 and reviewed several generative and discriminative models in Section 3 while we described the similarity kernel in Section 4. We examined different aspects of the multimodal image retrieval and classification in Section 5 and suggested methods for identifying quality assessments of Web documents in Section 6. Since the used similarity graphs (Section 4.2) are greatly constrained for computational purposes, we would like to continue work with more complex, evolving and capable graphs and apply for different problems such as capturing the rapid change in the distribution (e.g. The similarity kernel with the proper metrics reaches and in many cases improves over the state-of-the-art. Therefore we may exploit the Fisher kernel in the future over widely used generative models, such as Boltzmann Machines [Hinton et al., 1984], a particular subset, the Restricted Boltzmann Machines and Deep Belief Networks [Hinton et al., 2006]), Latent Dirichlet Allocation [Blei et al., 2003] or Hidden Markov Models [Baum and Petrie, 1966] to name a few.
Link: https://arxiv.org/abs/1705.04964
====================================================
Gabor Filter Assisted Energy Efficient Fast Learning Convolutional Neural Networks (Syed Shakib Sarwar - 12 May, 2017)
We show that the accuracy degradation can be mitigated by partially training the Gabor kernels, for a small fraction of the total training cycles. We evaluated the proposed approach on 4 benchmark applications. We also obtain improvement up to 1.4x in training time, up to 2.23x in storage requirements, and up to 2.2x in memory access energy
Link: https://arxiv.org/abs/1705.04748
====================================================
Rank Vertex Cover as a Natural Problem for Algebraic Compression (Syed Mohammad Meesum - 10 May, 2017)
The question of the existence of a polynomial kernelization of the Vertex Cover Above LP problem has been a longstanding, notorious open problem in Parameterized Complexity. Five years ago, the breakthrough work by Kratsch and Wahlstrom on representative sets has finally answered this question in the affirmative [FOCS 2012]
Link: https://arxiv.org/abs/1705.02822
====================================================
GaKCo: a Fast GApped k-mer string Kernel using COunting (Ritambhara Singh - 18 September, 2017)
String Kernel (SK) techniques, especially those using gapped $k$-mers as features (gk), have obtained great success in classifying sequences like DNA, protein, and text. Experimentally, we observe that GaKCo achieves the same accuracy as the state-of-the-art and outperforms its speed by factors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein (12 datasets), and character-based English text (2 datasets), respectively.
Link: https://arxiv.org/abs/1704.07468
====================================================
A Simple Proof of Fast Polarization (Ido Tal - 24 April, 2017)
It was proved for the binary polarizing $2 \times 2$ kernel by Arikan and Telatar
Link: https://arxiv.org/abs/1704.07179
====================================================
HPTT: A High-Performance Tensor Transposition C++ Library (Paul Springer - 10 May, 2017)
This modular design---inspired by BLIS---makes HPTT easy to port to different architectures, by only replacing the hand-vectorized micro-kernel (e.g., a 4x4 transpose). Most importantly, the integration of HPTT into the Cyclops Tensor Framework (CTF) improves the overall performance of tensor contractions by up to 3.1x.
Link: https://arxiv.org/abs/1704.04374
====================================================
Strassen's Algorithm for Tensor Contraction (Jianyu Huang - 10 April, 2017)
Tensor contraction (TC) is an important computational kernel widely used in numerous applications. Performance benefits are demonstrated with a performance model as well as in practice on modern single core, multicore, and distributed memory parallel architectures, achieving up to 1.3x speedup
Link: https://arxiv.org/abs/1704.03092
====================================================
Near-Optimal $\varepsilon$-Kernel Construction and Related Problems (Sunil Arya - 31 March, 2017)
The computation of (i) $\varepsilon$-kernels, (ii) approximate diameter, and (iii) approximate bichromatic closest pair are fundamental problems in geometric approximation. In each case the input is a set of $n$ points in $R^d$ for a constant dimension $d \geq 3$ and an approximation parameter $\varepsilon > 0$. We reduce the respective running times (i) from $O((n + 1/\varepsilon^{d-2})\log(1/\varepsilon))$ to $O(n \log(1/\varepsilon) + 1/\varepsilon^{(d-1)/2+Î±})$, (ii) from $O((n + 1/\varepsilon^{d-2})\log(1/\varepsilon))$ to $O(n \log(1/\varepsilon) + 1/\varepsilon^{(d-1)/2+Î±})$, and (iii) from $O(n / \varepsilon^{d/3})$ to $O(n / \varepsilon^{d/4+Î±}),$ for an arbitrarily small constant $Î±> 0$
Link: https://arxiv.org/abs/1703.10868
====================================================
MSE estimates for multitaper spectral estimation and off-grid compressive sensing (LuÃ­s Daniel Abreu - 20 June, 2017)
IEEE, 1982]: assuming bandwidth $W$ and $N$ time domain observations, the average of the square of the first $K=2NW$ Slepian functions approaches, as $K$ grows, an ideal band-pass kernel for the interval $[-W,W]$. We provide an analytic proof of this fact and measure the corresponding rate of convergence in the $L^{1}$ norm
Link: https://arxiv.org/abs/1703.08190
====================================================
Memos: Revisiting Hybrid Memory Management in Modern Operating System (Lei Liu - 22 March, 2017)
Powered by our newly designed kernel-level monitoring module and page migration engine, memos can dynamically optimize the data placement at the memory hierarchy in terms of the on-line memory patterns, current resource utilization and feature of memory medium. Our experimental results show that memos can achieve high memory utilization, contributing to system throughput by 19.1% and QoS by 23.6% on average
Link: https://arxiv.org/abs/1703.07725
====================================================
CLTune: A Generic Auto-Tuner for OpenCL Kernels (Cedric Nugteren - 19 March, 2017)
It evaluates and tunes kernel performance of a generic, user-defined search space of possible parameter-value combinations. CLTune can be used in the following scenarios: 1) when there are too many tunable parameters to explore manually, 2) when performance portability across OpenCL devices is desired, or 3) when the optimal parameters change based on input argument values (e.g
Link: https://arxiv.org/abs/1703.06503
====================================================
Scalable Accelerated Decentralized Multi-Robot Policy Search in Continuous Observation Spaces (Shayegan Omidshafiei - 16 March, 2017)
An SK-FSA search algorithm titled Entropy-based Policy Search using Continuous Kernel Observations (EPSCKO) is introduced and applied to the first ever continuous-observation Dec-POMDP/Dec-POSMDP domain, where it significantly outperforms state-of-the-art discrete approaches
Link: https://arxiv.org/abs/1703.05626
====================================================
Real-Time Machine Learning: The Missing Pieces (Robert Nishihara - 19 May, 2017)
These applications pose a new set of requirements, none of which are difficult to achieve in isolation, but the combination of which creates a challenge for existing distributed execution frameworks: computation with millisecond latency at high throughput, adaptive construction of arbitrary task graphs, and execution of heterogeneous kernels over diverse sets of resources. We assert that a new distributed execution framework is needed for such ML applications and propose a candidate approach with a proof-of-concept architecture that achieves a 63x performance improvement over a state-of-the-art execution framework for a representative application.
Link: https://arxiv.org/abs/1703.03924
====================================================
Assessing Code Authorship: The Case of the Linux Kernel (Guilherme Avelino - 8 March, 2017)
Towards that direction, we perform a case study on the Linux kernel. Our results show that: (a) only a small portion of developers (26 %) makes significant contributions to the code base; (b) the distribution of the number of files per author is highly skewed --- a small group of top authors (3 %) is responsible for hundreds of files, while most authors (75 %) are responsible for at most 11 files; (c) most authors (62 %) have a specialist profile; (d) authors with a high number of co-authorship connections tend to collaborate with others with less connections.
Link: https://arxiv.org/abs/1703.02925
====================================================
Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network (Chao Peng - 8 March, 2017)
One of recent trends [30, 31, 14] in network architec- ture design is stacking small filters (e.g., 1x1 or 3x3) in the entire network because the stacked small filters is more ef- ficient than a large kernel, given the same computational complexity. Our approach achieves state-of-art perfor- mance on two public benchmarks and significantly outper- forms previous results, 82.2% (vs 80.2%) on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.
Link: https://arxiv.org/abs/1703.02719
====================================================
Global Weisfeiler-Lehman Graph Kernels (Christopher Morris - 22 September, 2017)
Most state-of-the-art graph kernels only take local graph properties into account, i.e., the kernel is computed with regard to properties of the neighborhood of vertices or other small substructures. We support our theoretical results with experiments on several graph classification benchmarks, showing that our kernels often outperform the state-of-the-art in terms of classification accuracies.
Link: https://arxiv.org/abs/1703.02379
====================================================
Learning rates for classification with Gaussian kernels (Shao-Bo Lin - 5 October, 2017)
Our second result shows that, for a large number of loss functions, under some Tsybakov noise assumption, if the regression function is infinitely smooth, then SVM with Gaussian kernel can achieve the learning rate of order $m^{-1}$, where $m$ is the number of samples.
Link: https://arxiv.org/abs/1702.08701
====================================================
II-FCN for skin lesion analysis towards melanoma detection (Hongdiao Wen - 14 March, 2017)
Dermoscopy image detection stays a tough task due to the weak distinguishable property of the object.Although the deep convolution neural network signifigantly boosted the performance on prevelance computer vision tasks in recent years,there remains a room to explore more robust and precise models to the problem of low contrast image segmentation.Towards the challenge of Lesion Segmentation in ISBI 2017,we built a symmetrical identity inception fully convolution network which is based on only 10 reversible inception blocks,every block composed of four convolution branches with combination of different layer depth and kernel size to extract sundry semantic features.Then we proposed an approximate loss function for jaccard index metrics to train our model.To overcome the drawbacks of traditional convolution,we adopted the dilation convolution and conditional random field method to rectify our segmentation.We also introduced multiple ways to prevent the problem of overfitting.The experimental results shows that our model achived jaccard index of 0.82 and kept learning from epoch to epoch.
Link: https://arxiv.org/abs/1702.08699
====================================================
Enabling Sparse Winograd Convolution by Native Pruning (Sheng Li - 13 October, 2017)
Nevertheless, training convolution layers so that the resulting Winograd kernels are sparse has not hitherto been very successful. By introducing a Winograd layer in place of a standard convolution layer, we can learn and prune Winograd coefficients "natively" and obtain sparsity level beyond 90% with only 0.1% accuracy loss with AlexNet on ImageNet dataset. Furthermore, we present a sparse Winograd convolution algorithm and implementation that exploits the sparsity, achieving up to 31.7 effective TFLOP/s in 32-bit precision on a latest Intel Xeon CPU, which corresponds to a 5.4x speedup over a state-of-the-art dense convolution implementation.
Link: https://arxiv.org/abs/1702.08597
====================================================
Deep Voice: Real-time Neural Text-to-Speech (Sercan O. Arik - 7 March, 2017)
Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.
Link: https://arxiv.org/abs/1702.07825
====================================================
Scene Recognition by Combining Local and Global Image Descriptors (Jobin Wilson - 21 February, 2017)
Finally, we experiment with a multi-class SVM classifier with several kernels, in a cross-validation setting, and tabulate our results on the fifteen scene categories dataset. The average accuracy of our model was 76.4% in the case of a 40%-60% random split of images into training and testing datasets respectively
Link: https://arxiv.org/abs/1702.06850
====================================================
Boosted Multiple Kernel Learning for First-Person Activity Recognition (Fatih Ozkan - 5 June, 2017)
Our experimental results show that use of Multiple Kernel Learning (MKL) and Boosted MKL in first-person activity recognition problem exhibits improved results in comparison to the state-of-the-art
Link: https://arxiv.org/abs/1702.06799
====================================================
LED-it-GO: Leaking (a lot of) Data from Air-Gapped Computers via the (small) Hard Drive LED (Mordechai Guri - 22 February, 2017)
We also present various data modulation methods and describe the implementation of a user-level malware, that doesn't require a kernel component. Our experiment shows that sensitive data can be successfully leaked from air-gapped computers via the HDD LED at a maximum bit rate of 4000 bits per second, depending on the type of receiver and its distance from the transmitter. Notably, this speed is 10 times faster than the existing optical covert channels for air-gapped computers
Link: https://arxiv.org/abs/1702.06715
====================================================
Refinement-based Specification and Security Analysis of Separation Kernels (Yongwang Zhao - 20 February, 2017)
As an industrial standard for improving safety, ARINC 653 has been complied with by mainstream separation kernels. Due to the new trend of integrating safe and secure functionalities into one separation kernel, security analysis of ARINC 653 as well as a formal specification with security proofs are thus significant for the development and certification of ARINC 653 compliant Separation Kernels (ARINC SKs). A major part of separation kernel requirements in ARINC 653 are modeled, such as kernel initialization, two-level scheduling, partition and process management, and inter-partition communication. VxWorks 653, XtratuM, and POK, in accordance with the formal specification. During the verification and code review, six security flaws, which can cause information leakage, are found in the ARINC 653 standard and the implementations.
Link: https://arxiv.org/abs/1702.05997
====================================================
LAMMPS' PPPM Long-Range Solver for the Second Generation Xeon Phi (William McDoniel - 14 February, 2017)
Our main target is the optimization of computational kernels by means of vectorization, and we observe speedups in these kernels of up to 12x
Link: https://arxiv.org/abs/1702.04250
====================================================
Ensemble Estimation of Mutual Information (Kevin R. Moon - 27 January, 2017)
We derive the mean squared error convergence rates of kernel density-based plug-in estimators of mutual information measures between two multidimensional random variables $\mathbf{X}$ and $\mathbf{Y}$ for two cases: 1) $\mathbf{X}$ and $\mathbf{Y}$ are both continuous; 2) $\mathbf{X}$ is continuous and $\mathbf{Y}$ is discrete. Ensemble estimators that achieve the parametric rate are also derived for the first case ($\mathbf{X}$ and $\mathbf{Y}$ are both continuous) and another case 3) $\mathbf{X}$ and $\mathbf{Y}$ may have any mixture of discrete and continuous components.
Link: https://arxiv.org/abs/1701.08083
====================================================
Minimum-Distance Based Construction of Multi-Kernel Polar Codes (Valerio Bioglio - 4 September, 2017)
Compared to state-of-the-art punctured or shortened Arikan polar codes, multi-kernel polar codes with our new design show significantly improved error-rate performance.
Link: https://arxiv.org/abs/1701.07616
====================================================
On the Existence of Kernel Function for Kernel-Trick of k-Means (MieczysÅaw A. KÅopotek - 23 March, 2017)
The correction is needed in order to establish the existence of the kernel function used commonly in the kernel trick e.g. The scope of correction is explained in section 2.
Link: https://arxiv.org/abs/1701.05335
====================================================
GPGPU Performance Estimation with Core and Memory Frequency Scaling (Qiang Wang - 13 June, 2018)
Over a 2.5x range of both core and memory frequencies among 12 GPU kernels, our model achieves accurate results (within 3.5\%) on real hardware
Link: https://arxiv.org/abs/1701.05308
====================================================
On the Successive Cancellation Decoding of Polar Codes with Arbitrary Linear Binary Kernels (Zhiliang Huang - 16 January, 2017)
For a $m\times m$ binary kernel, the complexity of straightforward SC decoder is $O(2^{m}N\log N)$. With $W$-expressions, we reduce the complexity of straightforward SC decoder to $O(m^{2}N\log N)$ when $m\leq 16$
Link: https://arxiv.org/abs/1701.03264
====================================================
An $N \log N$ Parallel Fast Direct Solver for Kernel Matrices (Chenhan D. Yu - 9 January, 2017)
As a highlight, we are able to approximately factorize a dense $11M\times11M$ kernel matrix in 2 minutes on 3,072 x86 "Haswell" cores and a $4.5M\times4.5M$ matrix in 1 minute using 4,352 "Knights Landing" cores.
Link: https://arxiv.org/abs/1701.02324
====================================================
On the Complexity of Restoring Corrupted Colorings (Marzio De Biasi - 8 January, 2017)
Recently, Junosza-Szaniawski, Liedloff, and Rz{Ä}{Å¼}ewski [SOFSEM 2015] asked whether the problem has a polynomial kernel parameterized by the number of recolorings $k$. In a full version of the manuscript, the authors together with Garnero and Montealegre, answered the question in the negative: for every $r \geq 3$, the problem \probrFix does not admit a polynomial kernel unless $\NP \subseteq \coNP / \poly$. We show that for every $r \geq 3$, the problem \probrFixSwap is $\W[1]$-hard whereas \probrFix is known to be FPT. Moreover, when $r$ is part of the input, we observe both \probFix and \probFixSwap are $\W[1]$-hard parameterized by treewidth
Link: https://arxiv.org/abs/1701.01939
====================================================
High-Assurance Separation Kernels: A Survey on Formal Methods (Yongwang Zhao - 5 January, 2017)
More than 20 implementations of separation kernels have been developed and widely applied in critical domains, e.g., avionics/aerospace, military/defense, and medical devices
Link: https://arxiv.org/abs/1701.01535
====================================================
Equality Constrained Decision Trees: For the Algorithmic Enforcement of Group Fairness (Jack Fitzsimons - 10 October, 2018)
In this work, we consider how to incorporate group fairness constraints in kernel regression methods. More specifically, we focus on examining the incorporation of these constraints in decision tree regression when cast as a form of kernel regression, with direct applications to random forests and boosted trees amongst other widespread popular inference techniques
Link: https://arxiv.org/abs/1810.05041
====================================================
Dense Object Reconstruction from RGBD Images with Embedded Deep Shape Representations (Lan Hu - 11 October, 2018)
Unmodeled effects violating the lambertian surface assumption or geometric invariances of individual residuals are encountered through statistical averaging or the addition of robust kernels and smoothness terms
Link: https://arxiv.org/abs/1810.04891
====================================================
Of Kernels and Queues: when network calculus meets analytic combinatorics (Anne Bouillard - 11 October, 2018)
Applying the kernel method allows us to compute the generating functions of the queue state distributions in the stationary regime of the network. In this preliminary work, we focus on simple examples which are representative of the difficulties that the kernel method allows us to overcome.
Link: https://arxiv.org/abs/1810.04875
====================================================
Parameterized Complexity of Independent Set in H-Free Graphs (Ãdouard Bonnet - 10 October, 2018)
Finally, we present positive and negative results on the existence of polynomial (Turing) kernels for several graphs $H$.
Link: https://arxiv.org/abs/1810.04620
====================================================
Global Search with Bernoulli Alternation Kernel for Task-oriented Grasping Informed by Simulation (Rika Antonova - 10 October, 2018)
With this Bernoulli Alternation Kernel we ensure that discrepancies between simulation and reality do not hinder adapting robot control policies online. We learn task scores from a labeled dataset with a convolutional network, which is used to construct an informed kernel for our variant of Bayesian optimization
Link: https://arxiv.org/abs/1810.04438
====================================================
Penetrating the Fog: the Path to Efficient CNN Models (Kun Wan - 10 October, 2018)
Specifically, we present a sparse kernel scheme to illustrate how to reduce the space from three aspects. Second, to remove designs with large accuracy degradation, we find an unified property named information field behind various sparse kernel designs, which could directly indicate the final accuracy
Link: https://arxiv.org/abs/1810.04231
====================================================
Learning One-hidden-layer Neural Networks under General Input Distributions (Weihao Gao - 9 October, 2018)
Traditional estimation methods (example: kernel based) fail right at the outset; we bring statistical methods of local likelihood to design a novel estimator of score functions, that provably adapts to the local geometry of the unknown density.
Link: https://arxiv.org/abs/1810.04133
====================================================
MPI Windows on Storage for HPC Applications (Sergio Rivas-Gomez - 9 October, 2018)
Nonetheless, experimental results of a Distributed Hash Table, the HACC I/O kernel mini-application, and a novel MapReduce implementation based on the use of MPI one-sided communication, indicate that the overall penalty of MPI windows on storage can be negligible in most cases in real-world applications.
Link: https://arxiv.org/abs/1810.04110
====================================================
Convolutional Neural Networks In Convolution (Xiaobo Huang - 9 October, 2018)
In our architecture, namely "CNN In Convolution"(CNNIC), a small CNN, instead of the original generalized liner model(GLM) based filters, is convoluted as kernel on the original image, serving as feature extracting layer of this networks
Link: https://arxiv.org/abs/1810.03946
====================================================
Learning Bounds for Greedy Approximation with Explicit Feature Maps from Multiple Kernels (Shahin Shahrampour - 9 October, 2018)
We establish an out-of-sample error bound capturing the trade-off between the error in terms of explicit features (approximation error) and the error due to spectral properties of the best model in the Hilbert space associated to the combined kernel (spectral error). Our empirical results show that given a fixed number of explicit features, the method can achieve a lower test error with a smaller time cost, compared to the state-of-the-art in data-dependent random features.
Link: https://arxiv.org/abs/1810.03817
====================================================
Efficient Non-parametric Bayesian Hawkes Processes (Rui Zhang - 8 October, 2018)
We sample random branching structures, and thus split the Hawkes process into clusters of Poisson processes, where the intensity function of each of these processes is the nonparametric triggering kernel of the Hawkes process. We observe that the learned non-parametric kernel reflects the longevity of different content types.
Link: https://arxiv.org/abs/1810.03730
====================================================
Graph Classification with Geometric Scattering (Feng Gao - 6 October, 2018)
We demonstrate the utility of features extracted with this designed deep filter bank in graph classification, and show its competitive performance relative to other methods, including graph kernel methods and geometric deep learning ones, on both social and biochemistry data.
Link: https://arxiv.org/abs/1810.03068
====================================================
Eiffel: Efficient and Flexible Software Packet Scheduling (Ahmed Saeed - 6 October, 2018)
We evaluate Eiffel in a variety of settings and in both kernel and userspace deployments. We show that it outperforms state of the art systems by 3-40x in terms of either number of cores utilized for network processing or number of flows given fixed processing capacity.
Link: https://arxiv.org/abs/1810.03060
====================================================
Memristor-based Deep Convolution Neural Network: A Case Study (Fan Zhang - 14 September, 2018)
An improved conversion algorithm is developed to convert convolution kernels to memristor-based circuits, which minimizes the error with consideration of the data and kernel patterns in CNNs
Link: https://arxiv.org/abs/1810.02225
====================================================
Learning agent's spatial configuration from sensorimotor invariants (Alban LaflaquiÃ¨re - 3 October, 2018)
In a sufficiently rich environment, the kernels of these functions correspond uniquely to the spatial configuration of the agent's exteroceptors
Link: https://arxiv.org/abs/1810.01872
====================================================
Improving Bag-of-Visual-Words Towards Effective Facial Expressive Image Classification (Dawood Al Chanti - 30 September, 2018)
We speed up the learning process by using histogram intersection kernel by Support Vector Machine to learn a discriminative classifier
Link: https://arxiv.org/abs/1810.00360
====================================================
A Framework for Evaluating Motion Segmentation Algorithms (Christian R. G. Dreher - 30 September, 2018)
The testing ground features both a set of quality measures known from the literature and a novel approach tailored to the evaluation of motion segmentation algorithms, termed Integrated Kernel approach
Link: https://arxiv.org/abs/1810.00357
====================================================
A Randomized Kernel-Based Secret Image Sharing Scheme (Ravi Tej Akella - 29 September, 2018)
The kernel operation is optimized in terms of the security and computational requirements. The storage overhead of the kernel can further be made independent of its size by efficiently storing it as a sparse matrix
Link: https://arxiv.org/abs/1810.00181
====================================================
FusedLSTM: Fusing frame-level and video-level features for Content-based Video Relevance Prediction (Yash Bhalgat - 28 September, 2018)
In the second approach, an Online Kernel Similarity Learning method is proposed to learn a non-linear similarity measure to adhere the relevance training data
Link: https://arxiv.org/abs/1810.00136
====================================================
Ten Simple Rules for Using a Raspberry Pi (Anthony C Fletcher - 1 October, 2018)
These trends notwithstanding, science students often receive little to no training in the more technical and hardware-focused areas of computational sciences (e.g., Linux/Unix usage, device drivers, kernel modules and so on)
Link: https://arxiv.org/abs/1810.00027
====================================================
Point-of-Interest Recommendation: Exploiting Self-Attentive Autoencoders with Neighbor-Aware Influence (Chen Ma - 27 September, 2018)
To incorporate the geographical context information, we propose a neighbor-aware decoder to make users' reachability higher on the similar and nearby neighbors of checked-in POIs, which is achieved by the inner product of POI embeddings together with the radial basis function (RBF) kernel. To evaluate the proposed model, we conduct extensive experiments on three real-world datasets with many state-of-the-art baseline methods and evaluation metrics
Link: https://arxiv.org/abs/1809.10770
====================================================
Kernel based low-rank sparse model for single image super-resolution (Jiahe Shi - 27 September, 2018)
Then we map the nonlocal data matrix into a high-dimensional feature space by kernel method to capture their nonlinear structures. Under the assumption that the sparse coeffcients for the nonlocal data in the kernel space should be low-rank, we impose low-rank constraint on sparse coding to share similarities among representation coeffcients and remove outliers in order that stable weights for SR reconstruction can be obtained
Link: https://arxiv.org/abs/1809.10582
====================================================
Reoptimization of Parameterized Problems (Hans-Joachim BÃ¶ckenhauer - 27 September, 2018)
Moreover, we find that the reoptimization version of Vertex Cover has a polynomial kernel of size 2k using crown decomposition. Finally, in a negative result, we prove that the reoptimization version of Connected Vertex Cover does not have a Turing kernelization unless Set Cover has a polynomial kernel
Link: https://arxiv.org/abs/1809.10578
====================================================
Autonomously and Simultaneously Refining Deep Neural Network Parameters by a Bi-Generative Adversarial Network Aided Genetic Algorithm (Yantao Lu - 24 September, 2018)
Our proposed approach can be used to autonomously refine the number of convolutional layers and dense layers, number and size of kernels, and the number of neurons for the dense layers; choose the type of the activation function; and decide whether to use dropout and batch normalization or not, to improve the accuracy of different deep neural network architectures
Link: https://arxiv.org/abs/1809.10244
====================================================
A Kernel for Multi-Parameter Persistent Homology (RenÃ© Corbet - 26 September, 2018)
We contribute a kernel construction for multi-parameter persistence by integrating a one-parameter kernel weighted along straight lines. We prove that our kernel is stable and efficiently computable, which establishes a theoretical connection between topological data analysis and machine learning for multivariate data analysis.
Link: https://arxiv.org/abs/1809.10231
====================================================
Generalization Properties of hyper-RKHS and its Application to Out-of-Sample Extensions (Fanghui Liu - 26 September, 2018)
For applications, we propose a general kernel learning framework conducted by the introduced two regression models to deal with the out-of-sample extensions problem, i.e., to learn a underlying general kernel from the pre-given kernel/similarity matrix in hyper-RKHS. Experimental results on several benchmark datasets suggest that our methods are able to learn a general kernel function from an arbitrary given kernel matrix.
Link: https://arxiv.org/abs/1809.09910
====================================================
Memory Management in Successive-Cancellation based Decoders for Multi-Kernel Polar Codes (Valerio Bioglio - 25 September, 2018)
We propose an efficient, generalized memory management framework for implementation of successivecancellation decoding of multi-kernel polar codes. We illustrate the proposed solution for small kernel sizes, and give complexity estimates for various kernel combinations and code lengths.
Link: https://arxiv.org/abs/1809.09436
====================================================
Software for Sparse Tensor Decomposition on Emerging Computing Architectures (Eric Phipps - 24 September, 2018)
We are focused on the canonical polyadic tensor decomposition of sparse tensors, whose key kernel is the matricized tensor times Khatri-Rao product (MTTKRP). We show that we are competitive with state-of-the-art approaches available in the literature while having the advantage of being able to run on a wider of variety of architectures with a single code.
Link: https://arxiv.org/abs/1809.09175
====================================================
Gamifying the Escape from the Engineering Method Prison - An Innovative Board Game to Teach the Essence Theory to Future Project Managers and Software Engineers (Kai-Kristian Kemell - 23 September, 2018)
Essence consists of a language for modeling Software Engineering (SE) practices and methods and a kernel containing what its authors describe as being elements that are present in every software development project
Link: https://arxiv.org/abs/1809.08656
====================================================
EXTRA: Explaining Team Recommendation in Networks (Qinghai Zhou - 22 September, 2018)
The main advantages are (1) Algorithm efficacy: we propose an effective and fast algorithm to explain random walk graph kernel, the central technique for networked team recommendation; (2) Intuitive visual explanation: we present intuitive visual analysis of the recommendation results, which can help users better understand the rationality of the underlying team recommendation algorithm.
Link: https://arxiv.org/abs/1809.08511
====================================================
Shift-based Primitives for Efficient Convolutional Neural Networks (Huasong Zhong - 24 September, 2018)
The address shift and channel shift can be merged into the point-wise group convolution and invokes only a single kernel call, taking little time to perform spatial convolution and channel shift
Link: https://arxiv.org/abs/1809.08458
====================================================
Data-compression for Parametrized Counting Problems on Sparse graphs (Eun Jung Kim - 25 September, 2018)
We study the concept of \emph{compactor}, which may be seen as a counting-analogue of kernelization in counting parameterized complexity. Although the study on counting-analogue of kernelization is not unprecedented, it has received little attention so far
Link: https://arxiv.org/abs/1809.08160
====================================================
A Discriminative Model for Identifying Readers and Assessing Text Comprehension from Eye Movements (Silvia Makowski - 21 September, 2018)
We study whether a Fisher-SVM with this Fisher kernel and several reference methods are able to identify readers and estimate their level of text comprehension based on eye-tracking data. While none of the methods are able to estimate text comprehension accurately, we find that the SVM with Fisher kernel excels at identifying readers.
Link: https://arxiv.org/abs/1809.08031
====================================================
Distances for WiFi Based Topological Indoor Mapping (Bastian SchÃ¤fermeier - 19 September, 2018)
Combined with kernel density estimation we were able to retain the topological structure of rooms in a real-world office scenario.
Link: https://arxiv.org/abs/1809.07405
====================================================
A Generalized Representer Theorem for Hilbert Space - Valued Functions (Sanket Diwale - 19 September, 2018)
Representer theorems involving explicit basis functions and Reproducing Kernels are a common occurrence in various machine learning algorithms like generalized least squares, support vector machines, Gaussian process regression and kernel based deep neural networks to name a few. The implications of the theorem are presented with examples of multi input-multi output regression, kernel based deep neural networks, stochastic regression and sparsity learning problems as being special cases in this unified view.
Link: https://arxiv.org/abs/1809.07347
====================================================
Privacy-Preserving SVM Computing by Using Random Unitary Transformation (Takahiro Maekawa - 19 September, 2018)
The proposed scheme enables us not only to protect templates, but also to have the same performance as that of unprotected templates under some useful kernel functions
Link: https://arxiv.org/abs/1809.07055
====================================================
On a Convex Logic Fragment for Learning and Reasoning (Francesco Giannini - 18 September, 2018)
Within this framework, we are able to formulate learning with kernel machines as well as collective classification as a quadratic programming problem.
Link: https://arxiv.org/abs/1809.06778
====================================================
Kernelization and approximation of distance-$r$ independent sets on nowhere dense graphs (MichaÅ Pilipczuk - 15 September, 2018)
We study the duality between the maximum size of a distance-$2r$ independent set and the minimum size of a distance-$r$ dominating set in nowhere dense graph classes, as well as the kernelization complexity of the distance-$r$ independent set problem on these graph classes. Specifically, we prove that the distance-$r$ independent set problem admits an almost linear kernel on every nowhere dense graph class.
Link: https://arxiv.org/abs/1809.05675
====================================================
HDArray: Parallel Array Interface for Distributed Heterogeneous Devices (Hyun Dok Cho - 18 September, 2018)
Using work distribution and kernel def and use information, communication among processes and devices in a process is performed automatically
Link: https://arxiv.org/abs/1809.05657
====================================================
An FPGA Implementation of a Time Delay Reservoir Using Stochastic Logic (Lisa Loomis - 12 September, 2018)
The reservoir network approach is analyzed using a number of metrics, such as kernel quality, generalization rank, performance on simple benchmarks, and is also compared to a deterministic design
Link: https://arxiv.org/abs/1809.05407
====================================================
Revisiting Random Binning Features: Fast Convergence and Strong Parallelizability (Lingfei Wu - 18 September, 2018)
In this work, we observe that the RB features, with right choice of optimization solver, could be orders-of-magnitude more efficient than other random features and kernel approximation methods under the same requirement of accuracy. Our extensive experiments demonstrate the superior performance of the RB features over other random features and kernel approximation methods
Link: https://arxiv.org/abs/1809.05247
====================================================
Learning Deep Mixtures of Gaussian Process Experts Using Sum-Product Networks (Martin Trapp - 12 September, 2018)
We show that integrating GPs into the SPN framework leads to a promising probabilistic regression model which is: (1) computational and memory efficient, (2) allows efficient and exact posterior inference, (3) is flexible enough to mix different kernel functions, and (4) naturally accounts for non-stationarities in time series. In a variate of experiments, we show that the SPN-GP model can learn input dependent parameters and hyper-parameters and is on par with or outperforms the traditional GPs as well as state of the art approximations on real-world data.
Link: https://arxiv.org/abs/1809.04400
====================================================
A Global Alignment Kernel based Approach for Group-level Happiness Intensity Estimation (Xiaohua Huang - 3 September, 2018)
Lastly, we propose Multiple kernel learning based on three combination strategies for combining two respective GAKs based on RVLBP and deep CNN features, such that enhancing the discriminative ability of each GAK. Our experimental results demonstrate that the proposed approach achieves promising performance for group happiness intensity analysis, when compared with the recent state-of-the-art methods.
Link: https://arxiv.org/abs/1809.03313
====================================================
How is Contrast Encoded in Deep Neural Networks? (Arash Akbarinia - 5 September, 2018)
We analysed the activation of kernels in the convolutional layers of eight prominent networks with distinct architectures (e.g
Link: https://arxiv.org/abs/1809.01438
====================================================
IKA: Independent Kernel Approximator (Matteo Ronchetti - 5 September, 2018)
This paper describes a new method for low rank kernel approximation called IKA. In contrast the approximation produced by NystrÃ¶m method is a linear combination of kernel evaluations
Link: https://arxiv.org/abs/1809.01353
====================================================
Towards Understanding Regularization in Batch Normalization (Ping Luo - 30 September, 2018)
We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function
Link: https://arxiv.org/abs/1809.00846
====================================================
Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions (Sho Yokoi - 4 September, 2018)
PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels
Link: https://arxiv.org/abs/1809.00800
====================================================
Typed Linear Algebra for Efficient Analytical Querying (JoÃ£o M. Afonso - 3 September, 2018)
A kernel of LA operators has been implemented so that paths extracted from LA diagrams can be executed
Link: https://arxiv.org/abs/1809.00641
====================================================
Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks (Yuan Yuan - 2 September, 2018)
This complicated setting makes supervised learning and accurate kernel estimation impossible. Experiments on NTIRE2018 datasets demonstrate that the proposed unsupervised method achieves comparable results as the state-of-the-art supervised models.
Link: https://arxiv.org/abs/1809.00437
====================================================
Real-time Linux communications: an evaluation of the Linux communication stack for real-time robotic applications (Carlos San Vicente GutiÃ©rrez - 30 August, 2018)
We prove that, under an appropriate configuration, the Linux kernel greatly enhances the determinism of communications using the UDP protocol
Link: https://arxiv.org/abs/1808.10821
====================================================
Learning Data-adaptive Nonparametric Kernels (Fanghui Liu - 31 August, 2018)
Thereby, the training process for kernel and parameter learning in SVM/SVR can be efficiently optimized in a unified framework. Experimentally, our method outperforms other representative kernel learning based algorithms on various classification and regression benchmark datasets.
Link: https://arxiv.org/abs/1808.10724
====================================================
Facial Information Recovery from Heavily Damaged Images using Generative Adversarial Network- PART 1 (Pushparaja Murugan - 27 August, 2018)
In this article, we propose a kernel free framework based on conditional-GAN to recover the information from the heavily damaged images
Link: https://arxiv.org/abs/1808.08867
====================================================
Future Automation Engineering using Structural Graph Convolutional Neural Networks (Jiang Wan - 24 August, 2018)
Our Structural Graph Convolutional Neural Network (SGCNN) is capable of learning graphs and subgraphs with a novel graph invariant convolution kernel and downsampling/pooling algorithm
Link: https://arxiv.org/abs/1808.08213
====================================================
Multivariate Extension of Matrix-based Renyi's Î±-order Entropy Functional (Shujian Yu - 23 August, 2018)
The matrix-based Renyi's Î±-order entropy functional was recently introduced using the normalized eigenspectrum of an Hermitian matrix of the projected data in the reproducing kernel Hilbert space (RKHS)
Link: https://arxiv.org/abs/1808.07912
====================================================
Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance (Siyu Huang - 22 August, 2018)
Specifically, the multi-kernel pooling comprises of pooling kernels with multiple receptive fields to capture the responses at multi-scale local ranges. The stacked pooling is an equivalent form of multi-kernel pooling, while, it reduces considerable computing cost
Link: https://arxiv.org/abs/1808.07456
====================================================
Student Cluster Competition 2017, Team University ofTexas at Austin/Texas State University: Reproducing Vectorization of the Tersoff Multi-Body Potential on the Intel Skylake and NVIDIA V100 Architectures (James Sullivan - 21 August, 2018)
(2016) for an implementation of a vectorized code for the Tersoff multi-body potential kernel of the molecular dynamics code Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS)
Link: https://arxiv.org/abs/1808.07027
====================================================
Multi-task multiple kernel machines for personalized pain recognition from functional near-infrared spectroscopy brain signals (Daniel Lopez-Martinez - 21 August, 2018)
Especially, we employed multi-task multiple kernel learning to account for the inter-subject variability in pain response
Link: https://arxiv.org/abs/1808.06774
====================================================
Convolutional Neural Networks on 3D Surfaces Using Parallel Frames (Hao Pan - 14 August, 2018)
To handle irregular points of a discrete mesh while sharing kernel weights, we make the convolution semi-discrete, i.e. the convolution kernels are polynomial functions, and their convolution with discrete surface points becomes sampling and weighted summation
Link: https://arxiv.org/abs/1808.04952
====================================================
Image Registration and Predictive Modeling: Learning the Metric on the Space of Diffeomorphisms (Ayagoz Mussabayeva - 10 August, 2018)
For simplicity, we choose the kernel Fischer Linear Discriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters in an Expectation-Maximization framework, we define model fidelity via the hinge loss of the decision function
Link: https://arxiv.org/abs/1808.04439
====================================================
A Spin-based model checking for the simple concurrent program on a preemptive RTOS (Chen-Kai Lin - 7 August, 2018)
We adapt an existing preemptive scheduling model of RTOS kernel by eChronos from machine-assisted proof to Spin-based model checker. Moreover, we look into the designs of a Linux-like real-time kernel--Piko/RT and the specification of ARMv7-M architecture to reconstruct the model, and use LTL to specify a simple concurrent programs--consumer/producer problem during the development stage of the kernel
Link: https://arxiv.org/abs/1808.04239
====================================================
Learning Discriminative Hashing Codes for Cross-Modal Retrieval based on Multiorder Statistical Features (Jun Yu - 13 August, 2018)
The aim of kernelization is to learn a common subspace where heterogeneous data can be fused. Extensive experiments on three publicly available datasets clearly indicate the superiority of our method compared with the state-of-the-art methods.
Link: https://arxiv.org/abs/1808.04152
====================================================
On-Chip Optical Convolutional Neural Networks (Hengameh Bagherian - 16 August, 2018)
Convolutional Neural Networks (CNNs) are a class of Artificial Neural Networks(ANNs) that employ the method of convolving input images with filter-kernels for object recognition and classification purposes. In this paper, we propose a photonics circuit architecture which could consume a fraction of energy per inference compared with state of the art electronics.
Link: https://arxiv.org/abs/1808.03303
====================================================
Learning-Aided Physical Layer Authentication as an Intelligent Process (He Fang - 7 August, 2018)
Explicitly, a physical layer attribute fusion model based on a kernel machine is designed for dealing with multiple attributes without requiring the knowledge of their statistical properties. By formulating the learning (training) objective of the physical layer authentication as a convex problem, an adaptive algorithm based on kernel least-mean-square is then proposed as an intelligent process to learn and track the variations of multiple attributes, and therefore to enhance the authentication performance
Link: https://arxiv.org/abs/1808.02456
====================================================
Instance-Dependent PU Learning by Bayesian Optimal Relabeling (Fengxiang He - 6 August, 2018)
The relabelled examples have a biased domain, which is remedied by the kernel mean matching technique
Link: https://arxiv.org/abs/1808.02180
====================================================
Kerman: A Hybrid Lightweight Tracking Algorithm to Enable Smart Surveillance as an Edge Service (Seyed Yahya Nikouei - 6 August, 2018)
In this paper, we envision to achieve intelligent surveillance as an edge service by proposing a hybrid lightweight tracking algorithm named Kerman (Kernelized Kalman filter). Kerman is a decision tree based hybrid Kernelized Correlation Filter (KCF) algorithm proposed for human object tracking, which is coupled with a lightweight Convolutional Neural Network (L-CNN) for high performance
Link: https://arxiv.org/abs/1808.02134
====================================================
Self-Attention Recurrent Network for Saliency Detection (Fengdong Sun - 5 August, 2018)
On one hand, local information of shallow layers is enhanced by a recurrent structure which shared convolution kernel at different time steps
Link: https://arxiv.org/abs/1808.01634
====================================================
Generalized Spectral Mixture Kernels for Multi-Task Gaussian Processes (Kai Chen - 18 September, 2018)
Specifically, we use generalized convolution spectral mixture kernels for modeling dependencies at spectral mixture level, and coupling coregionalization for discovering task level correlations. The proposed kernels for MTGP are validated on artificial data and compared with existing MTGPs methods on three real-world experiments
Link: https://arxiv.org/abs/1808.01132
====================================================
Improved Deep Spectral Convolution Network For Hyperspectral Unmixing With Multinomial Mixture Kernel and Endmember Uncertainty (Savas Ozkan - 3 August, 2018)
Second, we introduce a multinomial mixture kernel with a neural network (NN) which mimics the Gaussian Mixture Model (GMM) to estimate the abundances per-pixel by using the low-dimension representations obtained from the improved DSCN. The results validate that the proposed method obtains state-of-the-art hyperspectral unmixing performance particularly on the real datasets compared to the baseline techniques.
Link: https://arxiv.org/abs/1808.01104
====================================================
Streaming Kernel PCA with $\tilde{O}(\sqrt{n})$ Random Features (Enayat Ullah - 2 August, 2018)
We study the statistical and computational aspects of kernel principal component analysis using random Fourier features and show that under mild assumptions, $O(\sqrt{n} \log n)$ features suffices to achieve $O(1/Îµ^2)$ sample complexity
Link: https://arxiv.org/abs/1808.00934
====================================================
Machine Learning of Space-Fractional Differential Equations (Mamikon Gulian - 14 August, 2018)
The methodology is compatible with a wide variety of fractional operators in $\mathbb{R}^d$ and stationary covariance kernels, including the Matern class, and can optimize the Matern parameter during training. We provide a user-friendly and feasible way to perform fractional derivatives of kernels, via a unified set of d-dimensional Fourier integral formulas amenable to generalized Gauss-Laguerre quadrature.
Link: https://arxiv.org/abs/1808.00931
====================================================
Spectral Mixture Kernels with Time and Phase Delay Dependencies (Kai Chen - 18 September, 2018)
To address these drawbacks we introduce Generalized Convolution Spectral Mixture (GCSM) kernels. We incorporate time and phase delay into the base spectral mixture and use cross-convolution between a base component and the complex conjugate of another base component to construct a complex-valued and positive definite kernel representing correlations between base components
Link: https://arxiv.org/abs/1808.00560
====================================================
Revisiting Client Puzzles for State Exhaustion Attacks Resilience (Mohammad A. Noureddine - 31 July, 2018)
We then present an implementation of client puzzles inside the TCP stack of the Linux 4.13.0 kernel
Link: https://arxiv.org/abs/1807.11892
====================================================
Extensible Grounding of Speech for Robot Instruction (Jonathan Connell - 31 July, 2018)
Creating this language learning kernel may be the last explicit programming the robot ever needs - the core mechanism could eventually be used for imparting a vast amount of knowledge, much as a child learns from its parents and teachers.
Link: https://arxiv.org/abs/1807.11838
====================================================
Remote sensing image regression for heterogeneous change detection (Luigi T. Luppino - 31 July, 2018)
Four regression methods are selected to carry out the transformation: Gaussian processes, support vector machines, random forests, and a recently proposed kernel regression method called homogeneous pixel transformation
Link: https://arxiv.org/abs/1807.11766
====================================================
Learning Collaborative Generation Correction Modules for Blind Image Deblurring and Beyond (Risheng Liu - 31 July, 2018)
Most existing works tend to introduce complex priors to estimate the sharp image structures for blur kernel estimation. Plenty of experiments demonstrate that our method can outperform the state-of-the-art approaches on both synthetic and real datasets.
Link: https://arxiv.org/abs/1807.11706
====================================================
Efficient feature learning and multi-size image steganalysis based on CNN (Ru Zhang - 30 July, 2018)
First, we use 3x3 kernels instead of the traditional 5x5 kernels and optimize convolution kernels in the preprocessing layer. The smaller convolution kernels are used to reduce the number of parameters and model the features in a small local region
Link: https://arxiv.org/abs/1807.11428
====================================================
Kernel Density Estimation-Based Markov Models with Hidden State (Gustav Eje Henter - 30 July, 2018)
We consider Markov models of stochastic processes where the next-step conditional distribution is defined by a kernel density estimator (KDE), similar to Markov forecast densities and certain time-series bootstrap schemes. We present guaranteed-ascent EM-update equations for model parameters in the case of Gaussian kernels, as well as relaxed update formulas that greatly accelerate training in practice
Link: https://arxiv.org/abs/1807.11320
====================================================
Harmonic-Percussive Source Separation with Deep Neural Networks and Phase Recovery (Konstantinos Drossos - 30 July, 2018)
Experiments conducted on realistic music mixtures show that this novel separation system outperforms the previous state-of-the art kernel additive model approach.
Link: https://arxiv.org/abs/1807.11298
====================================================
Stochastic Policy Gradient Ascent in Reproducing Kernel Hilbert Spaces (Santiago Paternain - 30 July, 2018)
In this paper, we consider the problem of finding such optimal policies while assuming they are continuous functions belonging to a reproducing kernel Hilbert space (RKHS)
Link: https://arxiv.org/abs/1807.11274
====================================================
Sparse Matrix Code Dependence Analysis Simplification at Compile Time (Mahdi Soltan Mohammadi - 27 July, 2018)
This makes it possible to leverage existing SMT solvers to determine whether such dependences are unsatisfiable and significantly reduces the number of dependences that require runtime analysis in a set of eight sparse matrix kernels
Link: https://arxiv.org/abs/1807.10852
====================================================
Deep PDF: Probabilistic Surface Optimization and Density Estimation (Dmitry Kopitkov - 15 September, 2018)
However, most density estimation techniques are limited in their representation expressiveness to specific kernel type or predetermined distribution family, and have other restrictions. For example, kernel density estimation (KDE) methods require meticulous parameter search and are extremely slow at querying new points
Link: https://arxiv.org/abs/1807.10728
====================================================
Semi-convolutional Operators for Instance Segmentation (David Novotny - 27 July, 2018)
We use the latter to show a connection to Hough voting as well as to a variant of the bilateral kernel that is spatially steered by a convolutional network
Link: https://arxiv.org/abs/1807.10712
====================================================
Fast & Flexible IO : A Compositional Approach to Storage Construction for High-Performance Devices (Daniel G. Waddington - 25 July, 2018)
Furthermore, because of restrictions imposed by kernel-based designs, many legacy implementations have traded software flexibility for performance
Link: https://arxiv.org/abs/1807.09696
====================================================
Visual Dynamics: Stochastic Future Generation via Layered Cross Convolutional Networks (Tianfan Xue - 25 July, 2018)
To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively
Link: https://arxiv.org/abs/1807.09245
====================================================
A Structured Perspective of Volumes on Active Learning (Xiaofeng Cao - 24 July, 2018)
In non-linear feature space, spanned by kernel, we use sequential optimization to globally optimize the original space to a sparse space by halving the size of the kernel space
Link: https://arxiv.org/abs/1807.08904
====================================================
Spectre Returns! Speculation Attacks using the Return Stack Buffer (Esmaeil Mohammadian Koruyeh - 20 July, 2018)
We also analyze additional types of the attack on the kernel or across address spaces and show that under some practical and widely used conditions they are possible. In particular, on Core-i7 Skylake and newer processors (but not on Intel's Xeon processor line), a patch called RSB refilling is used to address a vulnerability when the RSB underfills; this defense interferes with SpectreRSB's ability to launch attacks that switch into the kernel
Link: https://arxiv.org/abs/1807.07940
====================================================
3D Global Convolutional Adversarial Network\\ for Prostate MR Volume Segmentation (Haozhe Jia - 17 July, 2018)
However, 2D methods tend to have limited segmentation performance, since large amounts of spatial information of prostate volumes are discarded during the slice-by-slice segmentation process; and 3D methods also have room for improvement, since they use isotropic kernels to perform 3D convolutions whereas most prostate MR volumes have anisotropic spatial resolution. We evaluated our 3D GCA-Net model on two public prostate MR datasets and achieved state-of-the-art performances.
Link: https://arxiv.org/abs/1807.06742
====================================================
Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks (Giuseppe Marra - 5 October, 2018)
The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.
Link: https://arxiv.org/abs/1807.06302
====================================================
Evaluation as a Service architecture and crowdsourced problems solving implemented in Optil.io platform (Szymon Wasik - 14 July, 2018)
Secondly, the measured performance of the algorithm highly depends on the test environment architecture, i.e., CPU model, available memory, cache configuration, operating system's kernel, and even compilation flags
Link: https://arxiv.org/abs/1807.06002
====================================================
Sparsity-based Convolutional Kernel Network for Unsupervised Medical Image Analysis (Euijoon Ahn - 27 July, 2018)
Our algorithm introduces three new contributions: (i) we use kernel learning to identify and represent invariant characteristics across image sub-patches in an unsupervised manner; (ii) we leverage the sparsity inherent to medical image data and propose a new sparse convolutional kernel network (S-CKN) that can be pre-trained in a layer-wise fashion, thereby providing initial discriminative features for medical data; and (iii) we propose a spatial pyramid pooling framework to capture subtle geometric differences in medical image data. Furthermore, our approach achieves an accuracy that is competitive with state-of-the-art supervised CNNs.
Link: https://arxiv.org/abs/1807.05648
====================================================
Recurrent Neural Networks with Flexible Gates using Kernel Activation Functions (Simone Scardapane - 11 July, 2018)
To this end, we replace the sigmoid function in the standard gate with a non-parametric formulation extending the recently proposed kernel activation function (KAF), with the addition of a residual skip-connection
Link: https://arxiv.org/abs/1807.04065
====================================================
Efficient Decoding Algorithms for Polar Codes based on $2\times2$ Non-Binary Kernels (Peihong Yuan - 10 July, 2018)
Polar codes based on $2\times2$ non-binary kernels are discussed in this work. The kernel over $\text{GF}(q)$ is selected by maximizing the polarization effect and using Monte-Carlo simulation
Link: https://arxiv.org/abs/1807.03767
====================================================
Computing Kernels in Parallel: Lower and Upper Bounds (Max Bannach - 10 July, 2018)
An intriguing finding is that there are complex trade-offs between kernel size and the depth of the circuits needed to compute them: For the vertex cover problem, an exponential kernel can be computed by AC$^0$-circuits, a quadratic kernel by TC$^0$-circuits, and a linear kernel by randomized NC-circuits with derandomization being possible only if it is also possible for the matching problem. We also present natural problems for which computing kernels is inherently sequential.
Link: https://arxiv.org/abs/1807.03604
====================================================
Domain2Vec: Deep Domain Generalization (Aniket Anand Deshmukh - 8 July, 2018)
The proposed algorithm, D2V extends the idea of distribution regression and kernelized domain generalization to the neural networks setting. We evaluate our algorithm against standard domain generalization datasets for image classification and outperform other state of the art algorithms.
Link: https://arxiv.org/abs/1807.02919
====================================================
3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data (Maurice Weiler - 6 July, 2018)
These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived in this paper
Link: https://arxiv.org/abs/1807.02547
====================================================
Efficient ConvNets for Analog Arrays (Malte J. Rasch - 3 July, 2018)
Here, we propose to replicate the kernel matrix of a convolution layer on distinct analog arrays, and randomly divide parts of the compute among them, so that multiple kernel matrices are trained in parallel. With this modification, analog arrays execute ConvNets with an acceleration factor that is proportional to the number of kernel matrices used per layer (here tested 16-128)
Link: https://arxiv.org/abs/1807.01356
====================================================
One-Class Kernel Spectral Regression for Outlier Detection (Shervin Rahimzadeh Arashloo - 20 August, 2018)
In particular, it is shown that the dominant complexity of the proposed method is the complexity of computing the kernel matrix. Additional appealing characteristics of the proposed one-class classifier are: 1-the ability to be trained in an incremental fashion (allowing for application in streaming data scenarios while also reducing the computational complexity in a non-streaming operation mode); 2-being unsupervised, but providing the option for refining the solution using negative training examples, when available; Last but not least, 3-the use of the kernel trick which facilitates a nonlinear mapping of the data into a high-dimensional feature space to seek better solutions.
Link: https://arxiv.org/abs/1807.01085
====================================================
Linear Combination of Distance Measures for Surrogate Models in Genetic Programming (Martin Zaefferer - 3 July, 2018)
We compare the measures and suggest to use their linear combination in a kernel.
Link: https://arxiv.org/abs/1807.01019
====================================================
A First Analysis of Kernels for Kriging-based Optimization in Hierarchical Search Spaces (Martin Zaefferer - 3 July, 2018)
We discuss an existing kernel and propose alternatives. An artificial test function is used to investigate how different kernels and assumptions affect model quality and search performance.
Link: https://arxiv.org/abs/1807.01011
====================================================
A Novel Geometric Framework on Gram Matrix Trajectories for Human Behavior Understanding (Anis Kacem - 29 June, 2018)
Specifically, our approach involves three steps: (1) landmarks are first mapped into the Riemannian manifold of positive semidefinite matrices of fixed-rank to build time-parameterized trajectories; (2) a temporal warping is performed on the trajectories, providing a geometry-aware (dis-)similarity measure between them; (3) finally, a pairwise proximity function SVM is used to classify them, incorporating the (dis-)similarity measure into the kernel function
Link: https://arxiv.org/abs/1807.00676
====================================================
Multi-modal Egocentric Activity Recognition using Audio-Visual Features (Mehmet Ali ArabacÄ± - 2 July, 2018)
In this work, we propose a new framework for egocentric activity recognition problem based on combining audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). Then, the extracted multi-modal features are adaptively fused by MKL classifiers in which both the feature and kernel selection/weighing and recognition tasks are performed together
Link: https://arxiv.org/abs/1807.00612
====================================================
Automatic Rank Selection for High-Speed Convolutional Neural Network (Hyeji Kim - 29 June, 2018)
Low-rank decomposition plays a central role in accelerating convolutional neural network (CNN), and the rank of decomposed kernel-tensor is a key parameter that determines the complexity and accuracy of a neural network. Our method combined with truncated-SVD outperforms state-of-the-art methods in terms of inference and training time at almost the same accuracy.
Link: https://arxiv.org/abs/1806.10821
====================================================
Auto-Keras: Efficient Neural Architecture Search with Network Morphism (Haifeng Jin - 3 October, 2018)
In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search by introducing a neural network kernel and a tree-structured acquisition function optimization algorithm, which more efficiently explores the search space. Intensive experiments have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods
Link: https://arxiv.org/abs/1806.10282
====================================================
Dual SVM Training on a Budget (Sahar Qaadan - 26 June, 2018)
Budget methods are effective for reducing the training time of kernel SVM while retaining high accuracy
Link: https://arxiv.org/abs/1806.10182
====================================================
Detection of Alzheimers Disease from MRI using Convolutional Neural Network with Tensorflow (Gururaj Awate - 26 June, 2018)
Enter CNN, Convolutional Neural Networks are a hybrid of Kernel Convolutions and Neural Networks. Kernel Convolutions is a technique that uses filters to recognize and segment images based on features
Link: https://arxiv.org/abs/1806.10170
====================================================
Parameterized algorithms and data reduction for safe convoy routing (RenÃ© van Bevern - 25 June, 2018)
We also show a polynomial-time data reduction algorithm that reduces any problem instance to an equivalent instance (a so-called problem kernel) of size polynomial in the vertex cover number of the input graph. Regarding tree-like graphs, we obtain a $2^{O(\mathrm{tw})}\cdot \ell^2\cdot n$-time algorithm for graphs of treewidth $\mathrm{tw}$, show that there is no problem kernel with size polynomial in $\mathrm{tw}$, yet show a problem kernel with size polynomial in the feedback edge number of the input graph.
Link: https://arxiv.org/abs/1806.09540
====================================================
A Deeper Look at Power Normalizations (Piotr Koniusz - 24 June, 2018)
Linearization of such a kernel results in a positive definite matrix capturing the second-order statistics of the feature vectors, to which PN operators are applied. Our results demonstrate state-of-the-art performance across all these tasks.
Link: https://arxiv.org/abs/1806.09183
====================================================
SSIMLayer: Towards Robust Deep Representation Learning via Nonlinear Structural Similarity (Ahmed Abobakr - 28 July, 2018)
The core of its computations is evaluating the components of the structural similarity metric (SSIM) in a setting that allows the kernels to learn to match structural information
Link: https://arxiv.org/abs/1806.09152
====================================================
Deep Orthogonal Representations: Fundamental Properties and Applications (Hsiang Hsu - 21 June, 2018)
Examples include PCA, CCA, Kernel/Deep CCA, the ACE algorithm and correspondence analysis (CA)
Link: https://arxiv.org/abs/1806.08449
====================================================
Optimising finite-difference methods for PDEs through parameterised time-tiling in Devito (Nicholas Sim - 21 June, 2018)
The modelling process for researchers is not straightforward either, requiring models with differential equations to be translated into stencil kernels, then optimised separately
Link: https://arxiv.org/abs/1806.08299
====================================================
Injecting Relational Structural Representation in Neural Networks for Question Similarity (Antonio Uva - 20 June, 2018)
In this paper, we propose to inject structural representations in NNs by (i) learning an SVM model using Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus
Link: https://arxiv.org/abs/1806.08009
====================================================
Novel Convolution Kernels for Computer Vision and Shape Analysis based on Electromagnetism (Dominique Beaini - 20 June, 2018)
Therefore, this paper focuses on the development of the electromagnetic kernels and on their application on images for shape and stroke analysis. It also presents several interesting features of electromagnetic kernels, such as resolution, size and orientation independence, robustness to noise and deformation, long distance stroke interaction and ability to work with 3D images
Link: https://arxiv.org/abs/1806.07996
====================================================
An empirical study on evaluation metrics of generative adversarial networks (Qiantong Xu - 16 August, 2018)
Based on these results, we observe that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to satisfy most of the desirable properties, provided that the distances between samples are computed in a suitable feature space
Link: https://arxiv.org/abs/1806.07755
====================================================
Neural Tangent Kernel: Convergence and Generalization in Neural Networks (Arthur Jacot - 20 June, 2018)
While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping
Link: https://arxiv.org/abs/1806.07572
====================================================
On the Metric Distortion of Embedding Persistence Diagrams into Reproducing Kernel Hilbert Spaces (Mathieu Carriere - 18 June, 2018)
Due to the nonlinearity of the space of persistence diagrams equipped with their {\em diagram distances}, most of the recent attempts at using persistence diagrams in Machine Learning have been done through kernel methods, i.e., embeddings of persistence diagrams into Reproducing Kernel Hilbert Spaces (RKHS), in which all computations can be performed easily. Since persistence diagrams enjoy theoretical stability guarantees for the diagram distances, the {\em metric properties} of a kernel $k$, i.e., the relationship between the RKHS distance $d_k$ and the diagram distances, are of central interest for understanding if the persistence diagram guarantees carry over to the embedding
Link: https://arxiv.org/abs/1806.06924
====================================================
Deep Spatiotemporal Representation of the Face for Automatic Pain Intensity Estimation (Mohammad Tavakolian - 18 June, 2018)
The proposed model is built by stacking several convolutional modules where each module encompasses a 3D convolution kernel with a fixed temporal depth, several parallel 3D convolutional kernels with different temporal depths, and an average pooling layer. Extensive experiments on the UNBC-McMaster Shoulder Pain Expression Archive database show that our proposed model yields in a promising performance compared to the state-of-the-art in automatic pain intensity estimation.
Link: https://arxiv.org/abs/1806.06793
====================================================
Predicting Switching Graph Labelings with Cluster Specialists (Mark Herbster - 17 June, 2018)
We also give an algorithm based on a kernelized Perceptron with an intermediate per-trial time complexity of $\mathcal{O}(n)$ and a mistake bound which is not strictly comparable
Link: https://arxiv.org/abs/1806.06439
====================================================
High-speed Tracking with Multi-kernel Correlation Filters (Ming Tang - 17 June, 2018)
We reformulate the MKL version of CF objective function with its upper bound, alleviating the negative mutual interference of different kernels significantly. Extensive experiments on public datasets show that our method is superior to state-of-the-art algorithms for target objects of small move at very high speed.
Link: https://arxiv.org/abs/1806.06418
====================================================
Component SPD Matrices: A lower-dimensional discriminative data descriptor for image set classification (Kai-Xuan Chen - 16 June, 2018)
Finally, the CSPD matrix appears in the form of the kernel matrix for all the sub-image sets, and CSPDi,j denotes the similarity between i-th sub-image set and j-th sub-image set. Here, the Riemannian kernel is shown to satisfy the Mercer's theorem, so our proposed CSPD matrix is symmetric and positive definite and also lies on a Riemannian manifold
Link: https://arxiv.org/abs/1806.06178
====================================================
Riemannian kernel based NystrÃ¶m method for approximate infinite-dimensional covariance descriptors with application to image set classification (Kai-Xuan Chen - 16 June, 2018)
We propose a novel framework for representing image sets by approximating infinite-dimensional CovDs in the paradigm of the NystrÃ¶m method based on a Riemannian kernel. We then extend the NystrÃ¶m method to the SPD manifold and obtain the approximations of CovDs in RKHS (Reproducing Kernel Hilbert Space)
Link: https://arxiv.org/abs/1806.06177
====================================================
SafeSpec: Banishing the Spectre of a Meltdown with Leakage-Free Speculation (Khaled N. Khasawneh - 15 June, 2018)
Several attack variations are possible, allowing arbitrary exposure of the full kernel memory to an unprivileged attacker
Link: https://arxiv.org/abs/1806.05179
====================================================
Introducing user-prescribed constraints in Markov chains for nonlinear dimensionality reduction (Purushottam D. Dixit - 6 August, 2018)
Typically, the Markov chain is fully specified by the kernel through row normalization. Here, we introduce a path entropy maximization based approach to derive the transition probabilities of Markov chains using a kernel and additional user-specified constraints
Link: https://arxiv.org/abs/1806.05096
====================================================
Differentiable Compositional Kernel Learning for Gaussian Processes (Shengyang Sun - 5 August, 2018)
It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient-based optimization. We show that the NKN is universal for the class of stationary kernels
Link: https://arxiv.org/abs/1806.04326
====================================================
Transformationally Identical and Invariant Convolutional Neural Networks through Symmetric Element Operators (Shih Chung B. Lo - 10 July, 2018)
Hence the use of same TI property for every kernel in the CNN would serve as an orientation or a translation independent training guide in conjunction with the error-backpropagation during the training. This TI kernel property is desirable for applications requiring a highly consistent output result from corresponding transformation versions of an input
Link: https://arxiv.org/abs/1806.03636
====================================================
Training Faster by Separating Modes of Variation in Batch-normalized Models (Mahdi M. Kalayeh - 7 June, 2018)
In this work, we study BN from the viewpoint of Fisher kernels. That means BN can be explained in terms of kernels that naturally emerge from the probability density function of the underlying data distribution
Link: https://arxiv.org/abs/1806.02892
====================================================
Regularization by Denoising: Clarifications and New Interpretations (Edward T. Reehorst - 25 September, 2018)
We then show tight connections between SMD, kernel density estimation, and constrained minimum mean-squared error denoising
Link: https://arxiv.org/abs/1806.02296
====================================================
Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds (Pedro Hermosilla - 25 September, 2018)
Learning is enabled by four key novelties: first, representing the convolution kernel itself as a multilayer perceptron; second, phrasing convolution as a Monte Carlo integration problem, third, using this notion to combine information from multiple samplings at different levels; and fourth using Poisson disk sampling as a scalable means of hierarchical point cloud learning. By employing our method in hierarchical network architectures we can outperform most of the state-of-the-art networks on established point cloud segmentation, classification and normal estimation benchmarks
Link: https://arxiv.org/abs/1806.01759
====================================================
Sampling and Super-resolution of Sparse Signals Beyond the Fourier Domain (Ayush Bhandari - 4 June, 2018)
In analogy to the Shannon's sampling framework, we specify sampling theorems for recovery of sparse signals considering three specific cases: (1) sampling with arbitrary, bandlimited kernels, (2) sampling with smooth, time-limited kernels and, (3) recovery from Gabor transform measurements linked with the SAFT domain
Link: https://arxiv.org/abs/1806.01478
====================================================
GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks (Amir Yazdanbakhsh - 10 May, 2018)
This operator first inserts zeros within the multidimensional input, then convolves a kernel over this expanded array to add information to the embedded zeros
Link: https://arxiv.org/abs/1806.01107
====================================================
Performance evaluation over HW/SW co-design SoC memory transfers for a CNN accelerator (A. Rios-Navarro - 9 May, 2018)
We demonstrate that for longer enough packets, the kernel-level driver solution gets better timing in computing a CNN classification example. Main advantage of using kernel-level driver is to have safer solutions and to have tasks scheduling in the OS to manage other important processes for our application, like frames collection from sensors and their normalization.
Link: https://arxiv.org/abs/1806.01106
====================================================
Consolidating the innovative concepts towards Exascale computing for Co-Design of Co-Applications ll: Co-Design Automation - Workload Characterization ( Dhanasekar - 29 April, 2018)
None of the current day benchmark suites encompasses applications and kernels that can match the attributes of customizable workload model proposed in this paper
Link: https://arxiv.org/abs/1806.01104
====================================================
Targeted Kernel Networks: Faster Convolutions with Attentive Regularization (Kashyap Chitta - 21 September, 2018)
Traditional CNNs of different types and structures can be modified with this idea into equivalent Targeted Kernel Networks (TKNs), while keeping the network size nearly identical. By restricting kernel ROIs, we reduce the number of sliding convolutional operations performed throughout the network in its forward pass, speeding up both training and inference
Link: https://arxiv.org/abs/1806.00523
====================================================
A Parameterized Complexity View on Collapsing k-Cores (Junjie Luo - 31 May, 2018)
Furthermore, we show that Collapsed k-Core is in FPT when parameterized by the treewidth of the input graph and presumably does not admit a polynomial kernel when parameterized by the vertex cover number of the input graph.
Link: https://arxiv.org/abs/1805.12453
====================================================
FPGA-based Acceleration of FT Convolution for Pulsar Search Using OpenCL (Haomiao Wang - 28 June, 2018)
The performance and power consumption are evaluated using multiple FPGA devices simultaneously and compared with GPU results, which is achieved by porting FPGA-based OpenCL kernels
Link: https://arxiv.org/abs/1805.12280
====================================================
Omega: An Architecture for AI Unification (Eray Ãzkural - 16 May, 2018)
We retain the basic design of a fundamental algorithmic substrate called an "AI kernel" for problem solving and basic cognitive functions like memory, and a larger, modular architecture that re-uses the kernel in many ways
Link: https://arxiv.org/abs/1805.12069
====================================================
Regularized Kernel and Neural Sobolev Descent: Dynamic MMD Transport (Youssef Mroueh - 30 May, 2018)
We introduce Regularized Kernel and Neural Sobolev Descent for transporting a source distribution to a target distribution along smooth paths of minimum kinetic energy (defined by the Sobolev discrepancy), related to dynamic optimal transport. In the kernel version, we give a simple algorithm to perform the descent along gradients of the Sobolev critic, and show that it converges asymptotically to the target distribution in the MMD sense
Link: https://arxiv.org/abs/1805.12062
====================================================
Anonymous Walk Embeddings (Sergey Ivanov - 8 June, 2018)
Here, we coherently propose an approach for embedding entire graphs and show that our feature representations with SVM classifier increase classification accuracy of CNN algorithms and traditional graph kernels. Overall, our work represents a new scalable unsupervised learning of state-of-the-art representations of entire graphs.
Link: https://arxiv.org/abs/1805.11921
====================================================
Multiple Manifolds Metric Learning with Application to Image Set Classification (Rui Wang - 30 May, 2018)
A metric Learning method has been devised to embed these kernel spaces into a lower dimensional common subspace for classification. The state-of-the-art results achieved on three datasets corresponding to two different classification tasks, namely face recognition and object categorization, demonstrate the effectiveness of the proposed method.
Link: https://arxiv.org/abs/1805.11918
====================================================
NetLSD: Hearing the Shape of a Graph (Anton Tsitsulin - 29 May, 2018)
Graph comparisons still rely on direct approaches, graph kernels, or representation-based methods, which are all inefficient and impractical for large graph collections.
Link: https://arxiv.org/abs/1805.10712
====================================================
Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance (Neal Jean - 25 May, 2018)
We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.
Link: https://arxiv.org/abs/1805.10407
====================================================
Less is More: Simultaneous View Classification and Landmark Detection for Abdominal Ultrasound Images (Zhoubing Xu - 4 June, 2018)
This network is integrated to perform view classification and landmark detection simultaneously; it is also equipped with global convolutional kernels, coordinate constraints, and a conditional adversarial module to leverage the performances
Link: https://arxiv.org/abs/1805.10376
====================================================
Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes (Loucas Pillaud-Vivien - 28 June, 2018)
In order to define the notion of hardness and show that our predictive performances are optimal, we consider potentially infinite-dimensional models and notions typically associated to kernel methods, namely, the decay of eigenvalues of the covariance matrix of the features and the complexity of the optimal predictor as measured through the covariance matrix. We illustrate our results on synthetic experiments with non-linear kernel methods and on a classical benchmark with a linear model.
Link: https://arxiv.org/abs/1805.10074
====================================================
KONG: Kernels for ordered-neighborhood graphs (Moez Draief - 29 May, 2018)
Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs.
Link: https://arxiv.org/abs/1805.10014
====================================================
Progressive Transient Photon Beams (Julio Marco - 24 May, 2018)
Then, we develop a progressive version of spatio-temporal density estimations, that converges to the correct solution with finite memory requirements by iteratively averaging several realizations of independent renders with a progressively reduced kernel bandwidth. We derive the optimal convergence rates accounting for space and time kernels, and demonstrate our method against previous consistent transient rendering methods for participating media.
Link: https://arxiv.org/abs/1805.09562
====================================================
Use of symmetric kernels for convolutional neural networks (Viacheslav Dudar - 23 May, 2018)
We also study other types of symmetric kernels which lead to vertical flip invariance, and approximate rotational invariance. We show that usage of such kernels acts as regularizer, and improves generalization of the convolutional neural networks at the cost of more complicated training process.
Link: https://arxiv.org/abs/1805.09421
====================================================
Non-convex non-local flows for saliency detection (IvÃ¡n RamÃ­rez - 23 May, 2018)
A fast convolutional kernel based approximated solution is computed
Link: https://arxiv.org/abs/1805.09408
====================================================
Convolutional Polar Codes on Channels with Memory (Benjamin Bourassa - 23 May, 2018)
Furthermore, the polar code construction was extended by replacing the block polarization kernel by a convoluted kernel
Link: https://arxiv.org/abs/1805.09378
====================================================
Depth versus Breadth in Convolutional Polar Codes (Maxime Tremblay - 23 May, 2018)
The main conclusion drawn from our study is that increasing the convolution depth is more efficient than increasing the polarization kernel's breadth as previously explored.
Link: https://arxiv.org/abs/1805.09306
====================================================
Infinite-Task Learning with RKHSs (Romain Brault - 11 October, 2018)
We leverage tools from operator-valued kernels and the associated vector-valued RKHSs that provide an explicit control over the role of the hyperparameters, and also allows us to consider new type of constraints
Link: https://arxiv.org/abs/1805.08809
====================================================
Information Constraints on Auto-Encoding Variational Bayes (Romain Lopez - 11 September, 2018)
We propose a framework for learning representations that relies on Auto-Encoding Variational Bayes and whose search space is constrained via kernel-based measures of independence. We show that our method out-performs the state-of-the-art in this domain.
Link: https://arxiv.org/abs/1805.08672
====================================================
Knowledge-based Fully Convolutional Network and Its Application in Segmentation of Lung CT Images (Tao Yu - 22 May, 2018)
Experiments validate our knowledge assumption about the incorporation of prior knowledge into the convolution kernels of KFCN and show that KFCN can achieve a reasonable segmentation and a satisfactory accuracy.
Link: https://arxiv.org/abs/1805.08492
====================================================
Transformations of High-Level Synthesis Codes for High-Performance Computing (Johannes de Fine Licht - 8 August, 2018)
To quantify the effect of our transformations, we use them to optimize a set of high-throughput FPGA kernels, demonstrating that they are sufficient to scale up parallelism within the hardware constraints of the target device
Link: https://arxiv.org/abs/1805.08288
====================================================
Online Learning in Kernelized Markov Decision Processes (Sayak Ray Chowdhury - 21 May, 2018)
The bounds are in terms of explicit structural parameters of the kernels, namely a novel generalization of the information gain metric from kernelized bandit, and highlight the influence of transition and reward function structure on the learning performance. Our results are applicable to multi-dimensional state and action spaces with composite kernel structures, and generalize results from the literature on kernelized bandits, and the adaptive control of parametric linear dynamical systems with quadratic costs.
Link: https://arxiv.org/abs/1805.08052
====================================================
Relating Leverage Scores and Density using Regularized Christoffel Functions (Edouard Pauwels - 21 May, 2018)
This uncovers a variational formulation for leverage scores for kernel methods and allows to elucidate their relationships with the chosen kernel as well as population density. Our main result quantitatively describes a decreasing relation between leverage score and population density for a broad class of kernels on Euclidean spaces
Link: https://arxiv.org/abs/1805.07943
====================================================
Spherical Convolutional Neural Network for 3D Point Clouds (Huan Lei - 22 May, 2018)
We specify spherical kernels with the help of neurons in each layer that in turn are associated with spatial locations. We exploit this association to avert dynamic kernel generation during network training, that enables efficient learning with high resolution point clouds
Link: https://arxiv.org/abs/1805.07872
====================================================
Density-Adaptive Kernel based Re-Ranking for Person Re-Identification (Ruo-Pei Guo - 19 May, 2018)
In this paper, we propose to exploit a density-adaptive kernel technique to perform efficient and effective re-ranking for person ReID. Specifically, we present two simple yet effective re-ranking methods, termed inverse Density-Adaptive Kernel based Re-ranking (inv-DAKR) and bidirectional Density-Adaptive Kernel based Re-ranking (bi-DAKR), which are based on a smooth kernel function with a density-adaptive parameter
Link: https://arxiv.org/abs/1805.07698
====================================================
Reconciled Polynomial Machine: A Unified Representation of Shallow and Deep Learning Models (Jiawei Zhang - 18 May, 2018)
Reconciled polynomial machine predicts the output by computing the inner product of the feature kernel function and variable reconciling function
Link: https://arxiv.org/abs/1805.07507
====================================================
Blockchain Cohomology (Wyatt Meldman-Floch - 18 May, 2018)
We use recursion schemes to define kernels admitting smooth manifolds across protocol complexes, leading to the formal definition of a Poincare protocol.
Link: https://arxiv.org/abs/1805.07047
====================================================
Towards Malware Detection via CPU Power Consumption: Data Collection Design and Analytics (Extended Version) (Robert Bridges - 16 May, 2018)
We test a one-class anomaly detection ensemble (that baselines non-infected power profiles) and several kernel-based SVM classifiers (that train on both uninfected and infected profiles) in detecting previously unseen malware and clean profiles
Link: https://arxiv.org/abs/1805.06541
====================================================
End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition (Samet Oymak - 16 May, 2018)
We develop an algorithm for simultaneously learning all the kernels from the training data. from a Gaussian distribution and the labels are generated according to planted convolutional kernels
Link: https://arxiv.org/abs/1805.06523
====================================================
Regularized Finite Dimensional Kernel Sobolev Discrepancy (Youssef Mroueh - 16 May, 2018)
Given a Kernel with finite dimensional feature map we show that the Sobolev discrepancy can be approximated from finite samples. Assuming this discrepancy is finite, the error depends on the approximation error in the function space induced by the finite dimensional feature space kernel and on a statistical error due to the finite sample approximation.
Link: https://arxiv.org/abs/1805.06441
====================================================
Distribution-based Label Space Transformation for Multi-label Learning (Zongting Lyu - 15 May, 2018)
With the optimal latent code, a kernel logistic regression function is learned for the mapping from feature space to the latent space
Link: https://arxiv.org/abs/1805.05687
====================================================
Nonlinear Dimensionality Reduction for Discriminative Analytics of Multiple Datasets (Jia Chen - 2 October, 2018)
To account for nonlinear data correlations, (linear) dPCA models for one or multiple background datasets are generalized through kernel-based learning
Link: https://arxiv.org/abs/1805.05502
====================================================
Multi-view Common Component Discriminant Analysis for Cross-view Classification (Xinge You - 19 May, 2018)
Beyond kernel extension, optimization and complexity analysis of MvCCDA are also presented for completeness. Our MvCCDA is competitive with the state-of-the-art MvSL based methods on four benchmark datasets, demonstrating its superiority.
Link: https://arxiv.org/abs/1805.05029
====================================================
Domain Adapted Word Embeddings for Improved Sentiment Classification (Prathusha K Sarma - 11 May, 2018)
The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA. Evaluation results on sentiment classification tasks show that the DA embeddings substantially outperform both generic and DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for classification.
Link: https://arxiv.org/abs/1805.04576
====================================================
On the Parameterized Complexity of Graph Modification to First-Order Logic Properties (Fedor V. Fomin - 1 October, 2018)
We establish a number of sufficient and necessary conditions on the quantification pattern of the first-order formula $Ï$ for the problem to be fixed-parameter tractable or to admit a polynomial kernel.
Link: https://arxiv.org/abs/1805.04375
====================================================
Computing Coverage Kernels Under Restricted Settings (JÃ©rÃ©my Barbay - 15 May, 2018)
We consider the Minimum Coverage Kernel problem: given a set $B$ of $d$-dimensional boxes, find a subset of $B$ of minimum size covering the same region as $B$. We consider various classes of graphs, show that Minimum Coverage Kernel remains $\mathsf{NP}$-hard even for severely restricted instances, and provide two polynomial time approximation algorithms for this problem.
Link: https://arxiv.org/abs/1805.04223
====================================================
Supervising NystrÃ¶m Methods via Negative Margin Support Vector Selection (Mert Al - 17 May, 2018)
They approximate explicit, low-dimensional feature mappings for kernel functions from the pairwise comparisons with the training data. Experimental results on six datasets show that, without increasing the complexity over unsupervised techniques, our method can significantly improve the classification performance achieved via kernel approximation methods and reduce the number of features needed to reach or exceed the performance of the full-dimensional kernel machines.
Link: https://arxiv.org/abs/1805.04018
====================================================
Efficient Explicit Time Stepping of High Order Discontinuous Galerkin Schemes for Waves (Svenja Schoeder - 9 May, 2018)
A crucial step towards efficiency is to evaluate operators in a matrix-free way with sum-factorization kernels. We analyze and compare the performance of state-of-the-art Runge-Kutta schemes and ADER time stepping with the proposed optimizations
Link: https://arxiv.org/abs/1805.03981
====================================================
Deep Covariance Descriptors for Facial Expression Recognition (Naima Otberdout - 10 May, 2018)
By performing the classification of the facial expressions using Gaussian kernel on SPD manifold, we show that the covariance descriptors computed on DCNN features are more efficient than the standard classification with fully connected layers and softmax. By implementing our approach using the VGG-face and ExpNet architectures with extensive experiments on the Oulu-CASIA and SFEW datasets, we show that the proposed approach achieves performance at the state of the art for facial expression recognition.
Link: https://arxiv.org/abs/1805.03869
====================================================
On the Graver basis of block-structured integer programming (Lin Chen - 9 May, 2018)
We provide a matching lower bounded of $Î©(n^{s_c})$, which even holds for an arbitrary non-zero integral element in the kernel space
Link: https://arxiv.org/abs/1805.03741
====================================================
Learning representations for multivariate time series with missing data using Temporal Kernelized Autoencoders (Filippo Maria Bianchi - 9 May, 2018)
Our autoencoder learns fixed-length vectorial representations, whose pairwise similarities are aligned with a kernel function that operates in input space and handles missing values
Link: https://arxiv.org/abs/1805.03473
====================================================
Several Tunable GMM Kernels (Ping Li - 8 May, 2018)
In this study, we propose a series of "tunable GMM kernels" which are simple and perform largely comparably to tree methods on the same datasets
Link: https://arxiv.org/abs/1805.02830
====================================================
eBPF-based Content and Computation-aware Communication for Real-time Edge Computing (Sabur Baidya - 7 May, 2018)
The framework supports multi-streaming using the extended Berkeley Packet Filter (eBPF), where the traffic flow and packet replication for each specific computation process is controlled by a program running inside an in-kernel Virtual Ma- chine (VM)
Link: https://arxiv.org/abs/1805.02797
====================================================
EngineCL: Usability and Performance in Heterogeneous Computing (RaÃºl Nozal - 7 May, 2018)
This paper presents EngineCL, a new OpenCL-based runtime system that notably simplifies the execution of a single massive data-parallel kernel on a heterogeneous system
Link: https://arxiv.org/abs/1805.02755
====================================================
Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling (Chenyan Xiong - 3 May, 2018)
This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents. KESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience
Link: https://arxiv.org/abs/1805.01334
====================================================
Single-Channel Blind Source Separation for Singing Voice Detection: A Comparative Study (Dominique Fourer - 3 May, 2018)
Second, we propose an extension of the KAM method for which we propose a novel training algorithm used to compute a source-specific kernel from a given isolated source signal
Link: https://arxiv.org/abs/1805.01201
====================================================
Maximum cuts in edge-colored graphs (Luerbio Faria - 2 May, 2018)
On the positive side, we prove that Maximum Colored Cut is fixed-parameter tractable when parameterized by either $k$ or $p$, by constructing a cubic kernel in both cases.
Link: https://arxiv.org/abs/1805.00858
====================================================
Fast and Efficient Depth Map Estimation from Light Fields (Yuriy Anisimov - 1 May, 2018)
Line fitting is based on color values comparison using kernel density estimation
Link: https://arxiv.org/abs/1805.00264
====================================================
Learning Explicit Deep Representations from Deep Kernel Networks (Mingyuan Jiu - 30 April, 2018)
Deep kernel learning aims at designing nonlinear combinations of multiple standard elementary kernels by training deep networks. In this paper, we address the issue of efficient computation in Deep Kernel Networks (DKNs) by designing effective maps in the underlying Reproducing Kernel Hilbert Spaces
Link: https://arxiv.org/abs/1804.11159
====================================================
Big Data Quantum Support Vector Clustering (Arit Kumar Bishwas - 29 April, 2018)
Support vector clustering algorithm is a well-known clustering algorithm based on support vector machines and Gaussian kernels. We have developed a quantum algorithm which is based on quantum support vector machine and the quantum kernel (Gaussian kernel and polynomial kernel) formulation
Link: https://arxiv.org/abs/1804.10905
====================================================
QDR-Tree: An Efcient Index Scheme for Complex Spatial Keyword Query (Xinshi Zang - 27 April, 2018)
In the keyword cluster layer, a Quad-Cluster Tree (QC-Tree) is built based on the hierarchical clustering algorithm using kernel k-means to classify keywords
Link: https://arxiv.org/abs/1804.10726
====================================================
Alternative parameterizations of Metric Dimension (Gregory Gutin - 27 April, 2018)
We complement this observation by showing that it does not admit a polynomial kernel even when parameterized by $vc(G) + k$. Our reduction also gives evidence for non-existence of polynomial Turing kernels.
Link: https://arxiv.org/abs/1804.10670
====================================================
Regularized Nonparametric Volterra Kernel Estimation (Georgios Birpoutsoukis - 27 April, 2018)
Based on this, prior information about the structure of the Volterra kernel is introduced via an appropriate penalization term in the least squares cost function. It is shown that the proposed method is able to deliver accurate estimates of the Volterra kernels even in the case of a small amount of data points.
Link: https://arxiv.org/abs/1804.10435
====================================================
Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees (Haim Avron - 21 May, 2018)
Random Fourier features is one of the most popular techniques for scaling up kernel methods, such as kernel ridge regression. Specifically, we approach random Fourier features from a spectral matrix approximation point of view, give tight bounds on the number of Fourier features required to achieve a spectral approximation, and show how spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression.
Link: https://arxiv.org/abs/1804.09893
====================================================
When is there a Representer Theorem? Nondifferentiable Regularisers and Banach spaces (Kevin Schlegel - 25 April, 2018)
This is the core of kernel methods in machine learning as it makes the problem computationally tractable
Link: https://arxiv.org/abs/1804.09605
====================================================
Quantitative Susceptibility Map Reconstruction Using Annihilating Filter-based Low-Rank Hankel Matrix Approach (Hyun-Seo Ahn - 25 April, 2018)
Quantitative susceptibility mapping (QSM) inevitably suffers from streaking artifacts caused by zeros on the conical surface of the dipole kernel in k-space
Link: https://arxiv.org/abs/1804.09396
====================================================
Generalized Gaussian Kernel Adaptive Filtering (Tomoya Wada - 25 April, 2018)
Different from conventional kernel adaptive filters, the proposed regressor is a superposition of Gaussian kernels with all different parameters, which makes such regressor more flexible. The kernel adaptive filtering algorithm is established together with a l1-regularized least squares to avoid overfitting and the increase of dimensionality of the dictionary
Link: https://arxiv.org/abs/1804.09348
====================================================
Layered Fields for Natural Tessellations on Surfaces (Rhaleb Zayer - 24 April, 2018)
As our approach relies mainly on basic linear algebra kernels, it lends itself to efficient implementation on modern graphics hardware.
Link: https://arxiv.org/abs/1804.09152
====================================================
Polynomial Kernels for Hitting Forbidden Minors under Structural Parameterizations (Bart M. P. Jansen - 24 April, 2018)
We investigate polynomial-time preprocessing for the problem of hitting forbidden minors in a graph, using the framework of kernelization
Link: https://arxiv.org/abs/1804.08885
====================================================
Large Receptive Field Networks for High-Scale Image Super-Resolution (George Seif - 22 April, 2018)
In particular, we use two different methods to expand the network receptive field: 1-D separable kernels and atrous convolutions. We conduct considerable experiments to study the performance of various arrangement schemes of the 1-D separable kernels and atrous convolution in terms of accuracy (PSNR / SSIM), parameter count, and speed, while focusing on the more challenging high upscaling factors
Link: https://arxiv.org/abs/1804.08181
====================================================
Designing Practical PTASes for Minimum Feedback Vertex Set in Planar Graphs (Glencora Borradaile - 20 April, 2018)
We present two algorithms for the minimum feedback vertex set problem in planar graphs: an $O(n \log n)$ PTAS using a linear kernel and balanced separator, and a heuristic algorithm using kernelization and local search
Link: https://arxiv.org/abs/1804.07869
====================================================
Generating Music using an LSTM Network (Nikhil Kotecha - 18 April, 2018)
The probabilistic model presented is a Bi-axial LSTM trained with a kernel reminiscent of a convolutional kernel
Link: https://arxiv.org/abs/1804.07300
====================================================
Multi-view Hybrid Embedding: A Divide-and-Conquer Approach (Jiamiao Xu - 19 April, 2018)
The kernel extension is conducted to further boost the representation power of MvHE. Our methods demonstrate overwhelming advantages against the state-of-the-art MvSL based cross-view classification approaches in terms of classification accuracy and robustness.
Link: https://arxiv.org/abs/1804.07237
====================================================
Large-scale Nonlinear Variable Selection via Kernel Random Features (Magda GregorovÃ¡ - 1 September, 2018)
This is the first kernel-based variable selection method applicable to large datasets. It sidesteps the typical poor scaling properties of kernel methods by mapping the inputs into a relatively low-dimensional space of random features
Link: https://arxiv.org/abs/1804.07169
====================================================
VH-HFCN based Parking Slot and Lane Markings Segmentation on Panoramic Surround View (Yan Wu - 6 May, 2018)
The VH-stage consists of two independent linear convolution paths with vertical and horizontal convolution kernels respectively
Link: https://arxiv.org/abs/1804.07027
====================================================
Network Signatures from Image Representation of Adjacency Matrices: Deep/Transfer Learning for Subgraph Classification (Kshiteesh Hegde - 17 April, 2018)
Our conclusions from several datasets are that (a) deep learning using our structured image features performs the best compared to benchmark graph kernel and classical features based methods; and, (b) pure transfer learning works effectively with minimum interference from the user and is robust against small data.
Link: https://arxiv.org/abs/1804.06275
====================================================
IGCV$2$: Interleaved Structured Sparse Convolutional Neural Networks (Guotian Xie - 17 April, 2018)
In this paper, we study the problem of designing efficient convolutional neural network architectures with the interest in eliminating the redundancy in convolution kernels. In addition to structured sparse kernels, low-rank kernels and the product of low-rank kernels, the product of structured sparse kernels, which is a framework for interpreting the recently-developed interleaved group convolutions (IGC) and its variants (e.g., Xception), has been attracting increasing interests.
Link: https://arxiv.org/abs/1804.06202
====================================================
Parametric Models for Mutual Kernel Matrix Completion (Rachelle Rivero - 17 April, 2018)
A limitation of kernel matrix estimations done via optimization of an objective function is that the positive definiteness of the result is not guaranteed. In view of this limitation, our proposed methods employ the LogDet divergence, which ensures the positive definiteness of the resulting inferred kernel matrix
Link: https://arxiv.org/abs/1804.06095
====================================================
White matter fiber analysis using kernel dictionary learning and sparsity priors (Kuldeep Kumar - 15 April, 2018)
In this work, we address this by proposing a set of kernel dictionary learning and sparsity priors based methods
Link: https://arxiv.org/abs/1804.05427
====================================================
Data-efficient Neuroevolution with Kernel-Based Surrogate Models (Adam Gaier - 17 April, 2018)
Our main insight is that we can sidestep this problem by using kernel-based surrogate models, which require only the definition of a distance measure between individuals. Combining these two ideas, we introduce a surrogate-assisted neuroevolution algorithm that combines NEAT and a surrogate model built using a compatibility distance kernel
Link: https://arxiv.org/abs/1804.05364
====================================================
Mitigating Docker Security Issues (Robail Yasrab - 13 April, 2018)
The key of reason of Docker inadequate security protocols is containers sharing of Linux kernel, which can lead to risk of privileged escalations
Link: https://arxiv.org/abs/1804.05039
====================================================
Scalable and Interpretable One-class SVMs with Deep Learning and Random Fourier features (Minh-Nghia Nguyen - 13 April, 2018)
In this paper, we propose autoencoder based one-class SVM (AE-1SVM) that brings OC-SVM, with the aid of random Fourier features to approximate the radial basis kernel, into deep learning context by combining it with a representation learning architecture and jointly exploit stochastic gradient descend to obtain end-to-end training. Interestingly, this also opens up the possible use of gradient-based attribution methods to explain the decision making for anomaly detection, which has ever been challenging as a result of the implicit mappings between the input space and the kernel space
Link: https://arxiv.org/abs/1804.04888
====================================================
Learning Contracting Vector Fields For Stable Imitation Learning (Vikas Sindhwani - 13 April, 2018)
With curl-free kernels, our framework may also be viewed as a mechanism to learn potential fields and gradient flows. We develop large-scale techniques using randomized kernel approximations in this context
Link: https://arxiv.org/abs/1804.04878
====================================================
Learning Warped Guidance for Blind Face Restoration (Xiaoming Li - 16 April, 2018)
Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.
Link: https://arxiv.org/abs/1804.04829
====================================================
Causal Inference via Kernel Deviance Measures (Jovana Mitrovic - 12 April, 2018)
From a novel interpretation of the notion of asymmetry between cause and effect, we derive a corresponding asymmetry measure using the framework of reproducing kernel Hilbert spaces. Furthermore, we test our method on real-world time series data and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we outperform existing state-of-the-art methods.
Link: https://arxiv.org/abs/1804.04622
====================================================
Simultaneous Fidelity and Regularization Learning for Image Restoration (Dongwei Ren - 12 April, 2018)
For blind deconvolution, as estimation error of blur kernel is usually introduced, the subsequent non-blind deconvolution process does not restore the latent image well. Extensive experimental results demonstrate the effectiveness of the proposed model for image deconvolution with inaccurate blur kernels and rain streak removal
Link: https://arxiv.org/abs/1804.04522
====================================================
Graph Matching with Anchor Nodes: A Learning Approach (Nan Hu - 10 April, 2018)
Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges
Link: https://arxiv.org/abs/1804.03715
====================================================
Learning an Optimizer for Image Deconvolution (Dong Gong - 10 April, 2018)
As an integral component of blind image deblurring, non-blind deconvolution removes image blur with a given blur kernel, which is essential but difficult due to the ill-posed nature of the inverse problem
Link: https://arxiv.org/abs/1804.03368
====================================================
Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter (Yuki Fujimura - 9 April, 2018)
In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels
Link: https://arxiv.org/abs/1804.02836
====================================================
A $5k$-vertex Kernel for $P_2$-packing (Wenjun Li - 8 April, 2018)
We continue the study of its kernelization algorithms, and develop a $5k$-vertex kernel.
Link: https://arxiv.org/abs/1804.02801
====================================================
Does k Matter? k-NN Hubness Analysis for Kernel Additive Modelling Vocal Separation (Delia Fano Yela - 6 April, 2018)
Kernel Additive Modelling (KAM) is a framework for source separation aiming to explicitly model inherent properties of sound sources to help with their identification and separation. KAM separates a given source by applying robust statistics on the selection of time-frequency bins obtained through a source-specific kernel, typically the k-NN function
Link: https://arxiv.org/abs/1804.02325
====================================================
Normalized Cut Loss for Weakly-supervised CNN Segmentation (Meng Tang - 4 April, 2018)
We focus on normalized cut loss where dense Gaussian kernel is efficiently implemented in linear time by fast Bilateral filtering
Link: https://arxiv.org/abs/1804.01346
====================================================
PhaseNet for Video Frame Interpolation (Simone Meyer - 3 April, 2018)
Recent deep learning approaches that rely on kernels to represent motion can only alleviate these problems to some extent
Link: https://arxiv.org/abs/1804.00884
====================================================
Sparse Matrix-Matrix Multiplication on Multilevel Memory Architectures : Algorithms and Experiments (Mehmet Deveci - 2 April, 2018)
This paper investigates the performance of sparse matrix multiplication kernels on two leading high-performance computing architectures -- Intel's Knights Landing processor and NVIDIA's Pascal GPU. We describe a data placement method and a chunking-based algorithm for our kernels that exploits the existence of the multiple memory spaces in each hardware platform
Link: https://arxiv.org/abs/1804.00695
====================================================
Algebraic matching techniques for fast decoding of polar codes with Reed-Solomon kernel (Peter Trifonov - 2 April, 2018)
We propose to reduce the decoding complexity of polar codes with non-Arikan kernels by employing a (near) ML decoding algorithm for the codes generated by kernel rows. The obtained algorithm is used as a building block to obtain a decoder for polar codes with Reed-Solomon kernel.
Link: https://arxiv.org/abs/1804.00620
====================================================
Non-Linear Temporal Subspace Representations for Activity Recognition (Anoop Cherian - 27 March, 2018)
We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.
Link: https://arxiv.org/abs/1803.11064
====================================================
Canonical Correlation Analysis of Datasets with a Common Source Graph (Jia Chen - 27 March, 2018)
The resultant graph-kernel (gk) CCA is also obtained in closed form. Finally, corroborating image classification tests over several real datasets are presented to showcase the merits of the novel linear, dual, and kernel approaches relative to competing alternatives.
Link: https://arxiv.org/abs/1803.10309
====================================================
Heat Kernel analysis of Syntactic Structures (Andrew Ortegaray - 26 March, 2018)
We consider two different data sets of syntactic parameters and we discuss how to detect relations between parameters through a heat kernel method developed by Belkin-Niyogi, which produces low dimensional representations of the data, based on Laplace eigenfunctions, that preserve neighborhood information
Link: https://arxiv.org/abs/1803.09832
====================================================
Kernel-based Detection of Coincidentally Correct Test Cases to Improve Fault Localization Effectiveness (Farid Feyzi - 25 March, 2018)
In this regard, a new method is proposed that uses a support vector machine (SVM) with a customized kernel function. To build the kernel function, we applied a new sequence-matching algorithm that measures the similarities between passing and failing executions
Link: https://arxiv.org/abs/1803.09226
====================================================
Learning Deep Context-Network Architectures for Image Annotation (Mingyuan Jiu - 23 March, 2018)
In this paper, we introduce a novel context-aware kernel design framework based on deep learning. We apply this context and kernel learning framework to image classification using the challenging ImageCLEF Photo Annotation benchmark; the latter shows that our deep context learning provides highly effective kernels for image classification as corroborated through extensive experiments.
Link: https://arxiv.org/abs/1803.08794
====================================================
Sentiment Analysis of Comments on Rohingya Movement with Support Vector Machine (Hemayet Ahmed Chowdhury - 22 March, 2018)
We specifically used a support vector machine with linear kernel
Link: https://arxiv.org/abs/1803.08790
====================================================
Towards Universal Representation for Unseen Action Recognition (Yi Zhu - 22 March, 2018)
We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover 'building-blocks' from the large-scale ActivityNet dataset using distribution kernels
Link: https://arxiv.org/abs/1803.08460
====================================================
Multi-view Metric Learning in Vector-valued Kernel Spaces (Riikka Huusari - 21 March, 2018)
In order to scale the computation to large training sets, a block-wise Nystr{Ã¶}m approximation of the multi-view kernel matrix is introduced. We justify our approach theoretically and experimentally, and show its performance on real-world datasets against relevant state-of-the-art methods.
Link: https://arxiv.org/abs/1803.07821
====================================================
A Feature-Driven Active Framework for Ultrasound-Based Brain Shift Compensation (Jie Luo - 20 March, 2018)
Kernels of the GP are estimated by using variograms and a discrete grid search method
Link: https://arxiv.org/abs/1803.07682
====================================================
Asymmetric kernel in Gaussian Processes for learning target variance (Silvia L. Pintea - 19 March, 2018)
Subsequently, each center selects an individualized kernel metric. This enables each center to adjust the kernel space in its vicinity in correspondence with the topology of the targets --- a multi-modal approach
Link: https://arxiv.org/abs/1803.06952
====================================================
Depth-aware CNN for RGB-D Segmentation (Weiyue Wang - 18 March, 2018)
Convolutional neural networks (CNN) are limited by the lack of capability to handle geometric information due to the fixed grid kernel structure
Link: https://arxiv.org/abs/1803.06791
====================================================
A Kernel Theory of Modern Data Augmentation (Tri Dao - 16 March, 2018)
We connect this general approximation framework to prior work in invariant kernels, tangent propagation, and robust optimization. Under this model, we show that performing $k$-nearest neighbors with data augmentation is asymptotically equivalent to a kernel classifier
Link: https://arxiv.org/abs/1803.06084
====================================================
Robustness to incorrect system models in stochastic control (Ali Devran Kara - 28 August, 2018)
We show that continuity and robustness cannot be established under weak and setwise convergences of transition kernels in general, but that the expected induced cost is robust under total variation in that it is continuous in the mismatch of transition kernels under convergence in total variation. By imposing further assumptions, we show that the optimal cost can be made continuous under weak convergence of transition kernels as well
Link: https://arxiv.org/abs/1803.06046
====================================================
IDEL: In-Database Entity Linking with Neural Embeddings (Torsten Kilias - 13 March, 2018)
The system achieves zero cost for data shipping and transformation by utilizing MonetDB's ability to embed Python processes in the database kernel and exchange data in NumPy arrays
Link: https://arxiv.org/abs/1803.04884
====================================================
Correction by Projection: Denoising Images with Generative Adversarial Networks (Subarna Tripathi - 12 March, 2018)
To achieve the highest possible denoising quality, the best performing signal processing based methods, such as BM3D, require an estimate of the blur kernel.
Link: https://arxiv.org/abs/1803.04477
====================================================
Towards a Multi-array Architecture for Accelerating Large-scale Matrix Multiplication on FPGAs (Junzhong Shen - 10 March, 2018)
Large-scale floating-point matrix multiplication is a fundamental kernel in many scientific and engineering applications
Link: https://arxiv.org/abs/1803.03790
====================================================
Local Kernels that Approximate Bayesian Regularization and Proximal Operators (Frank Ong - 9 March, 2018)
Our results are valid for small regularization strength but the approach is powerful enough to be useful for a wide range of applications because we expose how to derive a "kernelized" solution to these problems that approximates the global solution in one-shot, using only local operations. As another side benefit in the reverse direction, given a local data-adaptive filter constructed with a particular choice of kernel, we enable the interpretation of such filters in the variational/Bayesian/proximal framework.
Link: https://arxiv.org/abs/1803.03711
====================================================
A Polynomial Kernel for Diamond-Free Editing (Yixin Cao - 2 May, 2018)
The incompressibility dichotomy for $H$ being a 3-connected graph and the classical complexity dichotomy suggest that except for $H$ being a complete/empty graph, $H$-free editing problems admit polynomial kernels only for a few small graphs $H$. Additionally, we give a cubic-vertex kernel for the diamond-free edge deletion problem, which is far simpler than the previous kernel of the same size for the problem.
Link: https://arxiv.org/abs/1803.03358
====================================================
Deep Semantic Face Deblurring (Ziyi Shen - 16 March, 2018)
We train the network with perceptual and adversarial losses to generate photo-realistic results and develop an incremental training strategy to handle random blur kernels in the wild. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm restores sharp images with more facial details and performs favorably against state-of-the-art methods in terms of restoration quality, face recognition and execution speed.
Link: https://arxiv.org/abs/1803.03345
====================================================
Borel Kernels and their Approximation, Categorically (Fredrik Dahlqvist - 8 March, 2018)
We construct a dagger symmetric monoidal category of Borel kernels where the dagger-structure is given by Bayesian inversion. We show functorial bridges between this category and categories of Banach lattices which formalize the move from kernel-based semantics to predicate transformer (backward) or state transformer (forward) semantics
Link: https://arxiv.org/abs/1803.02651
====================================================
Graph Learning from Filtered Signals: Graph System and Diffusion Kernel Identification (Hilmi E. Egilmez - 7 March, 2018)
The proposed approach can be applied to learn diffusion (heat) kernels, which are popular in various fields for modeling diffusion processes. Our experimental results demonstrate that the proposed algorithm outperforms the current state-of-the-art methods
Link: https://arxiv.org/abs/1803.02553
====================================================
Multiple Kernel $k$-means Clustering using Min-Max Optimization with $l_2$ Regularization (Seojin Bang - 6 March, 2018)
We propose a novel multiple kernel $k$-means clustering method which aims to effectively use complementary information from multiple views to identify clusters. Additionally, it allows to distill biological prior knowledge on the clustering by imposing a linear constraint on the kernel coefficients
Link: https://arxiv.org/abs/1803.02458
====================================================
Predictability of sequences and subsequences with spectrum degeneracy at periodically located points (Nikolai Dokuchaev - 19 August, 2018)
The predictability can be achieved for finite horizon with linear predictors defined by convolutions with certain kernels
Link: https://arxiv.org/abs/1803.02233
====================================================
On the Relation of Strong Triadic Closure and Cluster Deletion (Niels GrÃ¼ttemeier - 21 September, 2018)
We first show that Strong Triadic Closure admits a $4k$-vertex kernel. Then, we study parameterization by $\ell:=|E|-k$ and show that both problems are fixed-parameter tractable and unlikely to admit a polynomial kernel with respect to $\ell$
Link: https://arxiv.org/abs/1803.00807
====================================================
On the Structure of Abstract H*-Algebras (Kevin Dunne - 1 March, 2018)
We then combine these results with the dagger-kernel approach  to quantum logic of Heunen and Jacobs, which we use to prove a structure  theorem for H*-algebras
Link: https://arxiv.org/abs/1803.00705
====================================================
Planning Safe Paths through Hazardous Environments (Chris Denniston - 6 March, 2018)
We evaluate several commonly used kernels to assess their modeling performance, which includes modeling discontinuities in the data. Our results show that an additive MatÃ©rn kernel is most suitable for modeling seabed complexity data
Link: https://arxiv.org/abs/1803.00664
====================================================
Wasserstein Distance Measure Machines (Alain Rakotomamonjy - 1 March, 2018)
Instead of using  kernel mean embeddings  or generalized radial basis kernels, we  introduce embeddings  based on dissimilarity of distributions to some reference distributions denoted as templates.  Our experimental results show that this Wasserstein distance embedding performs better than  kernel mean embeddings  and computing Wasserstein distance is far more tractable than estimating pairwise Kullback-Leibler divergence of empirical distributions.
Link: https://arxiv.org/abs/1803.00250
====================================================
