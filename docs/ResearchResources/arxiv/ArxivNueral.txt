If there are any errors
please Abort, and run `arxiv_required` for required package installation, and start again
Please wait while we phrase the requested information from global arxiv[arxiv.org] servers 
------------>
---------------------------->
------------------------------------------------------>
 
Visually-aware Collaborative Food Recommendation (Xiaoyan Gao - 11 October, 2018)
To address this challenging problem, we develop a dedicated neural network-based solution \emph{Hierarchical Attention based Food Recommendation} (HAFR) which is capable of: 1) capturing the collaborative filtering effect like what similar users tend to eat; 2) inferring a user's preference at the ingredient level; and 3) learning user preference from the recipe's visual image. Extensive experiments show that our method outperforms several competing recommender solutions like Factorization Machine and Visual Bayesian Personalized Ranking with an average improvement of 12\%, offering promising results in predicting user preference on food.
Link: https://arxiv.org/abs/1810.05032
====================================================
Perfusion parameter estimation using neural networks and data augmentation (David Robben - 11 October, 2018)
A comparison on simulated CT Perfusion data shows that the neural network provides better estimations for both CBF and Tmax than a state of the art deconvolution method, and this over a wide range of noise levels. The proposed data augmentation enables to achieve these results with less than 100 datasets.
Link: https://arxiv.org/abs/1810.04898
====================================================
Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time (Yinghao Huang - 10 October, 2018)
We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user's body. To evaluate our method, we recorded DIP-IMU, a dataset consisting of $10$ subjects wearing 17 IMUs for validation in $64$ sequences with $330\,000$ time instants; this constitutes the largest IMU dataset publicly available
Link: https://arxiv.org/abs/1810.04703
====================================================
Multimodal Speech Emotion Recognition Using Audio and Text (Seunghyun Yoon - 10 October, 2018)
As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.
Link: https://arxiv.org/abs/1810.04635
====================================================
Secure Deep Learning Engineering: A Software Quality Assurance Perspective (Lei Ma - 10 October, 2018)
Deep neural networks are the key driving force behind its recent success, but still seem to be a magic black box lacking interpretability and understanding. In this paper, we perform a large-scale study and construct a paper repository of 223 relevant works to the quality assurance, security, and interpretation of deep learning
Link: https://arxiv.org/abs/1810.04538
====================================================
LIRS: Enabling efficient machine learning on NVM-based storage via a lightweight implementation of random shuffling (Zhi-Lin Ke - 10 October, 2018)
Machine learning algorithms, such as Support Vector Machine (SVM) and Deep Neural Network (DNN), have gained a lot of interests recently. Experimental results show that LIRS can reduce the total training time of SVM and DNN by 49.9% and 43.5% on average, and improve the final testing accuracy on DNN by 1.01%.
Link: https://arxiv.org/abs/1810.04509
====================================================
Persistence pays off: Paying Attention to What the LSTM Gating Mechanism Persists (Giancarlo D. Salton - 10 October, 2018)
Recurrent Neural Network LMs composed of LSTM units, especially those augmented with an external memory, have achieved state-of-the-art results
Link: https://arxiv.org/abs/1810.04437
====================================================
Inter-Scanner Harmonization of High Angular Resolution DW-MRI using Null Space Deep Learning (Vishwesh Nath - 9 October, 2018)
NSDN significantly improved absolute performance relative to histology by 3.87% over CSD and 1.42% over a recently proposed deep neural network approach. More-over, it improved reproducibility on the paired data by 21.19% over CSD and 10.09% over a recently proposed deep approach. Finally, NSDN improved gen-eralizability of the model to a third in vivo human scanner (which was not used in training) by 16.08% over CSD and 10.41% over a recently proposed deep learn-ing approach
Link: https://arxiv.org/abs/1810.04260
====================================================
Deep Neural Network Compression for Aircraft Collision Avoidance Systems (Kyle D. Julian - 9 October, 2018)
To improve storage efficiency, a deep neural network is used to approximate the table. Because only the network parameters need to be stored, the required storage space is reduced by a factor of 1000, enabling the collision avoidance system to operate using current avionics systems.
Link: https://arxiv.org/abs/1810.04240
====================================================
Image Captioning as Neural Machine Translation Task in SOCKEYE (Loris Bazzani - 10 October, 2018)
In this paper, we explore different decoders and attentional models popular in neural machine translation, namely attentional recurrent neural networks, self-attentional transformers, and fully-convolutional networks, which represent the current state of the art of neural machine translation
Link: https://arxiv.org/abs/1810.04101
====================================================
Deep learning with differential Gaussian process flows (Pashupati Hegde - 9 October, 2018)
We demonstrate state-of-the-art results that exceed the performance of deep Gaussian processes and neural networks
Link: https://arxiv.org/abs/1810.04066
====================================================
Hartley Spectral Pooling for Deep Learning (Hao Zhang - 7 October, 2018)
In most convolution neural networks (CNNs), downsampling hidden layers is adopted for increasing computation efficiency and the receptive field size. [1], we present the Hartley transform based spectral pooling method in CNNs
Link: https://arxiv.org/abs/1810.04028
====================================================
Comparison of U-net-based Convolutional Neural Networks for Liver Segmentation in CT (Hans Meine - 9 October, 2018)
Using a set of 219 liver CT datasets with reference segmentations from liver surgery planning, we evaluate the performance of several neural network classifiers based on 2D and 3D U-net architectures. An interesting observation is that slice-wise approaches perform surprisingly well, with mean and median Dice coefficients above 0.97, and may be preferable over 3D approaches given current hardware and software limitations.
Link: https://arxiv.org/abs/1810.04017
====================================================
Glioma Segmentation with Cascaded Unet (Dmitry Lachinov - 9 October, 2018)
Similar to recent methods for object detection, our implementation is based on neural networks; we propose modifications to the 3D UNet architecture and augmentation strategy to efficiently handle multimodal MRI input, besides this we introduce approach to enhance segmentation quality with context obtained from models of the same topology operating on downscaled data. We evaluate presented approach on BraTS 2018 dataset and discuss results.
Link: https://arxiv.org/abs/1810.04008
====================================================
Computationally Efficient Cascaded Training for Deep Unrolled Network in CT Imaging (Dufan Wu - 5 October, 2018)
Unrolled neural networks have reached state-of-the-art performance by learning the image reconstruction algorithm end-to-end
Link: https://arxiv.org/abs/1810.03999
====================================================
SAM-GCNN: A Gated Convolutional Neural Network with Segment-Level Attention Mechanism for Home Activity Monitoring (Yu-Han Shen - 3 October, 2018)
The proposed framework is a convolutional model with two auxiliary modules: a gated convolutional neural network and a segment-level attention mechanism. We evaluated our work on the development dataset of DCASE 2018 Task 5 and achieved competitive performance, with a macro-averaged F-1 score increasing from 83.76% to 89.33%, compared with the convolutional baseline system.
Link: https://arxiv.org/abs/1810.03986
====================================================
Extended Bit-Plane Compression for Convolutional Neural Network Accelerators (Lukas Cavigelli - 1 October, 2018)
We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4x relative to uncompressed data and a gain of 60% over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.
Link: https://arxiv.org/abs/1810.03979
====================================================
DeepImageSpam: Deep Learning based Image Spam Detection (Amara Dinesh Kumar - 3 October, 2018)
This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classification achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques
Link: https://arxiv.org/abs/1810.03977
====================================================
Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation (Parnia Bahar - 9 October, 2018)
This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modeling. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German$\leftrightarrow$English.
Link: https://arxiv.org/abs/1810.03975
====================================================
A Generative Adversarial Model for Right Ventricle Segmentation (NicolÃ³ Savioli - 27 September, 2018)
This work proposes a solution based on Fully Convolutional Neural Networks (FCNN), where our first contribution is the optimal combination of three concepts (the convolution Gated Recurrent Units (GRU), the Generative Adversarial Networks (GAN), and the L1 loss function) that achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff Distance respectively with respect to the baseline FCNN. The study is conducted in a large in-house dataset of $\sim$ 23.000 segmented MRI slices, and its generality is verified in a publicly available dataset.
Link: https://arxiv.org/abs/1810.03969
====================================================
Vision-based Navigation of Autonomous Vehicle in Roadway Environments with Unexpected Hazards (Mhafuzul Islam - 26 September, 2018)
Vision-based navigation of modern autonomous vehicles primarily depends on Deep Neural Network (DNN) based systems in which the controller obtains input from sensors/detectors such as cameras, and produces an output such as a steering wheel angle to navigate the vehicle safely in roadway traffic. This study finds the DNN-based model with hazardous object detection, and semantic segmentation improves the ability of an autonomous vehicle to avoid potential crashes by 21% compared to the traditional DNN-based autonomous driving system.
Link: https://arxiv.org/abs/1810.03967
====================================================
Rate-Accuracy Trade-Off In Video Classification With Deep Convolutional Neural Networks (Mohammad Jubran - 27 September, 2018)
Advanced video classification systems decode video frames to derive the necessary texture and motion representations for ingestion and analysis by spatio-temporal deep convolutional neural networks (CNNs). Based on three CNN architectures and two action recognition datasets, we achieve 11%-94% saving in bitrate with marginal effect on classification accuracy. A model-based selection between multiple CNNs increases these savings further, to the point where, if up to 7% loss of accuracy can be tolerated, video classification can take place with as little as 3 kbps for the transport of the required compressed video information to the system implementing the CNN models.
Link: https://arxiv.org/abs/1810.03964
====================================================
Reconstructing Faces from fMRI Patterns using Deep Generative Neural Networks (Rufin VanRullen - 9 October, 2018)
We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised training procedure over a large dataset of celebrity faces. We then presented several thousand face images to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Qualitative and quantitative evaluation of the reconstructions revealed robust pairwise decoding (>95% correct), and a strong improvement relative to a baseline model (PCA decomposition)
Link: https://arxiv.org/abs/1810.03856
====================================================
Deep residual networks for automatic sleep stage classification of raw polysomnographic waveforms (Alexander Neergaard Olesen - 8 October, 2018)
We have developed an automatic sleep stage classification algorithm based on deep residual neural networks and raw polysomnogram signals. Briefly, the raw data is passed through 50 convolutional layers before subsequent classification into one of five sleep stages. Three model configurations were trained on 1850 polysomnogram recordings and subsequently tested on 230 independent recordings. Our best performing model yielded an accuracy of 84.1% and a Cohen's kappa of 0.746, improving on previous reported results by other groups also using only raw polysomnogram data. Most errors were made on non-REM stage 1 and 3 decisions, errors likely resulting from the definition of these stages
Link: https://arxiv.org/abs/1810.03745
====================================================
Bootstrapped CNNs for Building Segmentation on RGB-D Aerial Imagery (Clint Sebastian - 8 October, 2018)
Convolutional Neural Networks (CNNs) are robust against some of these variations, although they fail to distinguish easy and difficult examples. Second, the proposed method outperforms the non-bootstrapped version by utilizing only one-sixth of the original training data and it obtains a precision-recall break-even of 95.10% on our aerial imagery dataset.
Link: https://arxiv.org/abs/1810.03570
====================================================
NSGA-NET: A Multi-Objective Genetic Algorithm for Neural Architecture Search (Zhichao Lu - 8 October, 2018)
Experimental results suggest that combining the objectives of minimizing both an error metric and computational complexity, as measured by FLOPS, allows NSGA-Net to find competitive neural architectures near the Pareto front of both objectives on two different tasks, object classification and object alignment. NSGA-Net obtains networks that achieve 3.72% (at 4.5 million FLOP) error on CIFAR-10 classification and 8.64% (at 26.6 million FLOP) error on the CMU-Car alignment task
Link: https://arxiv.org/abs/1810.03522
====================================================
Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling (Jaejin Cho - 4 October, 2018)
The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages
Link: https://arxiv.org/abs/1810.03459
====================================================
Phrase-Based Attentions (Phi Xuan Nguyen - 30 September, 2018)
Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.
Link: https://arxiv.org/abs/1810.03444
====================================================
Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values (Julius Adebayo - 8 October, 2018)
Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et al.
Link: https://arxiv.org/abs/1810.03307
====================================================
Triple Attention Mixed Link Network for Single Image Super Resolution (Xi Cheng - 7 October, 2018)
Recent approaches with deep convolutional neural networks have achieved im-pressive performance. In this work, to significantly enhance the feature representation, we proposed Triple Attention mixed link Network (TAN) which consists of 1) three different aspects (i.e., kernel, spatial and channel) of attention mechanisms and 2) fu-sion of both powerful residual and dense connections (i.e., mixed link)
Link: https://arxiv.org/abs/1810.03254
====================================================
A look at the topology of convolutional neural networks (Rickard BrÃ¼el Gabrielsson - 7 October, 2018)
Convolutional neural networks (CNN's) are powerful and widely used tools. We show that the weights of convolutional layers at depths from 1 through 13 learn simple global structures
Link: https://arxiv.org/abs/1810.03234
====================================================
Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling (Zhiqing Sun - 7 October, 2018)
As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.
Link: https://arxiv.org/abs/1810.03167
====================================================
DeepGeo: Photo Localization with Deep Neural Network (Sudharshan Suresh - 6 October, 2018)
A deep neural network based on the ResNet architecture is trained, and four different strategies of incorporating low-level cardinality information are presented. This model achieves an accuracy 20 times better than chance on a test dataset, which rises to 71.87% when taking the best of top-5 guesses. The network also beats human subjects in 4 out of 5 rounds of GeoGuessr.
Link: https://arxiv.org/abs/1810.03077
====================================================
Text-based Sentiment Analysis and Music Emotion Recognition (Erion Ãano - 6 October, 2018)
This thesis addresses the above problems to provide methodological and practical insights for utilizing neural networks on sentiment analysis of texts and achieving state of the art results
Link: https://arxiv.org/abs/1810.03031
====================================================
Understanding Recurrent Neural Architectures by Analyzing and Synthesizing Long Distance Dependencies in Benchmark Sequential Datasets (Abhijit Mahalunkar - 6 October, 2018)
At present, the state-of-the-art computational models across a range of sequential data processing tasks, including language modeling, are based on recurrent neural network architectures. Finally, we demonstrate how understanding the characteristics of the LDDs in a dataset can inform better hyper-parameter selection for current state-of-the-art recurrent neural architectures and also aid in understanding them...
Link: https://arxiv.org/abs/1810.02966
====================================================
RCCNet: An Efficient Convolutional Neural Network for Histological Routine Colon Cancer Nuclei Classification (Shabbeer Basha S H - 30 September, 2018)
This paper proposes an efficient Convolutional Neural Network (CNN) based architecture for classification of histological routine colon cancer nuclei named as RCCNet. The proposed method has achieved a classification accuracy of 80.61% and 0.7887 weighted average F1 score
Link: https://arxiv.org/abs/1810.02797
====================================================
Learning Depth with Convolutional Spatial Propagation Network (Xinjing Cheng - 4 October, 2018)
Specifically, it is an efficient linear propagation model, in which the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). In practice, we further extend CSPN in two aspects: 1) take sparse depth map as additional input, which is useful for the task of depth completion; 2) similar to commonly used 3D convolution operation in CNNs, we propose 3D CSPN to handle features with one additional dimension, which is effective in the task of stereo matching using 3D cost volume. We experimented the proposed CPSN conjunct algorithms over the popular NYU v2 and KITTI datasets, where we show that our proposed algorithms not only produce high quality (e.g., 30% more reduction in depth error), but also run faster (e.g., 2 to 5x faster) than previous SOTA spatial propagation network. We also evaluated our stereo matching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on both the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the effectiveness of the proposed module
Link: https://arxiv.org/abs/1810.02695
====================================================
Integrating Weakly Supervised Word Sense Disambiguation into Neural Machine Translation (Xiao Pu - 5 October, 2018)
This paper demonstrates that word sense disambiguation (WSD) can improve neural machine translation (NMT) by widening the source context considered when modeling the senses of potentially ambiguous words. The improvements are above one BLEU point over strong NMT baselines, +4% accuracy over all ambiguous nouns and verbs, or +20% when scored manually over several challenging words.
Link: https://arxiv.org/abs/1810.02614
====================================================
Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer (Ashnil Kumar - 4 October, 2018)
Our aim is to improve fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. We compared our method to baseline techniques for multi-modality image analysis (pre-fused inputs, multi-branch techniques, multi-channel techniques) and demonstrated that our approach had a significantly higher accuracy ($p < 0.05$) than the baselines.
Link: https://arxiv.org/abs/1810.02492
====================================================
Deep Learning Approaches for Understanding Simple Speech Commands (Roman A. Solovyev - 4 October, 2018)
Our experiments show that we found appropriate sound representation and corresponding convolutional neural networks. As a result we achieved good classification accuracy that allowed us to finish the challenge on 8-th place among 1315 teams.
Link: https://arxiv.org/abs/1810.02364
====================================================
Recurrent Transition Networks for Character Locomotion (FÃ©lix G. Harvey - 11 October, 2018)
In this paper, we present a novel approach, based on deep recurrent neural networks, to automatically generate such transitions given a \textit{past context} of a few frames and a target character state to reach. We further explore applications of this model in a animation super-resolution setting where we temporally decompress animations saved at 1 frame per second and show that the network is able to reconstruct motions that are hard to distinguish from un-compressed locomotion sequences.
Link: https://arxiv.org/abs/1810.02363
====================================================
Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding (Kexin Yi - 4 October, 2018)
Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset
Link: https://arxiv.org/abs/1810.02338
====================================================
A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks (Sanjeev Arora - 4 October, 2018)
We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as $x\mapsto W_N \cdots W_1x$) by minimizing the $\ell_2$ loss over whitened data. Moreover, in the important case of output dimension 1, i.e
Link: https://arxiv.org/abs/1810.02281
====================================================
Direct Prediction of Cardiovascular Mortality from Low-dose Chest CT using Deep Learning (Sanne G. M. van Velzen - 4 October, 2018)
To limit the analysis to the heart region, the heart was first localized by our previously developed algorithm for organ localization exploiting convolutional neural networks. The performance of the method was assessed in eight cross-validation experiments with 1,433 images used for training, 50 for validation and 100 for testing. The method achieved a performance with an area under the ROC curve of 0.72
Link: https://arxiv.org/abs/1810.02277
====================================================
Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks (Christopher Morris - 4 October, 2018)
In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. The following work investigates GNNs from a theoretical point of view and relates them to the $1$-dimensional Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic (sub-)graphs
Link: https://arxiv.org/abs/1810.02244
====================================================
Zooming Network (Yukun Yan - 4 October, 2018)
Generally, ZN consists of an encoding neural net that can build a hierarchical representation of a document, and an interpreting neural model that can read the information at multi-levels and issuing labeling actions through a policy-net. We applied the proposed model to long text sequence labeling tasks, with performance exceeding baseline model (biLSTM-crf) by 10 F1-measure.
Link: https://arxiv.org/abs/1810.02114
====================================================
Improving the Segmentation of Anatomical Structures in Chest Radiographs using U-Net with an ImageNet Pre-trained Encoder (Maayan Frid-Adar - 4 October, 2018)
In addition, we explore the influence of using different loss functions in the training process of a neural network for semantic segmentation. We evaluate all models on a common benchmark of 247 X-ray images from the JSRT database and ground-truth segmentation masks from the SCR dataset. This model outperformed the current state-of-the-art methods tested on the same benchmark, with Jaccard overlap scores of 96.1% for lung fields, 90.6% for heart and 85.5% for clavicles.
Link: https://arxiv.org/abs/1810.02113
====================================================
Towards Fast and Energy-Efficient Binarized Neural Network Inference on FPGA (Cheng Fu - 4 October, 2018)
Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN by using a single bit (-1/+1) for network parameters and intermediate representations, which has greatly reduced the off-chip data transfer and storage overhead. By analyzing local properties of images and the learned BNN kernel weights, we observe an average of $\sim$78% input similarity and $\sim$59% weight similarity among weight kernels, measured by our proposed metric in common network architectures
Link: https://arxiv.org/abs/1810.02068
====================================================
Exascale Deep Learning for Climate Analytics (Thorsten Kurth - 3 October, 2018)
We extract pixel-level masks of extreme weather patterns using variants of Tiramisu and DeepLabv3+ neural networks. The Tiramisu network scales to 5300 P100 GPUs with a sustained throughput of 21.0 PF/s and parallel efficiency of 79.0%. DeepLabv3+ scales up to 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel efficiency of 90.7% in single precision. By taking advantage of the FP16 Tensor Cores, a half-precision version of the DeepLabv3+ network achieves a peak and sustained throughput of 1.13 EF/s and 999.0 PF/s respectively.
Link: https://arxiv.org/abs/1810.01993
====================================================
CRED: A Deep Residual Network of Convolutional and Recurrent Units for Earthquake Signal Detection (S. Mostafa Mousavi - 3 October, 2018)
Here, we introduce the Cnn-Rnn Earthquake Detector (CRED), a detector based on deep neural networks. We train the network using 500,000 seismograms (250k associated with tectonic earthquakes and 250k identified as noise) recorded in Northern California and tested it with an F-score of 99.95. Our model is able to detect more than 700 microearthquakes as small as -1.3 ML induced during hydraulic fracturing far away than the training region
Link: https://arxiv.org/abs/1810.01965
====================================================
Analysis of Diffractive Optical Neural Networks and Their Integration with Electronic Neural Networks (Deniz Mengu - 10 October, 2018)
Recently, an optical machine learning method based on diffractive deep neural networks (D2NNs) has been introduced to execute a function as the input light diffracts through passive layers, designed by deep learning using a computer. Using five phase-only diffractive layers, we numerically achieved a classification accuracy of 97.18% and 87.67% for optical recognition of handwritten digits and fashion products, respectively; using both phase and amplitude modulation (complex-valued) at each layer, our inference performance improved to 97.81% and 88.76%, respectively. Using a 5-layer phase-only D2NN jointly-optimized with a single fully-connected electronic layer, we achieved a classification accuracy of 98.17% and 89.90% for the recognition of handwritten digits and fashion products, respectively. Moreover, the input to the electronic network was compressed by >7.8 times down to 10x10 pixels, making the jointly-optimized hybrid system perform classification with a simple electronic layer
Link: https://arxiv.org/abs/1810.01916
====================================================
Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units (Yixi Xu - 3 October, 2018)
We further analyze the approximation properties of $L_{p,q}$ weight normalized deep neural networks. In particular, for an $L_{1,\infty}$ weight normalized network, the approximation error can be controlled by the $L_1$ norm of the output layer, and the corresponding generalization error only depends on the architecture by the square root of the depth.
Link: https://arxiv.org/abs/1810.01877
====================================================
Relaxed Quantization for Discretized Neural Networks (Christos Louizos - 3 October, 2018)
Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.
Link: https://arxiv.org/abs/1810.01875
====================================================
Neural Segmental Hypergraphs for Overlapping Mention Recognition (Bailin Wang - 3 October, 2018)
Coupled with neural networks for feature learning, our model achieves the state-of-the-art performance in three benchmark datasets annotated with overlapping mentions.
Link: https://arxiv.org/abs/1810.01817
====================================================
A Robot Localization Framework Using CNNs for Object Detection and Pose Estimation (Lukas Hoyer - 3 October, 2018)
Object detection is performed on an external camera image of the operation zone providing robot bounding boxes for an identification and orientation estimation convolutional neural network. The framework was evaluated with 3 different robot types and various identification patterns. We achieved up to 98% mAP@IOU0.5 and only 1.6Â° orientation error, running with a frame rate of 50 Hz on a GPU.
Link: https://arxiv.org/abs/1810.01665
====================================================
Deep Learning Based Caching for Self-Driving Car in Multi-access Edge Computing (Anselme Ndikumana - 2 October, 2018)
Second, in order to cache entertainment contents stylized for car passengers' features such as age and gender, Convolutional Neural Network (CNN) is used to predict age and gender of passengers. The simulation results show that the accuracy of our prediction for the contents need to be cached in the areas of the self-driving car is achieved at 98.04% and our approach can minimize delay.
Link: https://arxiv.org/abs/1810.01548
====================================================
Optimally Segmenting Inputs for NMT Shows Preference for Character-Level Processing (Julia Kreutzer - 2 October, 2018)
Most modern neural machine translation (NMT) systems rely on presegmented inputs. To overcome suboptimal segmentation choices, we present an algorithm for dynamic segmentation based on the Adaptative Computation Time algorithm (Graves 2016), that is trainable end-to-end and driven by the NMT objective
Link: https://arxiv.org/abs/1810.01480
====================================================
Efficient Dialog Policy Learning via Positive Memory Retention (Rui Zhao - 2 October, 2018)
This paper is concerned with the training of recurrent neural networks as goal-oriented dialog agents using reinforcement learning. We show that our method is 10 times more sample-efficient than policy gradients in extensive experiments on a new synthetic number guessing game
Link: https://arxiv.org/abs/1810.01371
====================================================
Landmine Detection Using Autoencoders on Multi-polarization GPR Volumetric Data (Paolo Bestagini - 2 October, 2018)
The proposed solution exploits a specific kind of convolutional neural network (CNN) known as autoencoder to analyze volumetric data acquired with ground penetrating radar (GPR) using different polarizations. Experiments conducted on real data show that the proposed technique requires little training and no ad-hoc data pre-processing to achieve accuracy higher than 93% on challenging datasets.
Link: https://arxiv.org/abs/1810.01316
====================================================
Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network (Xuanqing Liu - 1 October, 2018)
First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. On CIFAR-10 with VGG network, our model leads to 14\% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu 2017) under PGD attack with $0.035$ distortion, and the gap becomes even larger on a subset of ImageNet.
Link: https://arxiv.org/abs/1810.01279
====================================================
CAAD 2018: Generating Transferable Adversarial Examples (Yash Sharma - 29 September, 2018)
Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations carefully crafted to fool the targeted DNN, in both the non-targeted and targeted case. The NIPS 2017 competition was organized to accelerate research in adversarial attacks and defenses, taking place in the realistic setting where submitted adversarial attacks attempt to transfer to submitted defenses. The CAAD 2018 competition took place with nearly identical rules to the NIPS 2017 one. Given the requirement that the NIPS 2017 submissions were to be open-sourced, participants in the CAAD 2018 competition were able to directly build upon previous solutions, and thus improve the state-of-the-art in this setting. Our team participated in the CAAD 2018 competition, and won 1st place in both attack subtracks, non-targeted and targeted adversarial attacks, and 3rd place in defense
Link: https://arxiv.org/abs/1810.01268
====================================================
Continuous Learning of Context-dependent Processing in Neural Networks (Guanxiong Zeng - 5 October, 2018)
Deep artificial neural networks (DNNs) are powerful tools for recognition and classification as they learn sophisticated mapping rules between the inputs and the outputs. We demonstrate that with OWM to protect previously acquired knowledge, the networks could sequentially learn up to thousands of different mapping rules without interference, and needing as few as $\sim$10 samples to learn each, reaching a human level ability in online, continual learning
Link: https://arxiv.org/abs/1810.01256
====================================================
An Entropic Optimal Transport Loss for Learning Deep Neural Networks under Label Noise in Remote Sensing Images (Bharath Bhushan Damodaran - 2 October, 2018)
The state-of-the-art performances of deep neural networks are conditioned to the availability of large number of accurately labeled samples
Link: https://arxiv.org/abs/1810.01163
====================================================
NU-LiteNet: Mobile Landmark Recognition using Convolutional Neural Networks (Chakkrit Termritthikun - 2 October, 2018)
This paper describes the development of the architecture of a new convolutional neural network model, NU-LiteNet. The model size of NU-LiteNet is therefore 2.6 times smaller than that of SqueezeNet
Link: https://arxiv.org/abs/1810.01074
====================================================
Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network using Truncated Gaussian Approximation (Zhezhi He - 1 October, 2018)
As the countermeasure to this problem, deep neural networks with ternarized weights (i.e. -1, 0, +1) have been widely explored to greatly reduce the model size and computational cost, with limited accuracy degradation
Link: https://arxiv.org/abs/1810.01018
====================================================
The Profiling Machine: Active Generalization over Knowledge (Filip Ilievski - 1 October, 2018)
We describe two generic state-of-the-art neural architectures that can be easily instantiated as profiling machines to generate expectations and applied to any kind of knowledge to fill gaps
Link: https://arxiv.org/abs/1810.00782
====================================================
Geometric Constellation Shaping for Fiber Optic Communication Systems via End-to-end Learning (Rasmus T. Jones - 1 October, 2018)
By embedding a differentiable fiber channel model within two neural networks, the learning algorithm is optimizing for a geometric constellation shape. Improved performances are reported, up to 0.13 bit/4D in simulation and experimentally up to 0.12 bit/4D.
Link: https://arxiv.org/abs/1810.00774
====================================================
Benchmark Analysis of Representative Deep Neural Network Architectures (Simone Bianco - 1 October, 2018)
This work presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition
Link: https://arxiv.org/abs/1810.00736
====================================================
Text Similarity in Vector Space Models: A Comparative Study (Omid Shahmirzadi - 24 September, 2018)
We address the real-world problem of modeling patent-to-patent similarity and compare TFIDF (and related extensions), topic models (e.g., latent semantic indexing), and neural models (e.g., paragraph vectors). Contrary to expectations, the added computational cost of text embedding methods is justified only when: 1) the target text is condensed; and 2) the similarity comparison is trivial
Link: https://arxiv.org/abs/1810.00664
====================================================
One-Click Annotation with Guided Hierarchical Object Detection (Adithya Subramanian - 1 October, 2018)
The proposed work is semi-automatic in nature where the task of annotations is split between the human and a neural network. The experiment conducted on PASCAL VOC dataset revealed that annotation created from our approach achieves a mAP of 0.995 and a recall of 0.903. The Our Approach has shown an overall improvement by 8.5%, 18.6% in mean average precision and recall score for KITTI and 69.6%, 36% for CITYSCAPES dataset
Link: https://arxiv.org/abs/1810.00609
====================================================
Layer-compensated Pruning for Resource-constrained Convolutional Neural Networks (Ting-Wu Chin - 30 September, 2018)
Resource-efficient convolution neural networks enable not only the intelligence on edge devices but also opportunities in system-level optimization such as scheduling. Specifically, we reduce the accuracy gap between the pruned and original networks from 0.9% to 0.7% with 8x reduction in time needed for meta-learning, i.e., from 1 hour down to 7 minutes
Link: https://arxiv.org/abs/1810.00518
====================================================
Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Neural Networks (Kenneth T. Co - 30 September, 2018)
Our attack demonstrates the fragility of these neural networks to Perlin noise, a type of procedural noise used for generating realistic textures. Perlin noise attacks achieve at least 90% top 1 error across all classifiers. More worryingly, we show that most Perlin noise perturbations are "universal" in that they generalize, as adversarial examples, across large portions of the dataset, with up to 73% of images misclassified using a single perturbation
Link: https://arxiv.org/abs/1810.00470
====================================================
Pseudo-Random Number Generation using Generative Adversarial Networks (Marcello De Bernardi - 30 September, 2018)
We demonstrate that a GAN can effectively train even a small feed-forward fully connected neural network to produce pseudo-random number sequences with good statistical properties. At best, subjected to the NIST test suite, the trained generator passed around 99% of test instances and 98% of overall tests, outperforming a number of standard non-cryptographic PRNGs.
Link: https://arxiv.org/abs/1810.00378
====================================================
DELMU: A Deep Learning Approach to Maximising the Utility of Virtualised Millimetre-Wave Backhauls (Rui Li - 2 October, 2018)
We further regulate the inferences made by the neural network through a simple 'sanity check' routine, which guarantees both flow rate admissibility within the network's capacity region and minimum service levels. This confirms the applicability of DELMU to highly dynamic traffic regimes and we demonstrate up to 62% network utility gains over a baseline greedy approach.
Link: https://arxiv.org/abs/1810.00356
====================================================
Neural Entity Reasoner for Global Consistency in NER (Xiaoxiao Yin - 30 September, 2018)
NE-Reasoner inherits and develops some features from Neural Reasoner 1) a symbolic memory, allowing it to exchange entities between layers. 2) the specific interaction-pooling mechanism, allowing it to connect each local word to multiple global entities, and 3) the deep architecture, allowing it to bootstrap the recognized entity set from coarse to fine
Link: https://arxiv.org/abs/1810.00347
====================================================
Text Morphing (Shaohan Huang - 30 September, 2018)
Specifically, the editing vectors are generated with a recurrent neural networks model from the lexical gap between the source sentence and the target sentence. We conduct experiments with 10 million text morphing sequences which are extracted from the Yelp review dataset
Link: https://arxiv.org/abs/1810.00341
====================================================
Mini-batch Serialization: CNN Training with Inter-layer Data Reuse (Sangkug Lym - 29 September, 2018)
Training convolutional neural networks (CNNs) requires intense computations and high memory bandwidth. Combined, WaveCore and MBS reduce DRAM traffic by 73%, improve performance by 45%, and save 24% system energy for modern deep CNN training compared to conventional training mechanisms and accelerators.
Link: https://arxiv.org/abs/1810.00307
====================================================
Generalization and Regularization in DQN (Jesse Farebrother - 28 September, 2018)
These results are unexpected given the fact that, in supervised learning, deep neural networks often learn robust features that generalize across tasks. We perform this study using different game modes of Atari 2600 games, a recently introduced modification for the ALE which supports slight variations of the Atari 2600 games used for benchmarking in the field
Link: https://arxiv.org/abs/1810.00123
====================================================
Deep Residual Network for Off-Resonance Artifact Correction with Application to Pediatric Body Magnetic Resonance Angiography with 3D Cones (David Y Zeng - 28 September, 2018)
Methods: A residual convolutional neural network to correct off-resonance artifacts (Off-ResNet) was trained with a prospective study of 30 pediatric magnetic resonance angiography exams. Each exam acquired a short-readout scan (1.18 ms +- 0.38) and a long-readout scan (3.35 ms +- 0.74) at 3T. Results: Long-readout scans were on average 59.3% shorter than short-readout scans. Images from Off-ResNet had superior NRMSE, SSIM, and PSNR compared to uncorrected images across +-1kHz off-resonance (P<0.01). The proposed method had superior NRMSE over -677Hz to +1kHz and superior SSIM and PSNR over +-1kHz compared to autofocus (P<0.01). Radiologic scoring demonstrated that long-readout scans corrected with Off-ResNet were non-inferior to short-readout scans (P<0.01)
Link: https://arxiv.org/abs/1810.00072
====================================================
Learning Recurrent Binary/Ternary Weights (Arash Ardakani - 28 September, 2018)
Recurrent neural networks (RNNs) have shown excellent performance in processing sequence data. Ultimately, we show that LSTMs with binary/ternary weights can achieve up to 12x memory saving and 10x inference speedup compared to the full-precision implementation on an ASIC platform.
Link: https://arxiv.org/abs/1809.11086
====================================================
SeqSleepNet: End-to-End Hierarchical Recurrent Neural Network for Sequence-to-Sequence Automatic Sleep Staging (Huy Phan - 1 October, 2018)
For this purpose, we propose a hierarchical recurrent neural network named SeqSleepNet. We show that the proposed network outperforms state-of-the-art approaches, achieving an overall accuracy, macro F1-score, and Cohen's kappa of 87.1%, 83.3%, and 0.815 on a publicly available dataset with 200 subjects.
Link: https://arxiv.org/abs/1809.10932
====================================================
Adaptive Input Representations for Neural Language Modeling (Alexei Baevski - 30 September, 2018)
We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. We achieve a new state of the art on the WikiText-103 benchmark of 20.51 perplexity, improving the next best known result by 8.7 perplexity. On the Billion word benchmark, we achieve a state of the art of 24.14 perplexity.
Link: https://arxiv.org/abs/1809.10853
====================================================
Using Multi-task and Transfer Learning to Solve Working Memory Tasks (T. S. Jayram - 28 September, 2018)
It uses dual recurrent neural network controllers, inside the encoder and solver, respectively, that interface with a shared memory module and is completely differentiable. We show by extensive experimentation that the trained MAES models achieve task-size generalization, i.e., they are capable of handling sequential inputs 50 times longer than seen during training, with appropriately large memory modules
Link: https://arxiv.org/abs/1809.10847
====================================================
Patient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks (Ivan Girardi - 27 September, 2018)
We use an attention-based convolutional neural network architecture trained on 600,000 doctor notes in German. These approaches achieve 79% and 66% precision, respectively, but on a confidence threshold of 0.6, precision increases to 85% and 75%, respectively
Link: https://arxiv.org/abs/1809.10804
====================================================
Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS (Negin Hayatbini - 27 September, 2018)
The proposed method compares favorably with the existing cloud-patch-based segmentation technique implemented in the PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network - Cloud Classification System) rainfall retrieval algorithm. Evaluation of event-based images indicates that the proposed algorithm has potential to improve rain detection and estimation skills with an average of more than 45% gain comparing to the segmentation technique used in PERSIANN-CCS and identifying cloud regions as objects with accuracy rates up to 98%.
Link: https://arxiv.org/abs/1809.10801
====================================================
Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects (Jonathan Tremblay - 27 September, 2018)
Using synthetic data generated in this manner, we introduce a one-shot deep neural network that is able to perform competitively against a state-of-the-art network trained on a combination of real and synthetic data
Link: https://arxiv.org/abs/1809.10790
====================================================
Controllable Neural Story Generation via Reinforcement Learning (Pradyumna Tambwekar - 27 September, 2018)
Neural language models can be trained on large corpora across many domains and then used to generate stories. We show that our technique can train a model that generates a story that reaches the goal 94% of the time and reduces model perplexity
Link: https://arxiv.org/abs/1809.10736
====================================================
Learning a High-Precision Robotic Assembly Task Using Pose Estimation from Simulated Depth Images (Yuval Litvak - 27 September, 2018)
We present a high-accuracy three-stage pose estimation pipeline based on deep convolutional neural networks, which includes detection, pose estimation, refinement, and handling of near- and full symmetries of parts. We obtain an average pose estimation error of 2.14 millimeters and 1.09 degree leading to 88.6% success rate for robotic assembly of randomly distributed parts
Link: https://arxiv.org/abs/1809.10699
====================================================
Dropout Distillation for Efficiently Estimating Model Confidence (Corina Gurau - 27 September, 2018)
Our method is more efficient than Bayesian neural networks or model ensembles which, despite providing more reliable uncertainty scores, are more cumbersome to train and slower to test. We evaluate DDN on the the task of image classification on the CIFAR-10 dataset and show that our calibration results are competitive even when compared to 100 Monte Carlo samples from a dropout network while they also increase the classification accuracy
Link: https://arxiv.org/abs/1809.10562
====================================================
Consistency and Variation in Kernel Neural Ranking Model (Mary Arpita Pyreddy - 27 September, 2018)
This paper studies the consistency of the kernel-based neural ranking model K-NRM, a recent state-of-the-art neural IR model, which is important for reproducible research and deployment in the industry
Link: https://arxiv.org/abs/1809.10522
====================================================
Sample Efficient Adaptive Text-to-Speech (Yutian Chen - 27 September, 2018)
The experiments show that these approaches are successful at adapting the multi-speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers.
Link: https://arxiv.org/abs/1809.10460
====================================================
CNN Based Posture-Free Hand Detection (Richard Adiguna - 27 September, 2018)
Fortunately, the Convolution Neural Network (CNN) based approach provides a better way that is less sensitive to translation and hand poses. Our evaluation shows that the proposed shallow CNN network performs at 93.9% accuracy and reaches much faster speed than its competitors.
Link: https://arxiv.org/abs/1809.10432
====================================================
Adaptive Pruning of Neural Language Models for Mobile Devices (Raphael Tang - 26 September, 2018)
Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a "knob" to select different operating points. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.
Link: https://arxiv.org/abs/1809.10282
====================================================
Deeply Informed Neural Sampling for Robot Motion Planning (Ahmed H. Qureshi - 26 September, 2018)
DeepSMP's neural architecture comprises of a Contractive AutoEncoder which encodes given workspaces directly from a raw point cloud data, and a Dropout-based stochastic deep feedforward neural network which takes the workspace encoding, start and goal configuration, and iteratively generates feasible samples for SMPs to compute end-to-end collision-free optimal paths. The results show that on average our method is at least 7 times faster in point-mass and rigid-body case and about 28 times faster in 6-link robot case than the existing state-of-the-art.
Link: https://arxiv.org/abs/1809.10252
====================================================
ConvPath: A Software Tool for Lung Adenocarcinoma Digital Pathological Image Analysis Aided by Convolutional Neural Network (Shidan Wang - 20 September, 2018)
In this study, we developed an automated cell type classification pipeline, ConvPath, which includes nuclei segmentation, convolutional neural network-based tumor, stromal and lymphocytes classification, and extraction of tumor microenvironment related features for lung cancer pathology images. The overall classification accuracy is 92.9% and 90.1% in training and independent testing datasets, respectively
Link: https://arxiv.org/abs/1809.10240
====================================================
Left Ventricle Segmentation and Quantification from Cardiac Cine MR Images via Multi-task Learning (Shusil Dangi - 26 September, 2018)
Here we propose a convolutional neural network based multi-task learning approach to perform both tasks simultaneously, such that, the network learns better representation of the data with improved generalization performance. We performed a five fold cross-validation of the myocardium segmentation obtained from the proposed multi-task network on 97 patient 4-dimensional cardiac cine-MRI datasets available through the STACOM LV segmentation challenge against the provided gold-standard myocardium segmentation, obtaining a Dice overlap of $0.849 \pm 0.036$ and mean surface distance of $0.274 \pm 0.083$ mm, while simultaneously estimating the myocardial area with mean absolute difference error of $205\pm198$ mm$^2$.
Link: https://arxiv.org/abs/1809.10221
====================================================
Multi-Scale Fully Convolutional Network for Cardiac Left Ventricle Segmentation (Han Kang - 19 September, 2018)
For the problem of left ventricular segmentation, a new fully convolutional neural network structure named MS-FCN is proposed in this paper. Based on the Sunnybrook cine-MR dataset provided by the MICCAI 2009 challenge, numerical experiments demonstrate that our proposed model has obtained state-of-the-art segmentation results: the Dice score of our method reaches 0.93 on the endocardium, and 0.96 on the epicardium.
Link: https://arxiv.org/abs/1809.10203
====================================================
Computer-Aided Diagnosis of Label-Free 3-D Optical Coherence Microscopy Images of Human Cervical Tissue (Yutao Ma - 17 September, 2018)
OCM image features were extracted using a convolutional neural network (CNN) model, concatenated with patient information (e.g., age, HPV results), and classified using a support vector machine classifier. Results: An 88.3 plus or minus 4.9% classification accuracy was achieved for five fine-grained classes of cervical tissue, namely normal, ectropion, low-grade and high-grade squamous intraepithelial lesions (LSIL and HSIL), and cancer. high-risk [HSIL and cancer]), the CADx method achieved an area-under-the-curve (AUC) value of 0.959 with an 86.7 plus or minus 11.4% sensitivity and 93.5 plus or minus 3.8% specificity
Link: https://arxiv.org/abs/1809.10196
====================================================
Graph Convolution over Pruned Dependency Trees Improves Relation Extraction (Yuhao Zhang - 26 September, 2018)
The resulting model achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models
Link: https://arxiv.org/abs/1809.10185
====================================================
High Performance Zero-Memory Overhead Direct Convolutions (Jiyuan Zhang - 19 September, 2018)
The computation of convolution layers in deep neural networks typically rely on high performance routines that trade space for time by using additional memory (either for packing purposes or required as part of the algorithm) to improve performance. In this paper, we demonstrate that direct convolution, when implemented correctly, eliminates all memory overhead, and yields performance that is between 10% to 400% times better than existing high performance implementations of convolution layers on conventional and embedded CPU architectures
Link: https://arxiv.org/abs/1809.10170
====================================================
Multispecies fruit flower detection using a refined semantic segmentation network (Philipe A. Dias - 19 September, 2018)
Our method relies on an end-to-end residual convolutional neural network (CNN) that represents the state-of-the-art in semantic segmentation
Link: https://arxiv.org/abs/1809.10080
====================================================
Learning short-term past as predictor of human behavior in commercial buildings (Romana Markovic - 17 September, 2018)
For that purpose, a deep neural network is trained to predict the window states, where the input sequence duration is handled as an additional hyperparameter. The results pointed out, that the optimal predictive performance was achieved for the case where 60 time-steps of the indoor climate data were used as input. The analysis of the prediction accuracy in the form of F1 score for the different time-lag of future window states dropped from 0.51 to 0.27, when shifting the prediction target from 10 to 60 minutes in future.
Link: https://arxiv.org/abs/1809.10020
====================================================
Morphed Learning: Towards Privacy-Preserving for Deep Learning Based Applications (Juncheng Shen - 20 September, 2018)
Morphed Learning has these three features: (1) Strong protection against reverse-engineering on the morphed data; (2) Acceptable computational and data transmission overhead with no correlation to the depth of the neural network; (3) No degradation of the neural network performance. Theoretical analyses on CIFAR-10 dataset and VGG-16 network show that our method is capable of providing 10^89 morphing possibilities with only 5% computational overhead and 10% transmission overhead under limited knowledge attack scenario
Link: https://arxiv.org/abs/1809.09968
====================================================
Fast and Continuous Foothold Adaptation for Dynamic Locomotion through Convolutional Neural Networks (Octavio Villarreal - 25 September, 2018)
To efficiently adapt the landing position, we implement a self-supervised foothold classifier based on a Convolutional Neural Network (CNN). Our method results in an up to 200 times faster computation with respect to the full-blown heuristics. We assess the performance of our method on the dynamic quadruped robot HyQ, executing static and dynamic gaits (at speeds up to 0.5 m/s) in both simulated and real scenarios; the benefit of safe foothold adaptation is clearly demonstrated by the overall robot behavior.
Link: https://arxiv.org/abs/1809.09759
====================================================
An Exploration of Mimic Architectures for Residual Network Based Spectral Mapping (Peter Plantinga - 25 September, 2018)
Spectral mapping uses a deep neural network (DNN) to map directly from noisy speech to clean speech. Our goal is to derive a model that can be used as a preprocessor for any recognition system; the features derived from our model are passed through the standard Kaldi ASR pipeline and achieve a WER of 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using only feature adaptation.
Link: https://arxiv.org/abs/1809.09756
====================================================
PLU: The Piecewise Linear Unit Activation Function (Andrei Nicolae - 3 September, 2018)
For this reason the Rectified Linear Unit (ReLU) defined by max(0, x) has become the prevailing activation function in deep neural networks
Link: https://arxiv.org/abs/1809.09534
====================================================
Fine-Tuning VGG Neural Network For Fine-grained State Recognition of Food Images (Kaoutar Ben Ahmed - 8 September, 2018)
In this paper, evidence is provided for the power of convolutional neural network (CNN) for food state recognition, even with a small data set. A small-scale dataset consisting of 5978 images of seven categories was constructed and annotated manually
Link: https://arxiv.org/abs/1809.09529
====================================================
RapidHARe: A computationally inexpensive method for real-time human activity recognition from wearable sensors (Roman Chereshnev - 25 September, 2018)
In our comparative tests, we show that RapidHARe is an extremely fast predictor, one and a half times faster than artificial neural networks (ANNs) methods, and more than eight times faster than recurrent neural networks (RNNs) and hidden Markov models (HMMs). Moreover, in performance, RapidHare achieves an F1 score of 94.27\% and accuracy of 98.94\%, and when compared to ANN, RNN, HMM, it reduces the F1-score error rate by 45\%, 65\%, and 63\% and the accuracy error rate by 41\%, 55\%, and 62\%, respectively
Link: https://arxiv.org/abs/1809.09412
====================================================
Utilizing Class Information for DNN Representation Shaping (Daeyoung Choi - 24 September, 2018)
Statistical characteristics of DNN (Deep Neural Network) representations, such as sparsity and correlation, are known to be relevant to the performance and interpretability of deep learning. In terms of classification performance, significant improvements over the baseline and L1/L2 weight decay methods were found for 20 out of 21 tasks over popular benchmark datasets. In particular, cw-VR achieved the best performance for 12 tasks.
Link: https://arxiv.org/abs/1809.09307
====================================================
Deep Confidence: A Computationally Efficient Framework for Calculating Reliable Errors for Deep Neural Networks (Isidro Cortes-Ciriano - 24 September, 2018)
Using a set of 24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles perform on par with Random Forest (RF) and ensembles of independently trained deep neural networks
Link: https://arxiv.org/abs/1809.09060
====================================================
Neural Educational Recommendation Engine (NERE) (Moin Nadeem - 20 September, 2018)
We also develop a confidence estimator for our neural network, which is a crucial requirement for productionizing this model. We achieved an R^2 score of 0.81 in the content embedding space, and a recall score of 54% on our 100 nearest neighbors. This vastly exceeds the recall@100 score of 12% that a standard matrix-factorization approach provides
Link: https://arxiv.org/abs/1809.08922
====================================================
Close to Human Quality TTS with Transformer (Naihan Li - 19 September, 2018)
Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-the-art performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).
Link: https://arxiv.org/abs/1809.08895
====================================================
Information-Weighted Neural Cache Language Models for ASR (Lyan Verwimp - 24 September, 2018)
We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs
Link: https://arxiv.org/abs/1809.08826
====================================================
Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting (Katharina Kann - 23 September, 2018)
Neural state-of-the-art sequence-to-sequence (seq2seq) models often do not perform well for small training sets. On a 52-language benchmark dataset, we outperform the previous state of the art by up to 9.71% absolute accuracy.
Link: https://arxiv.org/abs/1809.08733
====================================================
Textually Enriched Neural Module Networks for Visual Question Answering (Khyathi Raghavi Chandu - 23 September, 2018)
There has been recent success in visual question answering using deep neural network models which use the linguistic structure of the questions to dynamically instantiate network layouts. We achieve 57.1% overall accuracy on the test-dev open-ended questions from the visual question answering (VQA 1.0) real image dataset.
Link: https://arxiv.org/abs/1809.08697
====================================================
Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization (Bao Wang - 22 September, 2018)
We improve the robustness of deep neural nets to adversarial attacks by using an interpolating function as the output activation. Together with the total variation minimization of adversarial images and augmented training, under the strongest attack, we achieve up to 20.6$\%$, 50.7$\%$, and 68.7$\%$ accuracy improvement w.r.t
Link: https://arxiv.org/abs/1809.08516
====================================================
Neural Approaches to Conversational AI (Jianfeng Gao - 21 September, 2018)
For each category, we present a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies.
Link: https://arxiv.org/abs/1809.08267
====================================================
Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets for IsoIntense Infant Brain MRI Segmentation (Seyed Raein Hashemi - 27 September, 2018)
The most recent fast and accurate image segmentation methods are built upon fully convolutional deep neural networks. Infant brain MRI tissue segmentation is a complex deep learning task, where the white matter and gray matter of the developing brain at about 6 months of age show similar T1 and T2 relaxation times, having similar intensity values on both T1 and T2-weighted MRIs. By taking advantage of these strategies we were able to perform fast image segmentation, using a network with less parameters than many state-of-the-art networks, being image size independent overcoming issues such as 3D vs 2D training and large vs small patch size selection, while achieving the top performance in segmenting brain tissue among all methods in the 2017 iSeg challenge
Link: https://arxiv.org/abs/1809.08168
====================================================
Efficient Formal Safety Analysis of Neural Networks (Shiqi Wang - 19 September, 2018)
In this paper, we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10$\times$ larger than the ones supported by existing analysis techniques
Link: https://arxiv.org/abs/1809.08098
====================================================
On-field player workload exposure and knee injury risk monitoring via deep learning (William R. Johnson - 21 September, 2018)
In this study, using the pre-trained CaffeNet convolutional neural network (CNN) model, multivariate regression of marker-based motion capture to 3D KJM for three sports-related movement types were compared. The strongest overall mean correlation to source modeling of 0.8895 was achieved over the initial 33 % of stance phase for sidestepping
Link: https://arxiv.org/abs/1809.08016
====================================================
SG-FCN: A Motion and Memory-Based Deep Learning Model for Video Saliency Detection (Meijun Sun - 21 September, 2018)
Data-driven saliency detection has attracted strong interest as a result of applying convolutional neural networks to the detection of eye fixations. Extensive experiments in comparison with 11 state-of-the-art methods are carried out, and the results show that our proposed model outperforms all 11 methods across a number of publicly available datasets.
Link: https://arxiv.org/abs/1809.07988
====================================================
Rapid Customization for Event Extraction (Yee Seng Chan - 20 September, 2018)
The system will then automatically generate mention-level event annotation automatically, and train a Neural Network model for finding the corresponding event. Experiments show that with less than 10 minutes of human effort per event type, the system achieves good performance for 67 novel event types
Link: https://arxiv.org/abs/1809.07783
====================================================
Multitask Learning on Graph Neural Networks - Learning Multiple Graph Centrality Measures with a Unified Network (Pedro H. C. Avelar - 3 October, 2018)
Graph neural networks (GNN), consisting of trained neural modules which can be arranged in different topologies at run time, are sound alternatives to tackle relational problems which lend themselves to graph representations. The proposed model achieves $89\%$ accuracy on a test dataset of random instances with up to 128 vertices and is shown to generalise to larger problem sizes
Link: https://arxiv.org/abs/1809.07695
====================================================
OxIOD: The Dataset for Deep Inertial Odometry (Changhao Chen - 20 September, 2018)
Recent studies have shown that the notorious issue of drift can be significantly alleviated by using deep neural networks (DNNs), e.g. Our dataset contains 158 sequences totalling more than 42 km in total distance, much larger than previous inertial datasets
Link: https://arxiv.org/abs/1809.07491
====================================================
MTLE: A Multitask Learning Encoder of Visual Feature Representations for Video and Movie Description (Oliver Nina - 19 September, 2018)
Many of the current state of the art methods for video captioning and movie description rely on simple encoding mechanisms through recurrent neural networks to encode temporal visual information extracted from video data. Our method demonstrates its robustness on the Large Scale Movie Description Challenge (LSMDC) 2017 where our method won the movie description task and its results were ranked among other competitors as the most helpful for the visually impaired.
Link: https://arxiv.org/abs/1809.07257
====================================================
Pose Estimation for Non-Cooperative Spacecraft Rendezvous Using Convolutional Neural Networks (Sumant Sharma - 19 September, 2018)
The primary contribution to the state-of-the-art of this work is the introduction of a novel pose determination method based on Convolutional Neural Networks (CNN) to provide an initial guess of the pose in real-time on-board
Link: https://arxiv.org/abs/1809.07238
====================================================
Deep Learning Based Rib Centerline Extraction and Labeling (Matthias Lenga - 19 September, 2018)
More specifically, we first apply a fully convolutional neural network (FCNN) to generate a probability map for detecting the first rib pair, the twelfth rib pair, and the collection of all intermediate ribs. We applied our method to CT volumes from 116 patients which included a variety of different challenges and achieved a centerline accuracy of 0.787 mm with respect to manual centerline annotations.
Link: https://arxiv.org/abs/1809.07082
====================================================
RPRG: Toward Real-time Robotic Perception, Reasoning and Grasping with One Multi-task Convolutional Neural Network (Hanbo Zhang - 19 September, 2018)
In this paper, we propose a multi-task convolutional neural network for Robotic Perception, Reasoning and Grasping (RPRG), which can help robot find the target, make the plan for grasping and finally grasp the target step by step in object stacking scenes. Experiments demonstrate that with our model, Baxter robot can autonomously grasp the target with a success rate of 94.2%, 77.1% and 62.5% in object cluttered scenes, familiar stacking scenes and complex stacking scenes respectively at a speed of 6.5 FPS for each detection.
Link: https://arxiv.org/abs/1809.07081
====================================================
NICT's Corpus Filtering Systems for the WMT18 Parallel Corpus Filtering Task (Rui Wang - 19 September, 2018)
This corpus is too noisy to build an acceptable neural machine translation (NMT) system. Finally, we sampled 100 million and 10 million words and built corresponding NMT systems
Link: https://arxiv.org/abs/1809.07043
====================================================
NICT's Neural and Statistical Machine Translation Systems for the WMT18 News Translation Task (Benjamin Marie - 19 September, 2018)
For each translation direction, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems
Link: https://arxiv.org/abs/1809.07037
====================================================
Generating 3D Adversarial Point Clouds (Chong Xiang - 19 September, 2018)
Machine learning models especially deep neural networks (DNNs) have been successfully applied to a variety of applications. In addition, we propose 7 perturbation measurement metrics tailored to different attacks and conduct extensive experiments to evaluate the proposed algorithms on the ModelNet40 dataset. Overall, our attack algorithms achieve about 100% attack success rate for all targeted attacks.
Link: https://arxiv.org/abs/1809.07016
====================================================
FastDeepIoT: Towards Understanding and Optimizing Neural Network Execution Time on Mobile and Embedded Devices (Shuochao Yao - 18 September, 2018)
First, FastDeepIoT automatically learns an accurate and highly interpretable execution time model for deep neural networks on the target device. We evaluate FastDeepIoT using three different sensing-related tasks on two mobile devices: Nexus 5 and Galaxy Nexus. FastDeepIoT further reduces the neural network execution time by $48\%$ to $78\%$ and energy consumption by $37\%$ to $69\%$ compared with the state-of-the-art compression algorithms.
Link: https://arxiv.org/abs/1809.06970
====================================================
Towards a Generic Diver-Following Algorithm: Balancing Robustness and Efficiency in Deep Visual Detection (Md Jahidul Islam - 18 September, 2018)
Subsequently, we design an architecturally simple Convolutional Neural Network (CNN)-based diver-detection model that is much faster than the state-of-the-art deep models yet provides comparable detection performances
Link: https://arxiv.org/abs/1809.06849
====================================================
3D segmentation of mandible from multisectional CT scans by convolutional neural networks (Bingjiang Qiu - 18 September, 2018)
The proposed convolutional neural network adopts the architecture of the U-Net and then combines the resulting 2D segmentations from three different planes into a 3D segmentation. We implement such a segmentation approach on 11 neck CT scans and then evaluate the performance. We achieve an average dice coefficient of $ 0.89 $ on two testing mandible segmentation
Link: https://arxiv.org/abs/1809.06752
====================================================
Document Informed Neural Autoregressive Topic Models with Distributional Prior (Pankaj Gupta - 15 September, 2018)
We present novel neural autoregressive topic model variants that consistently outperform state-of-the-art generative topic models in terms of generalization, interpretability (topic coherence) and applicability (retrieval and classification) over 6 long-text and 8 short-text datasets from diverse domains.
Link: https://arxiv.org/abs/1809.06709
====================================================
Capsule Deep Neural Network for Recognition of Historical Graffiti Handwriting (Nikita Gordienko - 11 September, 2018)
Automatic recognition of the historical letters (XI-XVIII centuries) carved on the stoned walls of St.Sophia cathedral in Kyiv (Ukraine) was demonstrated by means of capsule deep learning neural network. CGCL dataset contains >4000 images for glyphs of 34 letters which are hardly recognized by experts even in contrast to notMNIST dataset with the better images of 10 letters taken from different fonts. The area under curve (AUC) values for receiver operating characteristic (ROC) were also higher for the capsule network model than for CNN model: 0.88-0.93 (capsule network) and 0.50 (CNN) without data augmentation, 0.91-0.95 (capsule network) and 0.51 (CNN) with lossless data augmentation, and similar results of 0.91-0.93 (capsule network) and 0.9 (CNN) in the regime of lossless data augmentation only
Link: https://arxiv.org/abs/1809.06693
====================================================
SECS: Efficient Deep Stream Processing via Class Skew Dichotomy (Boyuan Feng - 7 September, 2018)
Extensive evaluations show that SECS can realize end-to-end classification speedups by a factor of 3x to 11x relative to state-of-the-art convolutional neural networks, at a higher accuracy.
Link: https://arxiv.org/abs/1809.06691
====================================================
Learning Universal Sentence Representations with Mean-Max Attention Autoencoder (Minghua Zhang - 18 September, 2018)
In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model
Link: https://arxiv.org/abs/1809.06590
====================================================
MBS: Macroblock Scaling for CNN Model Reduction (Yu-Hsun Lin - 18 September, 2018)
We estimate the proper channel (width) scaling of Convolution Neural Networks (CNNs) for model reduction. These applicable models range from compact CNN models such as MobileNet (25.53% reduction, ImageNet) and ShuffleNet (20.74% reduction, ImageNet) to ultra-deep ones such as ResNet-101 (51.67% reduction, ImageNet) and ResNet-1202 (72.71% reduction, CIFAR-10) with negligible accuracy degradation
Link: https://arxiv.org/abs/1809.06569
====================================================
User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks (Yilin Shen - 18 September, 2018)
We design a novel coarse-to-fine deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.
Link: https://arxiv.org/abs/1809.06559
====================================================
Triad-based Neural Network for Coreference Resolution (Yuanliang Meng - 17 September, 2018)
We propose a triad-based neural network system that generates affinity scores between entity mentions for coreference resolution. Our experiments show that a standard hierarchical clustering using the scores produces state-of-art results with gold mentions on the English portion of CoNLL 2012 Shared Task
Link: https://arxiv.org/abs/1809.06491
====================================================
Mask Editor : an Image Annotation Tool for Image Segmentation Tasks (Chuanhai Zhang - 17 September, 2018)
Deep convolutional neural network (DCNN) is the state-of-the-art method for image segmentation, which is one of key challenging computer vision tasks
Link: https://arxiv.org/abs/1809.06461
====================================================
Fast embedding of multilayer networks: An algorithm and application to group fMRI (James D. Wilson - 17 September, 2018)
Maximum likelihood estimators of the nodal features are identified through the use of the Skip-gram neural network model on the collection of sampled neighborhoods. We demonstrate the efficacy of multi-node2vec on a multilayer functional brain network from resting state fMRI scans over a group of 74 healthy individuals
Link: https://arxiv.org/abs/1809.06437
====================================================
Scattering Networks for Hybrid Representation Learning (Edouard Oyallon - 17 September, 2018)
Scattering networks are a class of designed Convolutional Neural Networks (CNNs) with fixed weights. Specifically, even applying a shallow cascade of small-windowed scattering coefficients followed by 1$\times$1-convolutions results in AlexNet accuracy on the ILSVRC2012 classification task. Moreover, by combining scattering networks with deep residual networks, we achieve a single-crop top-5 error of 11.4% on ILSVRC2012
Link: https://arxiv.org/abs/1809.06367
====================================================
Apple Flower Detection using Deep Convolutional Networks (Philipe A. Dias - 17 September, 2018)
With the goal of devising a technique for flower identification which is robust to clutter and to changes in illumination, this paper presents a method in which a pre-trained convolutional neural network is fine-tuned to become specially sensitive to flowers. Experimental results on a challenging dataset demonstrate that our method significantly outperforms three approaches that represent the state of the art in flower detection, with recall and precision rates higher than $90\%$
Link: https://arxiv.org/abs/1809.06357
====================================================
Dynamics Estimation Using Recurrent Neural Network (Astha Sharma - 17 September, 2018)
We are using recurrent neural networks for building the neural network model to train on sequences which represents 1307 trails of pouring. The loss obtained with this test data is 4.5920
Link: https://arxiv.org/abs/1809.06148
====================================================
Revisit Multinomial Logistic Regression in Deep Learning: Data Dependent Model Initialization for Image Recognition (Bowen Cheng - 17 September, 2018)
We study in this paper how to initialize the parameters of multinomial logistic regression (a fully connected layer followed with softmax and cross entropy loss), which is widely used in deep neural network (DNN) models for classification problems. For example, for image classification, our approach can reduce the training time by 10 times and achieve 3.2% gain in accuracy for Flickr-style classification. For object detection, our approach can also be 10 times faster in training for the same accuracy, or 5% better in terms of mAP for VOC 2007 with slightly longer training.
Link: https://arxiv.org/abs/1809.06131
====================================================
FermiNets: Learning generative machines to generate efficient neural networks via generative synthesis (Alexander Wong - 16 September, 2018)
Experimental results for image classification, semantic segmentation, and object detection tasks illustrate the efficacy of generative synthesis in producing generators that automatically generate highly efficient deep neural networks (which we nickname FermiNets) with higher model efficiency and lower computational costs (reaching >10x more efficient and fewer multiply-accumulate operations than several tested state-of-the-art networks), as well as higher energy efficiency (reaching >4x improvements in image inferences per joule consumed on a Nvidia Tegra X2 mobile processor)
Link: https://arxiv.org/abs/1809.05989
====================================================
Attacking Object Detectors via Imperceptible Patches on Background (Yuezun Li - 16 September, 2018)
Deep neural networks have been proven vulnerable against adversarial perturbations. We demonstrate the efficacy of our method on 5 different state-of-the-art object detectors on MS COCO 2014 dataset.
Link: https://arxiv.org/abs/1809.05966
====================================================
Memory Efficient Experience Replay for Streaming Learning (Tyler L. Hayes - 16 September, 2018)
Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting
Link: https://arxiv.org/abs/1809.05922
====================================================
Comparison of Deep Learning and the Classical Machine Learning Algorithm for the Malware Detection (Mohit Sewak - 16 September, 2018)
Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 & 7 layers architectures with the four different feature sets, and found that irrespective of the features inputs, the classical RF accuracy outperforms the DNN.
Link: https://arxiv.org/abs/1809.05889
====================================================
An investigation of a deep learning based malware detection system (Mohit Sewak - 16 September, 2018)
In the investigation, we experiment with different combination of Deep Learning architectures including Auto-Encoders, and Deep Neural Networks with varying layers over Malicia malware dataset on which earlier studies have obtained an accuracy of (98%) with an acceptable False Positive Rates (1.07%). In our proposed approach, besides improving the previous best results (99.21% accuracy and a False Positive Rate of 0.19%) indicates that Deep Learning based systems could deliver an effective defense against malware
Link: https://arxiv.org/abs/1809.05888
====================================================
An FPGA-Accelerated Design for Deep Learning Pedestrian Detection in Self-Driving Vehicles (Abdallah Moussawi - 16 September, 2018)
Currently, a special topology of deep neural networks called Fused Deep Neural Network (F-DNN) is considered to be the state of the art in pedestrian detection, as it has the lowest miss rate, yet it is very slow. The second contribution is a new FPGA design for accelerating the model on the Altera Arria 10 platform. Preliminary results of the improved SSD shows 3% higher miss-rate than F-DNN on Caltech pedestrian detection benchmark, but 4x performance improvement
Link: https://arxiv.org/abs/1809.05879
====================================================
Real-Time, Highly Accurate Robotic Grasp Detection using Fully Convolutional Neural Networks with High-Resolution Images (Dongwon Park - 16 September, 2018)
In this paper, we propose fully convolutional neural network (FCNN) based methods for robotic grasp detection. Our methods also achieved state-of-the-art detection accuracy (up to 96.6%) with state-of- the-art real-time computation time for high-resolution images (6-20ms per 360x360 image) on Cornell dataset. With accurate vision-robot coordinate calibration through our proposed learning-based, fully automatic approach, our proposed method yielded 90% success rate.
Link: https://arxiv.org/abs/1809.05828
====================================================
Development of deep learning algorithms to categorize free-text notes pertaining to diabetes: convolution neural networks achieve higher accuracy than support vector machines (Boyi Yang - 16 September, 2018)
The convolutional neural network (CNN) model with a separable convolution layer accurately identified diabetes-related notes in the Brigham and Womens Hospital testing set with the highest AUC of 0.975
Link: https://arxiv.org/abs/1809.05814
====================================================
Mobility Mode Detection Using WiFi Signals (Arash Kalatian - 15 September, 2018)
Deep neural network (Multilayer Perceptron) along with three decision tree based classifiers (Decision Tree, Bagged Decision Tree and Random Forest) are developed. Results show that the best prediction accuracy is achieved by Multilayer Perceptron, with 86.52% correct predictions of mobility modes.
Link: https://arxiv.org/abs/1809.05788
====================================================
Non-iterative recomputation of dense layers for performance improvement of DCNN (Yimin Yang - 14 September, 2018)
An iterative method of learning has become a paradigm for training deep convolutional neural networks (DCNN). The experimental results show that the proposed method obtains significant improvements over 30 state-of-the-art methods
Link: https://arxiv.org/abs/1809.05606
====================================================
Non-Matrix Tactile Sensors: How Can Be Exploited Their Local Connectivity For Predicting Grasp Stability? (Brayan S. Zapata-Impata - 14 September, 2018)
In addition, we prove that they can be used for predicting grasp stability by training a Convolutional Neural Network (CNN) with them. We captured over 2500 real three-fingered grasps on 41 everyday objects to train a CNN that exploited the local connectivity inherent on the non-matrix tactile sensors, achieving 94.2% F1-score on predicting stability.
Link: https://arxiv.org/abs/1809.05551
====================================================
Ground Truth for training OCR engines on historical documents in German Fraktur and Early Modern Latin (Uwe Springmann - 14 September, 2018)
The special form of GT as line image/transcription pairs makes it directly usable to train state-of-the-art recognition models for OCR software employing recurring neural networks in LSTM architecture such as Tesseract 4 or OCRopus. We also provide some pretrained OCRopus models for subcorpora of our dataset yielding between 95\% (early printings) and 98\% (19th century Fraktur printings) character accuracy rates on unseen test cases, a Perl script to harmonize GT produced by different transcription rules, and give hints on how to construct GT for OCR purposes which has requirements that may differ from linguistically motivated transcriptions.
Link: https://arxiv.org/abs/1809.05501
====================================================
Numeral Understanding in Financial Tweets for Fine-grained Crowd-based Forecasting (Chung-Chi Chen - 14 September, 2018)
Neural network-based models with word and character-level encoders are proposed for 7-way classification and 17-way classification. The numeral corpus used in our experiments, called FinNum 1.0 , is available for research purposes.
Link: https://arxiv.org/abs/1809.05356
====================================================
A Domain Agnostic Normalization Layer for Unsupervised Adversarial Domain Adaptation (Rob Romijnders - 14 September, 2018)
Normalization layers are known to improve convergence and generalization and are part of many state-of-the-art fully-convolutional neural networks
Link: https://arxiv.org/abs/1809.05298
====================================================
Learning to Fingerprint the Latent Structure in Question Articulation (Kumar Mrityunjay - 14 September, 2018)
Further, we show that the optimization formulation can be approximated to building a memory of patterns represented as a trained neural auto-encoder. Experimental evaluation using many clusters of questions, each related to an objective, shows 80% recognition accuracy and negligible false positive across these clusters of questions. We also demonstrate a refinement scheme called K-fingerprints, that achieves nearly 100% recognition with negligible false positive across the different clusters of questions.
Link: https://arxiv.org/abs/1809.05275
====================================================
Graph Pattern Mining and Learning through User-defined Relations (Extended Version) (Carlos H. C. Teixeira - 13 September, 2018)
We provide both theoretical guarantees and empirical evaluations of our estimators in application scenarios such as stochastic optimization of deep high-order graph neural network models and pattern (motif) counting. Finally,we show that R-GPM is scalable, providing near-linear speedups on 44 cores in all of our tests.
Link: https://arxiv.org/abs/1809.05241
====================================================
Enhanced Optic Disk and Cup Segmentation with Glaucoma Screening from Fundus Images using Position encoded CNNs (Vismay Agrawal - 13 September, 2018)
In this manuscript, we present a robust method for glaucoma screening from fundus images using an ensemble of convolutional neural networks (CNNs). On the REFUGE validation data (n=400), the segmentation network achieved a dice score of 0.88 and 0.64 for optic disc and optic cup respectively. For the tasking differentiating images affected with glaucoma from healthy images, the area under the ROC curve was observed to be 0.85.
Link: https://arxiv.org/abs/1809.05216
====================================================
Context2Name: A Deep Learning-Based Approach to Infer Natural Variable Names from Usage Contexts (Rohan Bavishi - 31 August, 2018)
The approach combines a lightweight, token-based static analysis with an auto-encoder neural network that summarizes usage contexts and a recurrent neural network that predict natural names for a given usage context. We evaluate Context2Name with a large corpus of real-world JavaScript code and show that it successfully predicts 47.5% of all minified identifiers while taking only 2.9 milliseconds on average to predict a name. Moreover, Context2Name complements the state-of-the-art by predicting 5.3% additional identifiers that are missed by both existing tools.
Link: https://arxiv.org/abs/1809.05193
====================================================
Interpreting search result rankings through intent modeling (Jaspreet Singh - 13 September, 2018)
Given the recent interest in arguably accurate yet non-interpretable neural models, even with textual features, for document ranking we try to answer questions relating to how to interpret rankings. In this paper we take first steps towards a framework for the interpretability of retrieval models with the aim of answering 3 main questions "What is the intent of the query according to the ranker?", "Why is a document ranked higher than another for the query?" and "Why is a document relevant to the query?"
Link: https://arxiv.org/abs/1809.05190
====================================================
Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks (Siyue Wang - 13 September, 2018)
Based on the observations of the effect of test dropout rate on test accuracy and attack success rate, we propose a defensive dropout algorithm to determine an optimal test dropout rate given the neural network model and the attacker's strategy for generating adversarial examples.We also investigate the mechanism behind the outstanding defense effects achieved by the proposed defensive dropout. For example, our defensive dropout can reduce the attack success rate from 100% to 13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset.
Link: https://arxiv.org/abs/1809.05165
====================================================
Full Workspace Generation of Serial-link Manipulators by Deep Learning based Jacobian Estimation (Peiyuan Liao - 13 September, 2018)
Apart from solving complicated problems that require a certain level of intelligence, fine-tuned deep neural networks can also create fast algorithms for slow, numerical tasks. In this paper, we introduce an improved version of [1]'s work, a fast, deep-learning framework capable of generating the full workspace of serial-link manipulators. We also introduce M3 (Manipulability Maps of Manipulators), a MATLAB robotics library based on [2](RTB), the datasets generated by which are used by this work. Implementations of the algorithm (based on Keras[3]), including benchmark evaluation script, are available at https://github.com/liaopeiyuan/Jacobian-Estimation 
Link: https://arxiv.org/abs/1809.05020
====================================================
Query-Efficient Black-Box Attack by Active Learning (Pengcheng Li - 13 September, 2018)
Deep neural network (DNN) as a popular machine learning model is found to be vulnerable to adversarial attack. Our extensive experimental results on MNIST and CIFAR-10 show that the proposed method can reduce more than $90\%$ of queries while preserve attacking success rates and obtain an accurate substitute model which is more than $85\%$ similar with the target oracle.
Link: https://arxiv.org/abs/1809.04913
====================================================
Image Captioning based on Deep Reinforcement Learning (Haichao Shi - 13 September, 2018)
To the best of our knowledge, most state-of-the-art methods follow a pattern of sequential model, such as recurrent neural networks (RNN)
Link: https://arxiv.org/abs/1809.04835
====================================================
Canonical and Compact Point Cloud Representation for Shape Classification (Kent Fujiwara - 13 September, 2018)
We then train a neural network for each instance to non-linearly embed its distance field into network parameters. We demonstrate the descriptiveness of the instance-wise, shape-embedded network parameters by using them to classify shapes in $3$D datasets
Link: https://arxiv.org/abs/1809.04820
====================================================
Learning to Summarize Radiology Findings (Yuhao Zhang - 8 October, 2018)
On a large dataset of radiology reports collected from actual hospital studies, our model outperforms existing non-neural and neural baselines under the ROUGE metrics. In a blind experiment, a board-certified radiologist indicated that 67% of sampled system summaries are at least as good as the corresponding human-written summaries, suggesting significant clinical validity
Link: https://arxiv.org/abs/1809.04698
====================================================
SAFE: A Neural Survival Analysis Model for Fraud Early Detection (Panpan Zheng - 12 September, 2018)
Experimental results on two real world datasets demonstrate that SAFE outperforms both the survival analysis model and recurrent neural network model alone as well as state-of-the-art fraud early detection approaches.
Link: https://arxiv.org/abs/1809.04683
====================================================
Solving Sinhala Language Arithmetic Problems using Neural Networks (W. M. T Chathurika - 11 September, 2018)
All functions are combined through the neural network which builds an equation to solve the problem. Mahoshadha2 learns to solve arithmetic problems with the accuracy of 76%.
Link: https://arxiv.org/abs/1809.04557
====================================================
End-to-end Audiovisual Speech Activity Detection with Bimodal Recurrent Neural Models (Fei Tao - 12 September, 2018)
This study explores this idea proposing a \emph{bimodal recurrent neural network} (BRNN) framework for SAD. The experimental evaluation considers a large audiovisual corpus with over 60.8 hours of recordings, collected from 105 speakers. The results demonstrate that the proposed framework leads to absolute improvements up to 1.2% under practical scenarios over a VAD baseline using only audio implemented with deep neural network (DNN). The proposed approach achieves 92.7% F1-score when it is evaluated using the sensors from a portable tablet under noisy acoustic environment, which is only 1.0% lower than the performance obtained under ideal conditions (e.g., clean speech obtained with a high definition camera and a close-talking microphone).
Link: https://arxiv.org/abs/1809.04553
====================================================
Using the Tsetlin Machine to Learn Human-Interpretable Rules for High-Accuracy Text Categorization with Medical Applications (Geir Thore Berge - 16 September, 2018)
Our empirical comparison with NaÃ¯ve Bayes, decision trees, linear support vector machines (SVMs), random forest, long short-term memory (LSTM) neural networks, and other techniques, is quite conclusive. The Tsetlin Machine either performs on par with or outperforms all of the evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a non-public clinical dataset. Finally, our GPU implementation of the Tsetlin Machine executes 5 to 15 times faster than the CPU implementation, depending on the dataset
Link: https://arxiv.org/abs/1809.04547
====================================================
Isolated and Ensemble Audio Preprocessing Methods for Detecting Adversarial Examples against Automatic Speech Recognition (Krishan Rajaratnam - 11 September, 2018)
An adversarial attack is an exploitative process in which minute alterations are made to natural inputs, causing the inputs to be misclassified by neural models. In 2017, a genetic attack was shown to be quite potent against the Speech Commands Model. One particular combined defense incorporating compressions, speech coding, filtering, and audio panning was shown to be quite effective against the attack on the Speech Commands Model, detecting audio adversarial examples with 93.5% precision and 91.2% recall.
Link: https://arxiv.org/abs/1809.04397
====================================================
Bayesian Semi-supervised Learning with Graph Gaussian Processes (Yin Cheng Ng - 9 October, 2018)
The proposed model shows extremely competitive performance when compared to the state-of-the-art graph neural networks on semi-supervised learning benchmark experiments, and outperforms the neural networks in active learning experiments where labels are scarce
Link: https://arxiv.org/abs/1809.04379
====================================================
Deep learning for time series classification: a review (Hassan Ismail Fawaz - 12 September, 2018)
DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.
Link: https://arxiv.org/abs/1809.04356
====================================================
Transforming acoustic characteristics to deceive playback spoofing countermeasures of speaker verification systems (Fuming Fang - 13 September, 2018)
Experimental results showed that use of this "enhanced stolen speech" method significantly increases the equal error rates for the baseline used in the ASVspoof 2017 challenge and for a light convolutional neural network-based method
Link: https://arxiv.org/abs/1809.04274
====================================================
Rapid Training of Very Large Ensembles of Diverse Neural Networks (Abdul Wasay - 12 September, 2018)
In particular, our approach trains an ensemble of $100$ variants of deep neural networks with diverse architectures up to $6 \times$ faster as compared to existing approaches
Link: https://arxiv.org/abs/1809.04270
====================================================
Ensemble of Convolutional Neural Networks for Automatic Grading of Diabetic Retinopathy and Macular Edema (Avinash Kori - 11 September, 2018)
In this manuscript, we automate the procedure of grading of diabetic retinopathy and macular edema from fundus images using an ensemble of convolutional neural networks. For the task of grading DR, on the test data (n=56), the ensemble achieved an accuracy of 83.9\%, while for the task for grading macular edema the network achieved an accuracy of 95.45% (n=44).
Link: https://arxiv.org/abs/1809.04228
====================================================
Attention based visual analysis for fast grasp planning with multi-fingered robotic hand (Zhen Deng - 11 September, 2018)
Our approach uses a computational visual attention model to locate regions of interest in a scene, and uses a deep convolutional neural network to detect grasp type and point for a sub-region of the object presented in a region of interest. A new Grasp Type Dataset (GTD) that considers 6 commonly used grasp types and covers 12 household objects is also presented.
Link: https://arxiv.org/abs/1809.04226
====================================================
Multimodal neural pronunciation modeling for spoken languages with logographic origin (Minh Nguyen - 11 September, 2018)
In this work, we propose a multimodal approach to predict the pronunciation of Cantonese logographic characters, using neural networks with a geometric representation of logographs and pronunciation of cognates in historically related languages. The proposed framework improves performance by 18.1% and 25.0% respective to unimodal and multimodal baselines.
Link: https://arxiv.org/abs/1809.04203
====================================================
Layerwise Perturbation-Based Adversarial Training for Hard Drive Health Degree Prediction (Jianguo Zhang - 28 September, 2018)
Firstly, we design a layerwise perturbation-based adversarial training method which can add perturbations to any layers of a neural network to improve the generalization of the network. The model trained by the proposed method can correctly predict the hard drive health status 5 and 15 days in advance
Link: https://arxiv.org/abs/1809.04188
====================================================
Searching for Efficient Multi-Scale Architectures for Dense Image Prediction (Liang-Chieh Chen - 11 September, 2018)
The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\% on Cityscapes (street scene parsing), 71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL VOC 2012 (semantic image segmentation)
Link: https://arxiv.org/abs/1809.04184
====================================================
Iterative Segmentation from Limited Training Data: Applications to Congenital Heart Disease (Danielle F. Pace - 11 September, 2018)
In contrast, we develop a segmentation model that recursively evolves a segmentation in several steps, and implement it as a recurrent neural network. We demonstrate the advantages of this approach on a dataset of 20 images from CHD patients, learning a model that accurately segments individual heart chambers and great vessels
Link: https://arxiv.org/abs/1809.04182
====================================================
On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation (Tamer Alkhouli - 11 September, 2018)
We also propose alignment pruning to speed up decoding in alignment-based neural machine translation (ANMT), which speeds up translation by a factor of $1.8$ without loss in translation performance. We carry out experiments on the shared WMT 2016 English$\to$Romanian news task and the BOLT Chinese$\to$English discussion forum task.
Link: https://arxiv.org/abs/1809.03985
====================================================
A Real-time Robotic Grasp Approach with Oriented Anchor Box (Hanbo Zhang - 18 September, 2018)
In this paper, we build a vision-based, robust and real-time robotic grasp approach with fully convolutional neural network. Five-fold cross validation results demonstrate that our proposed algorithm achieves an accuracy of 98.8% and 97.8% in image-wise split and object-wise split respectively, and the speed of our detection algorithm is 67 FPS with GTX 1080Ti, outperforming all the current state-of-the-art grasp detection algorithms on Cornell Dataset both in speed and accuracy. Robotic experiments demonstrate the robustness and generalization ability in unseen objects and real-world environment, with the average success rate of 90.0% and 84.2% of familiar things and unseen things respectively on Baxter robot platform.
Link: https://arxiv.org/abs/1809.03873
====================================================
Convolutional Neural Networks for the segmentation of microcalcification in Mammography Imaging (Gabriele Valvano - 11 September, 2018)
In this paper we propose a novel approach based on convolutional neural networks for the detection and segmentation of microcalcification clusters. In this work we used 283 mammograms to train and validate our model, obtaining an accuracy of 98.22% in the detection of preliminary suspect regions and of 97.47% in the segmentation task
Link: https://arxiv.org/abs/1809.03788
====================================================
Normalization in Training Deep Convolutional Neural Networks for 2D Bio-medical Semantic Segmentation (Xiao-Yun Zhou - 11 September, 2018)
Segmentation methods based on Deep Convolutional Neural Network (DCNN) out-perform conventional methods in terms of both the accuracy and automation. 37 RVs from both asymptomatic and Hypertrophic Cardiomyopathy (HCM) subjects and 20 aortas from asymptomatic subjects were used for the validation
Link: https://arxiv.org/abs/1809.03783
====================================================
Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3D Pedestrian Pose and Gait Prediction (Xiaoxiao Du - 11 September, 2018)
This paper proposes a biomechanically inspired recurrent neural network (Bio-LSTM) that can predict the location and 3D articulated body pose of pedestrians in a global coordinate frame, given 3D poses and locations estimated in prior frames with inaccuracy. The proposed network is able to predict poses and global locations for multiple pedestrians simultaneously, for pedestrians up to 45 meters from the cameras (urban intersection scale)
Link: https://arxiv.org/abs/1809.03705
====================================================
Temporal-Spatial Mapping for Action Recognition (Xiaolin Song - 10 September, 2018)
Based on the VideoMap representation, we further propose a temporal attention model within a shallow convolutional neural network to efficiently exploit the temporal-spatial dynamics. The experiment results show that the proposed scheme achieves the state-of-the-art performance, with 4.2% accuracy gain over Temporal Segment Network (TSN), a competing baseline method, on the challenging human action benchmark dataset HMDB51.
Link: https://arxiv.org/abs/1809.03669
====================================================
Multi-view Models for Political Ideology Detection of News Articles (Vivek Kulkarni - 10 September, 2018)
Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.
Link: https://arxiv.org/abs/1809.03485
====================================================
Jointly Learning to See, Ask, and GuessWhat (Aashish Venkatesh - 10 September, 2018)
Our model exploits a neural network architecture to build a continuous representation of the dialogue state that integrates information from the visual and linguistic modalities and conditions future action. We show that the introduction of our new architecture combined with these learning regimes yields an increase of 19.5% in task success accuracy with respect to a baseline model that treats submodules independently
Link: https://arxiv.org/abs/1809.03408
====================================================
Toward a Standardized and More Accurate Indonesian Part-of-Speech Tagging (Kemal Kurniawan - 20 September, 2018)
A new state-of-the-art of 97.47 F1 score is achieved with a recurrent neural network
Link: https://arxiv.org/abs/1809.03391
====================================================
Deriving Enhanced Geographical Representations via Similarity-based Spectral Analysis: Predicting Colorectal Cancer Survival Curves in Iowa (Michael T. Lash - 6 September, 2018)
Neural networks are capable of learning rich, nonlinear feature representations shown to be beneficial in many predictive tasks. In this work, we use such models to explore different geographical feature representations in the context of predicting colorectal cancer survival curves for patients in the state of Iowa, spanning the years 1989 to 2013. Furthermore, we find that similarity-based spectral analysis-elicited representations improve upon the original spectral analysis results by approximately 40%.
Link: https://arxiv.org/abs/1809.03323
====================================================
A Comparison of Handcrafted and Deep Neural Network Feature Extraction for Classifying Optical Coherence Tomography (OCT) Images (Kuntoro Adi Nugroho - 1 September, 2018)
The proposed study aims to compare the effectiveness of handcrafted and deep neural network features. The evaluated dataset consist of 32339 instances distributed in four classes, namely CNV, DME, DRUSEN, and NORMAL. As a result, the deep neural network based methods outperformed the handcrafted feature with 88% and 89% accuracy for DenseNet and ResNet compared to 50 % and 42 % for HOG and LBP respectively
Link: https://arxiv.org/abs/1809.03306
====================================================
Action-conditional Sequence Modeling for Recommendation (Elena Smirnova - 7 September, 2018)
In particular, Recurrent Neural Networks (RNNs) has been shown to achieve substantial improvements over collaborative filtering baselines. Indeed, it is reported that in online services interactions with recommendations represent up to 30\% of total interactions
Link: https://arxiv.org/abs/1809.03291
====================================================
Classification by Re-generation: Towards Classification Based on Variational Inference (Shideh Rezaeifar - 10 September, 2018)
As Deep Neural Networks (DNNs) are considered the state-of-the-art in many classification tasks, the question of their semantic generalizations has been raised
Link: https://arxiv.org/abs/1809.03259
====================================================
Towards JointUD: Part-of-speech Tagging and Lemmatization using Recurrent Neural Networks (Gor Arakelyan - 10 September, 2018)
We have extended an LSTM-based neural network designed for sequence tagging to additionally generate character-level sequences. Sentence segmentation, tokenization and dependency parsing were handled by UDPipe 1.2 baseline
Link: https://arxiv.org/abs/1809.03211
====================================================
Learning Sequence Encoders for Temporal Knowledge Graph Completion (Alberto GarcÃ­a-DurÃ¡n - 10 September, 2018)
To incorporate temporal information, we utilize recurrent neural networks to learn time-aware representations of relation types which can be used in conjunction with existing latent factorization methods. The data sets are available under a permissive BSD-3 license 1.
Link: https://arxiv.org/abs/1809.03202
====================================================
Towards one-shot learning for rare-word translation with external experts (Ngoc-Quan Pham - 10 September, 2018)
Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof-domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English to Spanish and German to English
Link: https://arxiv.org/abs/1809.03182
====================================================
Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation (Chenze Shao - 10 September, 2018)
Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.
Link: https://arxiv.org/abs/1809.03132
====================================================
Analysis of the generalization error: Empirical risk minimization over deep artificial neural networks overcomes the curse of dimensionality in the numerical approximation of Black-Scholes partial differential equations (Julius Berner - 9 September, 2018)
In particular we show that for Kolmogorov PDEs with affine drift and diffusion coefficients and a given accuracy $\varepsilon>0$, ERM over deep neural network hypothesis classes of size scaling polynomially in the dimension $d$ and $\varepsilon^{-1}$ and with a number of training samples scaling polynomially in the dimension $d$ and $\varepsilon^{-1}$ approximates the solution of the Kolmogorov PDE to within accuracy $\varepsilon$ with high probability
Link: https://arxiv.org/abs/1809.03062
====================================================
SHOMA at Parseme Shared Task on Automatic Identification of VMWEs: Neural Multiword Expression Tagging with High Generalisation (Shiva Taslimipoor - 9 September, 2018)
We employ a neural architecture comprising of convolutional and recurrent layers with the addition of an optional CRF layer at the top. It outperformed all participating systems in both open and closed tracks with the overall macro-average MWE-based F1 score of 58.09 averaged among all languages
Link: https://arxiv.org/abs/1809.03056
====================================================
A Neural Temporal Model for Human Motion Prediction (Anand Gopalakrishnan - 14 September, 2018)
We propose novel neural temporal models for short-term motion prediction and long-term human motion synthesis, achieving state-of-art predictive performance while being computationally less expensive compared to previously proposed approaches. Key aspects of our proposed system include: 1) a novel, two-level processing architecture that aids in generating planned trajectories, 2) a simple set of easily computable features that integrate simple derivative information into the model, and 3) a novel multi-objective loss function that helps the model to slowly progress from the simpler task of next-step prediction to the harder task of multi-step closed-loop prediction
Link: https://arxiv.org/abs/1809.03036
====================================================
A Supervised Learning Methodology for Real-Time Disguised Face Recognition in the Wild (Saumya Kumaar - 8 September, 2018)
The algorithm put forward in this paper detects 20 facial key-points in the first stage, using a 14-layered convolutional neural network (CNN). Our key-point feature prediction accuracy is 65% while the classification rate is 72.4%. Moreover, the architecture works at 19 FPS, thereby performing in almost real-time
Link: https://arxiv.org/abs/1809.02875
====================================================
Rate-Adaptive Neural Networks for Spatial Multiplexers (Suhas Lohit - 8 September, 2018)
In resource-constrained environments, one can employ spatial multiplexing cameras to acquire a small number of measurements of a scene, and perform effective reconstruction or high-level inference using purely data-driven neural networks. Using standard datasets, we demonstrate that, when tested over a range of MRs, a rate-adaptive network can provide high quality reconstruction over a the entire range, resulting in up to about 15 dB improvement over previous methods, where the network is valid for only one MR
Link: https://arxiv.org/abs/1809.02850
====================================================
Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU Platform (Chi-Chung Chen - 8 September, 2018)
The training process of Deep Neural Network (DNN) is compute-intensive, often taking days to weeks to train a DNN model. The experimental results show that our proposal achieves up to 15.77x speedup compared to data parallelism and up to 2.18x speedup compared to the state-of-the-art model parallelism method without incurring accuracy loss.
Link: https://arxiv.org/abs/1809.02839
====================================================
Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering (Todor Mihaylov - 8 September, 2018)
Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop
Link: https://arxiv.org/abs/1809.02789
====================================================
Molecular Hypergraph Grammar with its Application to Molecular Optimization (Hiroshi Kajino - 7 September, 2018)
While it achieves the state-of-the-art performance, it requires several neural networks to be trained, which predict which atoms are used to connect fragments and stereochemistry of each bond
Link: https://arxiv.org/abs/1809.02745
====================================================
Learning to Solve NP-Complete Problems - A Graph Neural Network for the Decision TSP (Marcelo O. R. Prates - 7 September, 2018)
Graph Neural Networks (GNN) are a promising technique for bridging differential programming and combinatorial domains. We were able to obtain $80\%$ accuracy training with $-2\%,+2\%$ deviations, and the same trained model can generalize for more relaxed deviations with increasing performance
Link: https://arxiv.org/abs/1809.02721
====================================================
Adaptive Edge Features Guided Graph Attention Networks (Liyu Gong - 7 September, 2018)
However, current state-of-the-art neural network models designed for graph learning do not consider incorporating edge features, especially multi-dimensional edge features
Link: https://arxiv.org/abs/1809.02709
====================================================
Optimizing CNN Model Inference on CPUs (Yizhi Liu - 7 September, 2018)
The popularity of Convolutional Neural Network (CNN) models and the ubiquity of CPUs imply that better performance of CNN model inference on CPUs can deliver significant gain to a large number of users. Experiments show that our solution achieves up to 2.81x better latency for CNN model inference on a 18-core Intel Platinum 8000-series CPU compared to the state-of-the-art implementations using Intel MKL-DNN.
Link: https://arxiv.org/abs/1809.02697
====================================================
Neural Generation of Diverse Questions using Answer Focus, Contextual and Linguistic Features (Vrindavan Harrison - 5 October, 2018)
In this work we present a new Attentional Encoder--Decoder Recurrent Neural Network model for automatic question generation. Our model achieves state of the art results of 19.98 Bleu_4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin
Link: https://arxiv.org/abs/1809.02637
====================================================
Logographic Subword Model for Neural Machine Translation (Yihao Fang - 7 September, 2018)
Our approach drastically reduces the size of an artificial neural network, while maintaining comparable BLEU scores as those attained with the baseline RNN and CNN seq2seq models. Experiments demonstrate that in the tasks of English-Chinese/Chinese-English translation, the reduction of those aspects can be from $11\%$ to as high as $77\%$
Link: https://arxiv.org/abs/1809.02592
====================================================
Self-Supervised Generation of Spatial Audio for 360 Video (Pedro Morgado - 7 September, 2018)
Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360 video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360 videos uploaded with spatial audio. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360 video and a mono audio track.
Link: https://arxiv.org/abs/1809.02587
====================================================
Skin lesion classification with ensemble of squeeze-and-excitation networks and semi-supervised learning (Shunsuke Kitada - 7 September, 2018)
We fine-tuned multiple pre-trained neural network models based on Squeeze-and-Excitation Networks (SENet) which achieved state-of-the-art results in the field of image recognition. We confirmed our data augmentation strategy improved classification performance and demonstrated 87.2% in balanced accuracy on the official ISIC2018 validation dataset.
Link: https://arxiv.org/abs/1809.02568
====================================================
Predicting Lung Nodule Malignancies by Combining Deep Convolutional Neural Network and Handcrafted Features (Shulong Li - 7 September, 2018)
To predict lung nodule malignancy with a high sensitivity and specificity, we propose a fusion algorithm that combines handcrafted features (HF) into the features learned at the output layer of a 3D deep convolutional neural network (CNN). For each 3D CNN, the CNN features combined with the 29 handcrafted features were used as the input for the support vector machine (SVM) coupled with the sequential forward feature selection (SFS) method to select the optimal feature subset and construct the classifiers. The patient cohort includes 431 malignant nodules and 795 benign nodules extracted from the LIDC/IDRI database
Link: https://arxiv.org/abs/1809.02333
====================================================
Computation of Total Kidney Volume from CT images in Autosomal Dominant Polycystic Kidney Disease using Multi-Task 3D Convolutional Neural Networks (Deepak Keshwani - 6 September, 2018)
In this work, we propose multi-task 3D Convolutional Neural Networks to segment ADPK and achieve a mean DICE score of 0.95 and mean absolute percentage TKV error of 3.86
Link: https://arxiv.org/abs/1809.02268
====================================================
Character-Aware Decoder for Neural Machine Translation (Adithya Renduchintala - 11 September, 2018)
We achieve character-awareness by augmenting both the softmax and embedding layers of an attention-based encoder-decoder network with convolutional neural networks that operate on spelling of a word (or subword). We experiment on the TED multi-target dataset, translating English into 14 typologically diverse languages
Link: https://arxiv.org/abs/1809.02223
====================================================
2PFPCE: Two-Phase Filter Pruning Based on Conditional Entropy (Chuhan Min - 6 September, 2018)
Our experiment result shows that combining these two strategies can achieve a higher neural network compression ratio than applying only one of them under the same accuracy drop threshold. Two-phase pruning, that is, combining both global and layer-wise strategies, achieves 10 X FLOPs reduction and 46% inference time reduction on VGG-16, with 2% accuracy drop.
Link: https://arxiv.org/abs/1809.02220
====================================================
ProdSumNet: reducing model parameters in deep neural networks via product-of-sums matrix decompositions (Chai Wah Wu - 6 September, 2018)
are all subsumed in this framework and we illustrate other types of neural network architectures within this framework. As an example, by using this decomposition on a reference CNN architecture for MNIST with over 3x10^6 trainable parameters, we are able to obtain an accuracy of 98.44% using only 3554 trainable parameters.
Link: https://arxiv.org/abs/1809.02209
====================================================
Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings (Mohsan Alvi - 27 September, 2018)
Neural networks achieve the state-of-the-art in image classification tasks
Link: https://arxiv.org/abs/1809.02169
====================================================
Deep learning for in vitro prediction of pharmaceutical formulations (Yilong Yang - 6 September, 2018)
The result shows the accuracies of both two deep neural networks were above 80% and higher than other machine learning models, which showed good prediction in pharmaceutical formulations
Link: https://arxiv.org/abs/1809.02069
====================================================
Code-switched Language Models Using Dual RNNs and Same-Source Pretraining (Saurabh Garg - 6 September, 2018)
We propose two techniques that significantly improve these LMs: 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data
Link: https://arxiv.org/abs/1809.01962
====================================================
RDPD: Rich Data Helps Poor Data via Imitation (Shenda Hong - 6 September, 2018)
RDPD consistently outperformed all baselines across all three datasets, especially achieving the greatest performance improvement over a standard neural network model trained on the common features (Direct model) by 24.56% on PR-AUC and 12.21% on ROC-AUC, and over the standard knowledge distillation model by 5.91% on PR-AUC and 4.44% on ROC-AUC.
Link: https://arxiv.org/abs/1809.01921
====================================================
Travel Speed Prediction with a Hierarchical Convolutional Neural Network and Long Short-Term Memory Model Framework (Wei Wang - 8 September, 2018)
In this paper, we propose a data-driven modelling approach with a novel hierarchical D-CLSTM-t deep learning model for short-term traffic speed prediction, a framework combined with convolutional neural network (CNN) and long short-term memory (LSTM) models. The model is trained end-to-end to predict travel speed in 15 to 90 minutes in the future
Link: https://arxiv.org/abs/1809.01887
====================================================
Deep Recurrent Electricity Theft Detection in AMI Networks with Random Tuning of Hyper-parameters (Mahmoud Nabil - 5 September, 2018)
Different from the existing research that utilizes shallow, static, and customer-specific-based electricity theft detectors, this paper proposes a generalized deep recurrent neural network (RNN)-based electricity theft detector that can effectively thwart these cyberattacks. Extensive test studies are carried out to investigate the detector's performance using publicly available real data of 107,200 energy consumption days from 200 customers
Link: https://arxiv.org/abs/1809.01774
====================================================
Neural Comic Style Transfer: Case Study (Maciej PÄÅko - 11 September, 2018)
[1] recently showed a neural style algorithm that can produce an image in the style of another image. We select different combinations of Adaptive Instance Normalization [11] and Universal Style Transfer [16] models and confront them to find their advantages and disadvantages in terms of qualitative and quantitative analysis. Finally, we present the results of a survey conducted on over 100 people that aims at validating the evaluation results in a real-life application of comic style transfer.
Link: https://arxiv.org/abs/1809.01726
====================================================
Pack and Detect: Fast Object Detection in Videos Using Region-of-Interest Packing (Athindran Ramesh Kumar - 1 October, 2018)
Although great progress has been made in improving the accuracy of object detection in recent years due to improved techniques for training and deploying deep neural networks, they are computationally very intensive. For example, processing a video at 300x300 resolution using the SSD300 (Single Shot Detector) object detection network with VGG16 as backbone at 30 fps requires 1.87 trillion FLOPS/s. This method can potentially reduce the number of FLOPS required for a frame by 4x. Tuning the algorithm parameters can provide a 1.3x increase in throughput with only a 2.5% drop in accuracy.
Link: https://arxiv.org/abs/1809.01701
====================================================
Breast Tumor Segmentation and Shape Classification in Mammograms using Generative Adversarial and Convolutional Neural Network (Vivek Kumar Singh - 2 October, 2018)
In addition, a shape descriptor based on a Convolutional Neural Network (CNN) is proposed to classify the generated masks into four mass shapes: irregular, lobular, oval and round. The proposed shape descriptor was trained on Digital Database for Screening Mammography (DDSM) yielding an overall accuracy of 80%, which outperforms the current state-of-the-art.
Link: https://arxiv.org/abs/1809.01687
====================================================
Bimodal network architectures for automatic generation of image annotation from text (Mehdi Moradi - 5 September, 2018)
In this work we propose two separate deep neural network architectures for automatic marking of a region of interest (ROI) on the image best representing a finding location, given a textual report or a set of keywords. The centroids of the ROIs marked by this network were on average at a distance equivalent to 5.1% of the image width from the centroids of the ground truth ROIs.
Link: https://arxiv.org/abs/1809.01610
====================================================
CNNs-based Acoustic Scene Classification using Multi-Spectrogram Fusion and Label Expansions (Weiping Zheng - 5 September, 2018)
Spectrograms have been widely used in Convolutional Neural Networks based schemes for acoustic scene classification, such as the STFT spectrogram and the MFCC spectrogram, etc. Specifically, accuracies of 0.9744, 0.8865 and 0.7778 are obtained for the LITIS Rouen dataset, the DCASE Development set and Evaluation set respectively.
Link: https://arxiv.org/abs/1809.01543
====================================================
Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models (Daniel Watson - 5 September, 2018)
We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.
Link: https://arxiv.org/abs/1809.01534
====================================================
Sentylic at IEST 2018: Gated Recurrent Neural Network and Capsule Network Based Approach for Implicit Emotion Detection (Prabod Rathnayaka - 5 September, 2018)
We have used a Gated Recurrent Neural Network (GRU) and a Capsule Network based model for the task. The proposed model managed to achieve a macro-F1 score of 0.692.
Link: https://arxiv.org/abs/1809.01452
====================================================
Retinal Vessel Segmentation under Extreme Low Annotation: A Generative Adversarial Network Approach (Avisek Lahiri - 5 September, 2018)
In this paper, we present a data efficient learning framework using the recent concept of Generative Adversarial Networks; this allows a deep neural network to perform significantly better than its fully supervised counterpart in low annotation regime. We experiment with extreme low annotation budget (0.8 - 1.6% of contemporary annotation size)
Link: https://arxiv.org/abs/1809.01348
====================================================
ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions (Hongyang Gao - 5 September, 2018)
Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs
Link: https://arxiv.org/abs/1809.01330
====================================================
RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency (Richard Futrell - 5 September, 2018)
Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language
Link: https://arxiv.org/abs/1809.01329
====================================================
BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis (Pamela Shapiro - 8 September, 2018)
Neural Machine Translation (NMT) in low-resource settings and of morphologically rich languages is made difficult in part by data sparsity of vocabulary words. We translate from 8 languages into English, using a multi-way parallel collection of TED transcripts
Link: https://arxiv.org/abs/1809.01301
====================================================
Unsupervised Statistical Machine Translation (Mikel Artetxe - 4 September, 2018)
While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points
Link: https://arxiv.org/abs/1809.01272
====================================================
Coverage-Guided Fuzzing for Deep Neural Networks (Xiaofei Xie - 6 September, 2018)
In company with the data explosion over the past decade, deep neural network (DNN) based software has experienced unprecedented leap and is becoming the key driving force of many novel industrial applications, including many safety-critical scenarios such as autonomous driving. The effectiveness of our framework is extensively investigated on 3 popular datasets (MNIST, CIFAR-10, ImageNet) and 7 DNNs with diverse complexities, under large set of 6 coverage criteria as feedback
Link: https://arxiv.org/abs/1809.01266
====================================================
A Quantum Spatial Graph Convolutional Neural Network using Quantum Passing Information (Lu Bai - 4 September, 2018)
Unlike state-of-the-art Graph Convolutional Neural Network (GCN) models, the proposed QSGCNN model incorporates the process of identifying transitive aligned vertices between graphs, and transforms arbitrary sized graphs into fixed-sized aligned vertex grid structures
Link: https://arxiv.org/abs/1809.01090
====================================================
Computing optimal discrete readout weights in reservoir computing is NP-hard (Fatemeh Hadaeghi - 4 September, 2018)
This problem has recently become practically relevant in the context of novel memristor-based neuromorphic microchip designs, where solving the UNQP is a key operation for on-chip training of the neural network implemented on the chip. The proof of NP-hardness is by reduction from the Unconstrained Binary Quadratic Programming problem, which is a special case of UNQP where $S = \{0, 1\}$ and which is known to be NP-hard.
Link: https://arxiv.org/abs/1809.01021
====================================================
Aesthetic Discrimination of Graph Layouts (Moritz Klammler - 4 September, 2018)
This paper addresses the following basic question: given two layouts of the same graph, which one is more aesthetically pleasing? We propose a neural network-based discriminator model trained on a labeled dataset that decides which of two layouts has a higher aesthetic quality. The mean prediction accuracy of our model is 95.70%, outperforming discriminators based on stress and on the linear combination of popular quality metrics by a statistically significant margin.
Link: https://arxiv.org/abs/1809.01017
====================================================
Geometric Operator Convolutional Neural Network (Yangling Ma - 4 September, 2018)
We present a framework called Geometric Operator Convolutional Neural Network (GO-CNN) that uses domain knowledge, wherein the kernel of the first convolutional layer is replaced with a kernel generated by a geometric operator function. In the practical task of medically diagnosing bone fractures, GO-CNN obtains 3% improvement in terms of the recall.
Link: https://arxiv.org/abs/1809.01016
====================================================
Migrating Knowledge between Physical Scenarios based on Artificial Neural Networks (Yurui Qu - 27 August, 2018)
Here, we propose to use transfer learning methods to migrate knowledge between different physical scenarios and significantly improve the prediction accuracy of artificial neural networks trained on a small dataset. First, we demonstrate that in predicting the transmission from multilayer photonic film, the relative error rate is reduced by 46.8% (26.5%) when the source data comes from 10-layer (8-layer) films and the target data comes from 8-layer (10-layer) films. Second, we show that the relative error rate is decreased by 22% when knowledge is transferred between two very different physical scenarios: transmission from multilayer films and scattering from multilayer nanoparticles
Link: https://arxiv.org/abs/1809.00972
====================================================
Deep Learning Based Vehicle Make-Model Classification (Burak Satar - 23 August, 2018)
A pipeline is proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN (Convolutional Neural Network) model to train on the database. It is reached approximately 4% better classification accuracy result than using a conventional CNN model
Link: https://arxiv.org/abs/1809.00953
====================================================
Improving generalization of vocal tract feature reconstruction: from augmented acoustic inversion to articulatory feature reconstruction without articulatory data (Rosanna Turrisi - 4 September, 2018)
We show that phonetic labels, used as input to deep recurrent neural networks that reconstruct articulatory features, are in general more helpful than acoustic features in both matched and mismatched training-testing conditions. Results show that articulatory features generated by this approach can correlate up to 0.59 Pearson product-moment correlation with measured articulatory features.
Link: https://arxiv.org/abs/1809.00938
====================================================
Image Reassembly Combining Deep Learning and Shortest Path Problem (M. -M. Paumard - 4 September, 2018)
The main contributions of this work are: 1) several deep neural architectures to predict the relative position of image fragments that outperform the previous state of the art; 2) casting the reassembly problem into the shortest path in a graph problem for which we provide several construction algorithms depending on available information; 3) a new dataset of images taken from the Metropolitan Museum of Art (MET) dedicated to image reassembly for which we provide a clear setup and a strong baseline.
Link: https://arxiv.org/abs/1809.00898
====================================================
NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification (Alexandra Chronopoulou - 3 September, 2018)
In this work, we experiment with neural Transfer Learning (TL) methods. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703.
Link: https://arxiv.org/abs/1809.00717
====================================================
Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks (Zhixiang Hao - 3 September, 2018)
Convolutional Neural Networks have demonstrated superior performance on single image depth estimation in recent years. Hence, it runs fast and can predict depth map in about 15 fps
Link: https://arxiv.org/abs/1809.00646
====================================================
Have You Stolen My Model? Evasion Attacks Against Deep Neural Network Watermarking Techniques (Dorjan Hitaj - 3 September, 2018)
Deep neural networks have had enormous impact on various domains of computer science, considerably outperforming previous state of the art machine learning techniques
Link: https://arxiv.org/abs/1809.00615
====================================================
Optical Flow Super-Resolution Based on Image Guidence Using Convolutional Neural Network (Liping Zhang - 3 September, 2018)
With the motivation of various convolutional neural network(CNN) structures succeeded in single image super-resolution(SISR) task, an end-to-end convolutional neural network is proposed to reconstruct the high resolution(HR) optical flow field from initial LR optical flow with the guidence of the first frame used in optical flow estimation. We evaluate the proposed approach on two different optical flow estimation mehods and show that it can not only obtain the full image resolution, but generate more accurate optical flow field (Accuracy improve 15% on FlyingChairs, 13% on MPI Sintel) with sharper edges than the estimation result of original method.
Link: https://arxiv.org/abs/1809.00588
====================================================
Data Augmentation for Neural Online Chat Response Selection (Wenchao Du - 2 September, 2018)
We investigate two data augmentation proxies, permutation and flipping, for neural dialog response selection task on various models over multiple datasets, including both Chinese and English languages. Empirical results show that our approach can gain 1 to 3 recall-at-1 points over baseline models in both full-scale and small-scale settings.
Link: https://arxiv.org/abs/1809.00428
====================================================
Neural Character-based Composition Models for Abuse Detection (Pushkar Mishra - 2 September, 2018)
The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words
Link: https://arxiv.org/abs/1809.00378
====================================================
Neural Ranking Models for Temporal Dependency Structure Parsing (Yuchen Zhang - 2 September, 2018)
It utilizes a neural ranking model with minimal feature engineering, and parses time expressions and events in a text into a temporal dependency tree structure. In a parsing-only evaluation setup where gold time expressions and events are provided, our parser reaches 0.81 and 0.70 f-score on unlabeled and labeled parsing respectively, a result that is very competitive against alternative approaches
Link: https://arxiv.org/abs/1809.00370
====================================================
Identifying Land Patterns from Satellite Imagery in Amazon Rainforest using Deep Learning (Somnath Rakshit - 2 September, 2018)
Here, it is shown how convolutional neural networks can be used to track changes in land patterns in the Amazon rainforests. In this work, a testing accuracy of 96.71% was obtained
Link: https://arxiv.org/abs/1809.00340
====================================================
Evaluation of Neural Networks for Image Recognition Applications: Designing a 0-1 MILP Model of a CNN to create adversarials (Lucas Schelkes - 1 September, 2018)
We follow up on (Fischetti & Jo, December, 2017) and show how standard convolutional neural network can be optimized to a more sophisticated capsule architecture. 2. 3
Link: https://arxiv.org/abs/1809.00216
====================================================
Learning Low Precision Deep Neural Networks through Regularization (Yoojin Choi - 31 August, 2018)
We consider the quantization of deep neural networks (DNNs) to produce low-precision models for efficient inference of fixed-point operations. We observe only $0.5$~dB peak signal-to-noise ratio (PSNR) loss when using binary weights and 8-bit activations
Link: https://arxiv.org/abs/1809.00095
====================================================
3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes (Ken C. L. Wong - 24 September, 2018)
With the introduction of fully convolutional neural networks, deep learning has raised the benchmark for medical image segmentation on both speed and accuracy, and different networks have been proposed for 2D and 3D segmentation with promising results. Nevertheless, most networks only handle relatively small numbers of labels (<10), and there are very limited works on handling highly unbalanced object sizes especially in 3D segmentation. We achieve an average Dice coefficient of 82% on brain segmentation with 20 labels, with the ratio of the smallest to largest object sizes as 0.14%. Less than 100 epochs are required to reach such accuracy, and segmenting a 128x128x128 volume only takes around 0.4 s.
Link: https://arxiv.org/abs/1809.00076
====================================================
Open Source Dataset and Machine Learning Techniques for Automatic Recognition of Historical Graffiti (Nikita Gordienko - 31 August, 2018)
The multinomial logistic regression (MLR) and a 2D convolutional neural network (CNN) models were applied. The MLR model demonstrated the area under curve (AUC) values for receiver operating characteristic (ROC) are not lower than 0.92 and 0.60 for notMNIST and CGCL, respectively. The CNN model gave AUC values close to 0.99 for both notMNIST and CGCL (despite the much smaller size and quality of CGCL in comparison to notMNIST) under condition of the high lossy data augmentation
Link: https://arxiv.org/abs/1808.10862
====================================================
Self-Attention Linguistic-Acoustic Decoder (Santiago Pascual - 31 August, 2018)
The conversion from text to speech relies on the accurate mapping from linguistic to acoustic symbol sequences, for which current practice employs recurrent statistical models like recurrent neural networks. On average, it increases Mel cepstral distortion between 0.1 and 0.3 dB, but it is over an order of magnitude faster on average
Link: https://arxiv.org/abs/1808.10678
====================================================
AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale (Jiayu Du - 12 September, 2018)
Pipelines support various state-of-the-art techniques, such as time-delayed neural networks and Lattic-Free MMI objective funciton
Link: https://arxiv.org/abs/1808.10583
====================================================
Multi-Cell Multi-Task Convolutional Neural Networks for Diabetic Retinopathy Grading (Kang Zhou - 11 October, 2018)
Considering the resolution of retinal image is very high, where small pathological tissues can be detected only with large resolution image and large local receptive field are required to identify those late stage disease, but directly training a neural network with very deep architecture and high resolution image is both time computational expensive and difficult because of gradient vanishing/exploding problem, we propose a \textbf{Multi-Cell} architecture which gradually increases the depth of deep neural network and the resolution of input image, which both boosts the training time but also improves the classification accuracy. Experimental results on the Kaggle dataset show that our method achieves a Kappa of 0.841 on test set which is the 4-th rank of all state-of-the-arts methods
Link: https://arxiv.org/abs/1808.10564
====================================================
Generalize Symbolic Knowledge With Neural Rule Engine (Shen Li - 4 September, 2018)
As neural networks have dominated the state-of-the-art results in a wide range of NLP tasks, it attracts considerable attention to improve the performance of neural models by integrating symbolic knowledge
Link: https://arxiv.org/abs/1808.10326
====================================================
RoI-based Robotic Grasp Detection in Object Overlapping Scenes Using Convolutional Neural Network (Hanbo Zhang - 18 September, 2018)
Recent works demonstrate the advanced performance of Convolutional Neural Network (CNN) on robotic grasp detection. Experimental results demonstrate that our algorithm achieves 24.9% miss rate at 1FPPI and 68.2% mAP with grasp on our dataset. Robotic experiments demonstrate that our proposed algorithm can help robots grasp specified target in multi-object scenes at 84% success rate.
Link: https://arxiv.org/abs/1808.10313
====================================================
Multi-Source Syntactic Neural Machine Translation (Anna Currey - 30 August, 2018)
We introduce a novel multi-source technique for incorporating source syntax into neural machine translation using linearized parses. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English-German task
Link: https://arxiv.org/abs/1808.10267
====================================================
Direct Output Connection for a High-Rank Language Model (Sho Takase - 30 August, 2018)
This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers
Link: https://arxiv.org/abs/1808.10143
====================================================
Hard Non-Monotonic Attention for Character-Level Transduction (Shijie Wu - 29 August, 2018)
In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1
Link: https://arxiv.org/abs/1808.10024
====================================================
Reasoning about Actions and State Changes by Injecting Commonsense Knowledge (Niket Tandon - 29 August, 2018)
Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing hard and soft constraints to steer the model away from unlikely predictions. We show that the new model significantly outperforms earlier systems on a benchmark dataset for procedural text comprehension (+8% relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.
Link: https://arxiv.org/abs/1808.10012
====================================================
Learning End-to-End Goal-Oriented Dialog with Multiple Answers (Janarthanan Rajendran - 24 August, 2018)
We show that there is a significant drop in performance of existing end-to-end neural methods from 81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on permuted-bAbI dialog tasks. We also show that our proposed method improves the performance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog tasks.
Link: https://arxiv.org/abs/1808.09996
====================================================
FPGA Implementation of Convolutional Neural Networks with Fixed-Point Calculations (Roman A. Solovyev - 29 August, 2018)
Mobile neural networks typically have reduced number of parameters and require a relatively small number of arithmetic operations. The use of mobile networks without further optimization may not provide sufficient performance when high processing speed is required, for example, in real-time video processing (30 frames per second)
Link: https://arxiv.org/abs/1808.09945
====================================================
Searching Toward Pareto-Optimal Device-Aware Neural Architectures (An-Chieh Cheng - 29 August, 2018)
Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performance in many tasks such as image classification and language understanding
Link: https://arxiv.org/abs/1808.09830
====================================================
Rule induction for global explanation of trained models (Madhumita Sushil - 29 August, 2018)
We find that the output rule-sets can explain the predictions of a neural network trained for 4-class text classification from the 20 newsgroups dataset to a macro-averaged F-score of 0.80
Link: https://arxiv.org/abs/1808.09744
====================================================
An Operation Sequence Model for Explainable Neural Machine Translation (Felix Stahlberg - 29 August, 2018)
In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical machine translation as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.
Link: https://arxiv.org/abs/1808.09688
====================================================
Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation (Yilin Yang - 10 September, 2018)
Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality
Link: https://arxiv.org/abs/1808.09582
====================================================
Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation (Renjie Zheng - 28 August, 2018)
Neural text generation, including neural machine translation, image captioning, and summarization, has been quite successful recently. However, during training time, typically only one reference is considered for each example, even though there are often multiple references available, e.g., 4 references in NIST MT evaluations, and 5 references in image captioning data
Link: https://arxiv.org/abs/1808.09564
====================================================
Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules? (FrÃ©deric Godin - 28 August, 2018)
2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks
Link: https://arxiv.org/abs/1808.09551
====================================================
Towards Semi-Supervised Learning for Deep Semantic Role Labeling (Sanket Vaibhav Mehta - 28 August, 2018)
Neural models have shown several state-of-the-art performances on Semantic Role Labeling (SRL). On CoNLL-2012 English section, the proposed semi-supervised training with 1%, 10% SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58, +0.78 F1, respectively, over the pre-trained models that were trained on SOTA architecture with ELMo on the same SRL-labeled data. Additionally, by using the syntactic-inconsistency loss on inference time, the proposed model achieves +3.67, +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively.
Link: https://arxiv.org/abs/1808.09543
====================================================
Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data (Zi Lin - 29 August, 2018)
Based on the new data, we then evaluate three off-the-shelf SRL systems, i.e., the PCFGLA-parser-based, neural-parser-based and neural-syntax-agnostic systems, to gauge how successful SRL for learner Chinese can be. We find two non-obvious facts: 1) the L1-sentence-trained systems performs rather badly on the L2 data; 2) the performance drop from the L1 data to the L2 data of the two parser-based systems is much smaller, indicating the importance of syntactic parsing in SRL for interlanguages. Our model achieves an F-score of 72.06, which is a 2.02 point improvement over the best baseline.
Link: https://arxiv.org/abs/1808.09409
====================================================
Understanding Back-Translation at Scale (Sergey Edunov - 2 October, 2018)
An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set.
Link: https://arxiv.org/abs/1808.09381
====================================================
A Tree-based Decoder for Neural Machine Translation (Xinyi Wang - 28 August, 2018)
Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.
Link: https://arxiv.org/abs/1808.09374
====================================================
Bridging Knowledge Gaps in Neural Entailment via Symbolic Models (Dongyeop Kang - 4 September, 2018)
Our new architecture combines standard neural entailment models with a knowledge lookup module. On the SciTail dataset, NSnet outperforms a simpler combination of the two predictions by 3% and the base entailment model by 5%.
Link: https://arxiv.org/abs/1808.09333
====================================================
Motorcycle Classification in Urban Scenarios using Convolutional Neural Networks for Feature Extraction (Jorge E. Espinosa - 28 August, 2018)
This paper presents a motorcycle classification system for urban scenarios using Convolutional Neural Network (CNN). The obtained results show a mean accuracy of 99.40% and 99.29% on a classification task of three and five classes respectively
Link: https://arxiv.org/abs/1808.09273
====================================================
Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks (Martin Schmitt - 28 August, 2018)
We conduct experiments with different neural architectures and word representations on the recent GermEval 2017 dataset. The combination of a convolutional neural network and fasttext embeddings outperformed the best submission of the shared task in 2017, establishing a new state of the art.
Link: https://arxiv.org/abs/1808.09238
====================================================
Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation ( Hardy - 28 August, 2018)
In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance using the latter is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset
Link: https://arxiv.org/abs/1808.09160
====================================================
Disfluency Detection using Auto-Correlational Neural Networks (Paria Jamshid Lou - 27 August, 2018)
The model uses a convolutional neural network (CNN) and augments it with a new auto-correlation operator at the lowest layer that can capture the kinds of "rough copy" dependencies that are characteristic of repair disfluencies in speech. In experiments, the ACNN model outperforms the baseline CNN on a disfluency detection task with a 5% increase in f-score, which is close to the previous best result on this task.
Link: https://arxiv.org/abs/1808.09092
====================================================
Evaluating the Utility of Hand-crafted Features in Sequence Labelling (Minghao Wu - 27 August, 2018)
We evaluate on the task of named entity recognition (NER), where we show that including manual features for part-of-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a $F_1$ of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of auto-encoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60\%, while retaining the same predictive accuracy.
Link: https://arxiv.org/abs/1808.09075
====================================================
Cognitive Consistency Routing Algorithm of Capsule-network (Huayu Li - 19 September, 2018)
Capsule Neural Network (Sabour S, et al.[2017]) is a novel structure of Convolutional Neural Networks which simulates the visual processing system of human brain
Link: https://arxiv.org/abs/1808.09062
====================================================
Parameter sharing between dependency parsers for related languages (Miryam de Lhoneux - 4 October, 2018)
Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different language family
Link: https://arxiv.org/abs/1808.09055
====================================================
Review Helpfulness Assessment based on Convolutional Neural Network (Xianshan Qu - 27 August, 2018)
In this paper we describe the implementation of a convolutional neural network (CNN) used to assess online review helpfulness. We demonstrate that this can improve the overall accuracy by 2%. Finally, we evaluate the method on a benchmark dataset and show an improvement in accuracy relative to published results for traditional methods of 2.5% for a model trained using only review text and 4.24% for a model trained on a combination of rating star information and review text.
Link: https://arxiv.org/abs/1808.09016
====================================================
Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation (Marzieh Fadaee - 21 September, 2018)
Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.
Link: https://arxiv.org/abs/1808.09006
====================================================
COFGA: Classification Of Fine-Grained Features In Aerial Images (Eran Dahan - 27 August, 2018)
We examine the results of existing state-of-the-art models and modified deep neural networks
Link: https://arxiv.org/abs/1808.09001
====================================================
Large Margin Neural Language Model (Jiaji Huang - 27 August, 2018)
Conventionally, neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction for speech recognition and 1.0 BLEU increase for machine translation.
Link: https://arxiv.org/abs/1808.08987
====================================================
Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures (Gongbo Tang - 28 August, 2018)
Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.
Link: https://arxiv.org/abs/1808.08946
====================================================
Deep Learning for Stress Field Prediction Using Convolutional Neural Networks (Zhenguo Nie - 27 August, 2018)
One is Feature Representation embedded Convolutional Neural Network (FR-CNN) with a single input channel, and the other is Squeeze-and-Excitation Residual network modules embedded Fully Convolutional Neural network (SE-Res-FCN) with multiple input channels. Mean relative error (MRE) of the SE-Res-FCN model is about 0.25% with respect to the average ground truth
Link: https://arxiv.org/abs/1808.08914
====================================================
Real-Time MDNet (Ilchae Jung - 27 August, 2018)
We present a fast and accurate visual tracking algorithm based on the multi-domain convolutional neural network (MDNet). We accomplish approximately 25 times speed-up with almost identical accuracy compared to MDNet
Link: https://arxiv.org/abs/1808.08834
====================================================
Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions (Ke Ning - 27 August, 2018)
First, we propose a bi-directional Recurrent Neural Network (RNN) with a finely calibrated vision-language attentive mechanism to comprehensively understand the free-formed natural language descriptions. Our ASST outperforms the state-of-the-art by $4.28\%$ in Rank$@1$ on the DiDeMo dataset. On the Charades-STA dataset, we significantly improve the state-of-the-art by $13.41\%$ in Rank$@1,IoU=0.5$.
Link: https://arxiv.org/abs/1808.08803
====================================================
Natural Language Inference with Hierarchical BiLSTM Max Pooling Architecture (Aarne Talman - 27 August, 2018)
Recurrent neural networks have proven to be very effective for natural language inference tasks. We also show that our sentence embeddings can be utilized in a wide variety of transfer learning tasks, outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks. Furthermore, our model beats the InferSent model in 8 out of 10 recently published SentEval probing tasks designed to evaluate sentence embeddings' ability to capture some of the important linguistic properties of sentences.
Link: https://arxiv.org/abs/1808.08762
====================================================
Generating Text through Adversarial Training using Skip-Thought Vectors (Afroz Ahamad - 27 August, 2018)
In the field of Natural Language Processing, word embeddings such as word2vec and GLoVe are state-of-the-art methods for applying neural network models on textual data
Link: https://arxiv.org/abs/1808.08703
====================================================
Augmenting Bottleneck Features of Deep Neural Network Employing Motor State for Speech Recognition at Humanoid Robots (Moa Lee - 27 August, 2018)
When learning the bottleneck features to catch, we first exploit the motor on/off state data as supplementary information in addition to the acoustic features as the input of the first deep neural network (DNN) for preliminary acoustic modeling. When the proposed method is evaluated in terms of phoneme error rate (PER) on TIMIT database, the experimental results show that achieve obvious improvement (11% relative) is achieved by our algorithm over the conventional systems.
Link: https://arxiv.org/abs/1808.08702
====================================================
Deeply Supervised Depth Map Super-Resolution as Novel View Synthesis (Xibin Song - 27 August, 2018)
Deep convolutional neural network (DCNN) has been successfully applied to depth map super-resolution and outperforms existing methods by a wide margin. However, there still exist two major issues with these DCNN based depth map super-resolution methods that hinder the performance: i) The low-resolution depth maps either need to be up-sampled before feeding into the network or substantial deconvolution has to be used; and ii) The supervision (high-resolution depth maps) is only applied at the end of the network, thus it is difficult to handle large up-sampling factors, such as $\times 8, \times 16$. $\times 8, \times 16$)
Link: https://arxiv.org/abs/1808.08688
====================================================
Fast and Accurate Recognition of Chinese Clinical Named Entities with Residual Dilated Convolutions (Jiahui Qiu - 29 August, 2018)
Specifically, Chinese characters and dictionary features are first projected into dense vector representations, then they are fed into the residual dilated convolutional neural network to capture contextual features. Computational results on the CCKS-2017 Task 2 benchmark dataset show that our proposed RD-CNN-CRF method competes favorably with state-of-the-art RNN-based methods both in terms of computational performance and training time.
Link: https://arxiv.org/abs/1808.08669
====================================================
Online Human Activity Recognition using Low-Power Wearable Devices (Ganapati Bhat - 26 August, 2018)
Using these features, we design an artificial neural network classifier which is trained online using the policy gradient algorithm. Experiments on a low power IoT device (TI-CC2650 MCU) with nine users show 97.7% accuracy in identifying six activities and their transitions with less than 12.5 mW power consumption.
Link: https://arxiv.org/abs/1808.08615
====================================================
Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge (Pasquale Minervini - 26 August, 2018)
Furthermore, we propose a method for adversarially regularising neural NLI models for incorporating background knowledge. Our results show that, while the proposed method does not always improve results on the SNLI and MultiNLI datasets, it significantly and consistently increases the predictive accuracy on adversarially-crafted datasets -- up to a 79.6% relative improvement -- while drastically reducing the number of background knowledge violations
Link: https://arxiv.org/abs/1808.08609
====================================================
Semi-Autoregressive Neural Machine Translation (Chunqi Wang - 26 August, 2018)
Existing approaches to neural machine translation are typically autoregressive models. On WMT'14 English-German translation, the SAT achieves 5.58$\times$ speedup while maintaining 88\% translation quality, significantly better than the previous non-autoregressive methods
Link: https://arxiv.org/abs/1808.08583
====================================================
Rain Streak Removal for Single Image via Kernel Guided CNN (Ye-Tao Wang - 28 August, 2018)
In this paper, we propose a novel rain streak removal approach using a kernel guided convolutional neural network (KGCNN), achieving the state-of-the-art performance with simple network architectures
Link: https://arxiv.org/abs/1808.08545
====================================================
Event Detection with Neural Networks: A Rigorous Empirical Evaluation (J. Walker Orr - 26 August, 2018)
While the neural network models have generally led the state-of-the-art, the differences in performance between different architectures have not been rigorously studied
Link: https://arxiv.org/abs/1808.08504
====================================================
Paraphrases as Foreign Languages in Multilingual Neural Machine Translation (Zhong Zhou - 25 August, 2018)
We treat paraphrases as foreign languages, tag source sentences with paraphrase labels, and train in the style of multilingual Neural Machine Translation (NMT). We achieve a BLEU score of 57.2 for French-to-English translation, training on 24 paraphrases of the Bible, which is ~+27 above the WMT'14 baseline.
Link: https://arxiv.org/abs/1808.08438
====================================================
Meta-Learning for Low-Resource Neural Machine Translation (Jiatao Gu - 25 August, 2018)
In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML) for low-resource neural machine translation (NMT). For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT'16 by seeing only 16,000 translated words (~600 parallel sentences).
Link: https://arxiv.org/abs/1808.08437
====================================================
Deep Convolutional Neural Network with Mixup for Environmental Sound Classification (Zhichao Zhang - 25 August, 2018)
In this paper, we propose to use a novel deep convolutional neural network for ESC tasks. Our experimental results demonstrated that our ESC system has achieved the state-of-the-art performance (83.7%) on UrbanSound8K and competitive performance on ESC-50 and ESC-10.
Link: https://arxiv.org/abs/1808.08405
====================================================
A Novel Deep Neural Network Architecture for Mars Visual Navigation (Jiang Zhang - 25 August, 2018)
Specifically, to achieve precise landing and autonomous navigation, a novel deep neural network architecture with double branches and non-recurrent structure is designed, which can represent both global and local deep features of Martian environment images effectively. Moreover, compared with the existing state-of-the-art algorithm, the training time is reduced by 45.8%
Link: https://arxiv.org/abs/1808.08395
====================================================
Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection (Mahdieh Abbasi - 3 October, 2018)
Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision
Link: https://arxiv.org/abs/1808.08282
====================================================
Improving Breast Cancer Detection using Symmetry Information with Deep Learning (Yeman Brhane Hagos - 17 August, 2018)
Convolutional Neural Networks (CNN) have had a huge success in many areas of computer vision and medical image analysis. The network was trained on a large-scale dataset of 28294 mammogram images. At candidate level, AUC value of 0.933 with 95% confidence interval of [0.920, 0.954] was obtained when symmetry information is incorporated in comparison with baseline architecture which yielded AUC value of 0.929 with [0.919, 0.947] confidence interval. By incorporating symmetrical information, although there was no a significant candidate level performance again (p = 0.111), we have found a compelling result at exam level with CPM value of 0.733 (p = 0.001)
Link: https://arxiv.org/abs/1808.08273
====================================================
Recalibrating Fully Convolutional Networks with Spatial and Channel 'Squeeze & Excitation' Blocks (Abhijit Guha Roy - 23 August, 2018)
In a wide range of semantic segmentation tasks, fully convolutional neural networks (F-CNNs) have been successfully leveraged to achieve state-of-the-art performance. Importantly, SE blocks only lead to a minimal increase in model complexity of about 1.5%, while the Dice score increases by 4-9% in the case of U-Net
Link: https://arxiv.org/abs/1808.08127
====================================================
Memory Time Span in LSTMs for Multi-Speaker Source Separation (Jeroen Zegers - 24 August, 2018)
With deep learning approaches becoming state-of-the-art in many speech (as well as non-speech) related machine learning tasks, efforts are being taken to delve into the neural networks which are often considered as a black box
Link: https://arxiv.org/abs/1808.08097
====================================================
Atherosclerotic carotid plaques on panoramic imaging: an automatic detection using deep learning with small dataset (Lazar Kats - 24 August, 2018)
Recently, there has been a definite breakthrough in the field of analysis of medical images due to the use of deep learning based on neural networks. We aimed to assess the operation of the algorithm on a small database of 65 panoramic images. ACP was detected with a sensitivity of 75%, specificity of 80% and an accuracy of 83%. The ROC analysis showed a significant Area Under Curve (AUC) difference from 0.5
Link: https://arxiv.org/abs/1808.08093
====================================================
Features of word similarity (Arthur M. Jacobs - 23 August, 2018)
Using regression and predictive modeling tools (neural net, decision tree) the performance of a total of 28 models using different combinations of both surface and semantic word features is evaluated
Link: https://arxiv.org/abs/1808.07999
====================================================
Ontology Reasoning with Deep Neural Networks (Patrick Hohenecker - 4 September, 2018)
In this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform basic ontology reasoning
Link: https://arxiv.org/abs/1808.07980
====================================================
Deconvolutional Networks for Point-Cloud Vehicle Detection and Tracking in Driving Scenarios (Victor Vaquero - 23 August, 2018)
Our detection step uses a Convolutional Neural Network (CNN) that receives as input a featured representation of the 3D information provided by a Velodyne HDL-64 sensor and returns a per-point classification of whether it belongs to a vehicle or not. Our lidar-based approach uses about a 4% of the data needed for an image-based detector with similarly competitive results.
Link: https://arxiv.org/abs/1808.07935
====================================================
The Importance of Generation Order in Language Modeling (Nicolas Ford - 23 August, 2018)
Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks
Link: https://arxiv.org/abs/1808.07910
====================================================
LIFT: Reinforcement Learning in Computer Systems by Learning From Demonstrations (Michael Schaarschmidt - 23 August, 2018)
Recent successes in combining deep neural networks with reinforcement learning have sparked significant new interest in this domain. Results show LIFT controllers initialized from demonstrations can outperform human baselines and heuristics across latency metrics and space usage by up to 70%.
Link: https://arxiv.org/abs/1808.07903
====================================================
Segmentation of Bleeding Regions in Wireless Capsule Endoscopy for Detection of Informative Frames (Mohsen Hajabdollahi - 23 August, 2018)
Suitable color channels are selected as neural networks inputs, and image classification is conducted using a multi-layer perceptron (MLP) and a convolutional neural network (CNN) separately. Simulation results show that applying simplification methods on both MLP and CNN structures reduces the number of computational operations significantly with AUC greater than 0.97
Link: https://arxiv.org/abs/1808.07746
====================================================
Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations (Guangxiang Zhao - 22 August, 2018)
The proposed method consists of two parts: a label-graph based neural network, and a soft training mechanism with correlation-based continuous label representation. Especially, the micro F1 is improved from 53.9 to 64.5, and the one-error is reduced from 30.5 to 22.6
Link: https://arxiv.org/abs/1808.07604
====================================================
SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation (Xinyi Wang - 28 August, 2018)
In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a)
Link: https://arxiv.org/abs/1808.07512
====================================================
Progressive Deep Neural Networks Acceleration via Soft Filter Pruning (Yang He - 23 August, 2018)
3) Pruning the neural network progressively makes the selection of low-norm filters much more stable, which has a potential to get a better performance. Notably, on ILSCRC-2012, our method reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art. On ResNet-50, our progressive pruning method have 1.08% top-1 accuracy improvement over the pruning method without progressive pruning.
Link: https://arxiv.org/abs/1808.07471
====================================================
3D Topology Optimization using Convolutional Neural Networks (Saurabh Banga - 22 August, 2018)
We conduct a comparative study between multiple strategies for training the neural network and assess the effect of using various input combinations for the CNN to finalize the strategy with the highest accuracy in predictions for practical deployment. For the best performing network, we achieved about 40% reduction in overall computation time while also attaining structural accuracies in the order of 96%.
Link: https://arxiv.org/abs/1808.07440
====================================================
Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks (Charith Mendis - 20 August, 2018)
Ithemal uses a novel Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven approach for throughput estimation. In particular, our model has a worst case average error of 10.53% on actual throughput values when compared to best case average errors of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's machine code analyzer when compared on three different microarchitectures, while predicting throughput values at a faster rate than aforementioned tools
Link: https://arxiv.org/abs/1808.07412
====================================================
Neural Named Entity Recognition from Subword Units (Abdalghani Abujabal - 27 August, 2018)
Existing neural models for NER rely mostly on dedicated word-level representations, which suffer from two main shortcomings: 1) the vocabulary size is large, yielding large memory requirements and training time, and 2) they cannot learn morphological representations. Our experiments show that 1) with increasing training data, performance of models trained solely on subword units becomes closer to that of models with dedicated word-level embeddings (91.35 vs 93.92 F1 for English), while using a much smaller vocabulary size (332 vs 74K), 2) subword units enhance models with dedicated word-level embeddings, and 3) combining different subword units improves performance.
Link: https://arxiv.org/abs/1808.07364
====================================================
An Attention-Gated Convolutional Neural Network for Sentence Classification (Yang Liu - 28 August, 2018)
In this paper, we propose an Attention Gated Convolutional Neural Network (AGCNN) for sentence classification, which generates attention weights from the feature's context windows of different sizes by using specialized convolution encoders, to enhance the influence of critical features in predicting the sentence's category. Experimental results demonstrate that our model could achieve a general accuracy improvement highest up to 3.1% (compared with standard CNN models), and gain competitive results over the strong baseline methods on four out of the six tasks
Link: https://arxiv.org/abs/1808.07325
====================================================
k-meansNet: When k-means Meets Differentiable Programming (Xi Peng - 22 August, 2018)
In such a way, we advance the boundary of differentiable programming by treating the neural network as from an alternative optimization approach to the problem formulation. Extensive experimental studies show that our method achieves promising performance comparing with 12 clustering methods on some challenging datasets.
Link: https://arxiv.org/abs/1808.07292
====================================================
Neural Architecture Optimization (Renqian Luo - 5 September, 2018)
There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. Specifically we obtain $2.07\%$ test set error rate for CIFAR-10 image classification task and $55.9$ test set perplexity of PTB language modeling task
Link: https://arxiv.org/abs/1808.07233
====================================================
Don't Use Large Mini-Batches, Use Local SGD (Tao Lin - 6 October, 2018)
Mini-batch stochastic gradient methods are the current state of the art for large-scale distributed training of neural networks and other machine learning models
Link: https://arxiv.org/abs/1808.07217
====================================================
Coarse-to-Fine Annotation Enrichment for Semantic Segmentation Learning (Yadan Luo - 21 August, 2018)
Extensive experiments on the Cityscapes and PASCAL VOC 2012 benchmarks have shown that the neural networks trained with the enriched annotations from our framework yield a significant improvement over that trained with the original coarse labels
Link: https://arxiv.org/abs/1808.07209
====================================================
Fisher Information and Natural Gradient Learning of Random Deep Networks (Shun-ichi Amari - 21 August, 2018)
A deep neural network is a hierarchical nonlinear model transforming input signals to output signals. We further prove that the Fisher information matrix of a single unit has a simple reduced form, a sum of a diagonal matrix and a rank 2 matrix of weight-bias correlations
Link: https://arxiv.org/abs/1808.07172
====================================================
On Deep Neural Networks for Detecting Heart Disease (Nathalie-Sofia Tomov - 21 August, 2018)
To this end, we investigate the potential of using data analysis, and in particular the design and use of deep neural networks (DNNs) for detecting heart disease based on routine clinical data. The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area.
Link: https://arxiv.org/abs/1808.07168
====================================================
Exploring a Unified Attention-Based Pooling Framework for Speaker Verification (Yi Liu - 21 August, 2018)
The pooling layer is an essential component in the neural network based speaker verification. Experiments on the Fisher and NIST SRE 2010 dataset show that involving outputs from lower layers to compute the attention weights can outperform average pooling and achieve better results than vanilla attention method
Link: https://arxiv.org/abs/1808.07120
====================================================
Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture (Soumil Mandal - 21 August, 2018)
Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28% and 93.32% is achieved on our two testing sets.
Link: https://arxiv.org/abs/1808.07118
====================================================
Improving Super-Resolution Methods via Incremental Residual Learning (Muneeb Aadil - 21 August, 2018)
Recently, deep Convolutional Neural Networks (CNNs) have shown promising performance in accurate reconstruction of high resolution (HR) image, given its low resolution (LR) counter-part. Furthermore, as our method is trained to learned residuals, complete set of branches are trained in only 20% of time relative to base network.
Link: https://arxiv.org/abs/1808.07110
====================================================
MLPdf: An Effective Machine Learning Based Approach for PDF Malware Detection (Jason Zhang - 21 August, 2018)
In this paper, we propose a novel approach based on a multilayer perceptron (MLP) neural network model, termed MLPdf, for the detection of PDF based malware. A group of high quality features are extracted from two real-world datasets which comprise around 105000 benign and malicious PDF documents. Evaluation results indicate that the proposed MLPdf approach exhibits excellent performance which significantly outperforms all evaluated eight well known commercial anti-virus scanners with a much higher true positive rate of 95.12% achieved while maintaining a very low false positive rate of 0.08%.
Link: https://arxiv.org/abs/1808.06991
====================================================
End to End Vehicle Lateral Control Using a Single Fisheye Camera (Marin Toromanoff - 20 August, 2018)
Convolutional neural networks are commonly used to control the steering angle for autonomous cars. Experiments are conducted on a custom dataset corresponding to more than 10000 km and 200 hours of open road driving. In our simulator based on real-world videos, the final model was capable of more than 99% autonomy on urban road
Link: https://arxiv.org/abs/1808.06940
====================================================
Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks (Yang He - 21 August, 2018)
This paper proposed a Soft Filter Pruning (SFP) method to accelerate the inference procedure of deep Convolutional Neural Networks (CNNs). Notably, on ILSCRC-2012, SFP reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art
Link: https://arxiv.org/abs/1808.06866
====================================================
Deep Learned Full-3D Object Completion from Single View (Dario Rethage - 21 August, 2018)
This writing proposes a new approach to 3D reconstruction and scene understanding, which implicitly learns 3D geometry from depth maps pairing a deep convolutional neural network architecture with an auto-encoder. The relatively small network, consisting of roughly 4 million weights, achieves a 92.9% reconstruction accuracy at a 30x30x30 resolution through the use of a pre-trained decompression layer
Link: https://arxiv.org/abs/1808.06843
====================================================
Translational Grounding: Using Paraphrase Recognition and Generation to Demonstrate Semantic Abstraction Abilities of MultiLingual NMT (JÃ¶rg Tiedemann - 21 August, 2018)
In this paper, we investigate whether multilingual neural translation models learn a stronger semantic abstraction of sentences than bilingual ones. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English
Link: https://arxiv.org/abs/1808.06826
====================================================
Are You Tampering With My Data? (Michele Alberti - 21 August, 2018)
We demonstrate on two widely used datasets (CIFAR-10 and SVHN) that a universal modification of just one pixel per image for all the images of a class in the training set is enough to corrupt the training procedure of several state-of-the-art deep neural networks causing the networks to misclassify any images to which the modification is applied
Link: https://arxiv.org/abs/1808.06809
====================================================
Lessons from Natural Language Inference in the Clinical Domain (Alexey Romanov - 27 August, 2018)
State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. SNLI) and 2) incorporate domain knowledge from external data and lexical sources (e.g
Link: https://arxiv.org/abs/1808.06752
====================================================
Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning (Tianyi Liu - 20 August, 2018)
Then we construct a neural network inputting the sub-tree while applying the entity-wise attention to identify the important semantic features of relational words in each instance. Experiments show that our approach is effective and improves the area of Precision/Recall(PR) from 0.35 to 0.39 over the state-of-the-art work.
Link: https://arxiv.org/abs/1808.06738
====================================================
Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks (Sercan O. Arik - 20 August, 2018)
We propose the multi-head convolutional neural network (MCNN) architecture for waveform synthesis from spectrograms. MCNN achieves more than an order of magnitude higher compute intensity than commonly-used iterative algorithms like Griffin-Lim, yielding efficient utilization for modern multi-core processors, and very fast (more than 300x real-time) waveform synthesis
Link: https://arxiv.org/abs/1808.06719
====================================================
VERAM: View-Enhanced Recurrent Attention Model for 3D Shape Classification (Songle Chen - 20 August, 2018)
Multi-view deep neural network is perhaps the most successful approach in 3D shape classification. This is surmounted by three essential view-enhancement strategies: 1) enhancing the information flow of gradient backpropagation for the view estimation subnetwork, 2) devising a highly informative reward function for the reinforcement training of view estimation and 3) formulating a novel loss function that explicitly circumvents view duplication. Taking grayscale image as input and AlexNet as CNN architecture, VERAM with 9 views achieves instance-level and class-level accuracy of 95:5% and 95:3% on ModelNet10, 93:7% and 92:1% on ModelNet40, both are the state-of-the-art performance under the same number of views.
Link: https://arxiv.org/abs/1808.06698
====================================================
Class2Str: End to End Latent Hierarchy Learning (Soham Saha - 20 August, 2018)
Deep neural networks for image classification typically consists of a convolutional feature extractor followed by a fully connected classifier network. Compared to the previous work of HDCNN, which also learns a 2 level hierarchy, we are able to learn a hierarchy at an arbitrary number of levels as well as obtain an accuracy improvement on the Imagenet classification task over them
Link: https://arxiv.org/abs/1808.06675
====================================================
A Hybrid Differential Evolution Approach to Designing Deep Convolutional Neural Networks for Image Classification (Bin Wang - 21 August, 2018)
Convolutional Neural Networks (CNNs) have demonstrated their superiority in image classification, and evolutionary computation (EC) methods have recently been surging to automatically design the architectures of CNNs to save the tedious work of manually designing CNNs. The proposed algorithm is tested on six widely-used benchmark datasets and the results are compared to 12 state-of-the-art methods, which shows the proposed method is vigorously competitive to the state-of-the-art algorithms
Link: https://arxiv.org/abs/1808.06661
====================================================
R-CRNN: Region-based Convolutional Recurrent Neural Network for Audio Event Detection (Chieh-Chi Kao - 20 August, 2018)
This paper proposes a Region-based Convolutional Recurrent Neural Network (R-CRNN) for audio event detection (AED). The proposed method is tested on DCASE 2017 Challenge dataset. Compared to the other region-based network for AED (R-FCN) with an event-based error rate (ER) of 0.18 on the development set, our method reduced the ER to half.
Link: https://arxiv.org/abs/1808.06627
====================================================
Detecting cognitive impairments by agreeing on interpretations of linguistic features (Zining Zhu - 4 September, 2018)
All neural networks are optimized iteratively. Overall, using all of the 413 linguistic features, our models significantly outperform traditional classifiers, which are used by the state-of-the-art papers.
Link: https://arxiv.org/abs/1808.06570
====================================================
Collaborative Pressure Ulcer Prevention: An Automated Skin Damage and Pressure Ulcer Assessment Tool for Nursing Professionals, Patients, Family Members and Carers (Paul Fergus - 17 August, 2018)
Using state-of-the-art technologies in convolutional neural networks and transfer learning along with end-to-end web technologies, this platform allows pressure ulcers to be analysed and findings to be reported
Link: https://arxiv.org/abs/1808.06503
====================================================
Deep Residual Network for Sound Source Localization in the Time Domain (Dmitry Suvorov - 20 August, 2018)
This study presents a system for sound source localization in time domain using a deep residual neural network. Data from the linear 8 channel microphone array with 3 cm spacing is used by the network for direction estimation. The accuracy classification of 30 m sec sound frames is 99.2%. Its usage decreased word error rate by 1.14% in comparison with similar speech recognition pipeline using GCC-PHAT sound source localization.
Link: https://arxiv.org/abs/1808.06429
====================================================
Question Generation from SQL Queries Improves Neural Semantic Parsing (Daya Guo - 27 August, 2018)
First, we demonstrate that question generation is an effective method that empowers us to learn a state-of-the-art neural network based semantic parser with thirty percent of the supervised training data
Link: https://arxiv.org/abs/1808.06304
====================================================
Neural Machine Translation of Text from Non-Native Speakers (Alison Lui - 19 August, 2018)
Neural Machine Translation (NMT) systems are known to degrade when confronted with noisy data, especially when the system is trained only on clean data. In combination with an automatic grammar error correction system, we can recover 1.5 BLEU out of 2.4 BLEU lost due to grammatical errors
Link: https://arxiv.org/abs/1808.06267
====================================================
Lexicosyntactic Inference in Neural Models (Aaron Steven White - 19 August, 2018)
We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.
Link: https://arxiv.org/abs/1808.06232
====================================================
SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Taku Kudo - 19 August, 2018)
This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.
Link: https://arxiv.org/abs/1808.06226
====================================================
Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts (Di Jin - 19 August, 2018)
Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.
Link: https://arxiv.org/abs/1808.06161
====================================================
A Recipe for Arabic-English Neural Machine Translation (Abdullah Alrajeh - 18 August, 2018)
We compare neural systems with traditional phrase-based systems using various parallel corpora including UN, ISI and Ummah. The presented results are based on test sets from NIST MT 2005 and 2012. The best neural system produces a gain of +13 BLEU points compared to an equivalent simple phrase-based system in NIST MT12 test set. Unexpectedly, we find that tuning a model trained on the whole data using a small high quality corpus like Ummah gives a substantial improvement (+3 BLEU points)
Link: https://arxiv.org/abs/1808.06116
====================================================
CellLineNet: End-to-End Learning and Transfer Learning For Multiclass Epithelial Breast cell Line Classification via a Convolutional Neural Network (Darlington Ahiale Akogo - 18 August, 2018)
We developed a Convolutional Neural Network that classifies 5 types of epithelial breast cell lines comprised of two human cancer lines, 2 normal immortalized lines, and 1 immortalized mouse line (MDA-MB-468, MCF7, 10A, 12A and HC11) without requiring feature extraction. CellLineNet is 31-layer Convolutional Neural Network trained, validated and tested on a 3,252 image dataset of 5 types of Epithelial Breast cell Lines (MDA-MB-468, MCF7, 10A, 12A and HC11) in an end-to-end fashion. CellLineNet simply requires an imaged Cell Line as input and it outputs the type of breast epithelial cell line (MDA-MB-468, MCF7, 10A, 12A or HC11) as predicted probabilities for the 5 classes. CellLineNet scored a 96.67% Accuracy.
Link: https://arxiv.org/abs/1808.06041
====================================================
Concept Mask: Large-Scale Segmentation from Semantic Concepts (Yufei Wang - 17 August, 2018)
We first train a deep neural network on a 6M stock image dataset with only image-level labels to learn visual-semantic embedding on 18K concepts. Then, we refine and extend the embedding network to predict an attention map, using a curated dataset with bounding box annotations on 750 concepts
Link: https://arxiv.org/abs/1808.06032
====================================================
First Steps Toward CNN based Source Classification of Document Images Shared Over Messaging App (Sharad Joshi - 17 August, 2018)
In this letter, we investigate a convolutional neural network-based approach to solve source smartphone identification problem for printed text documents which have been captured by smartphone cameras and shared over messaging platform. In absence of any publicly available dataset addressing this problem, we introduce a new image dataset consisting of 315 images of documents printed in three different fonts, captured using 21 smartphones and shared over WhatsApp
Link: https://arxiv.org/abs/1808.05941
====================================================
Read + Verify: Machine Reading Comprehension with Unanswerable Questions (Minghao Hu - 5 September, 2018)
To address this problem, we propose a novel read-then-verify system, which not only utilizes a neural reader to extract candidate answers and produce no-answer probabilities, but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets. Our experiments on the SQuAD 2.0 dataset show that our system achieves a score of 74.2 F1 on the test set, outperforming all previous approaches at the time of submission (Aug
Link: https://arxiv.org/abs/1808.05759
====================================================
Auto-Classification of Retinal Diseases in the Limit of Sparse Data Using a Two-Streams Machine Learning Model (C. -H. Huck Yang - 4 September, 2018)
Based on the fact that fundus structure and vascular disorders are the main characteristics of retinal diseases, we propose a novel visual-assisted diagnosis hybrid model mixing the support vector machine (SVM) and deep neural networks (DNNs). Furthermore, we present a new clinical retina dataset, called EyeNet2, for ophthalmology incorporating 52 retina diseases classes. Using EyeNet2, our model achieves 90.43\% diagnosis accuracy, and the model performance is comparable to the professional ophthalmologists.
Link: https://arxiv.org/abs/1808.05754
====================================================
Dynamic Routing on Deep Neural Network for Thoracic Disease Classification and Sensitive Area Localization (Yan Shen - 17 August, 2018)
Deep neural networks have shown great success in a plethora of visual recognition tasks such as image classification and object detection by stacking multiple layers of convolutional neural networks (CNN) in a feed-forward manner. We demonstrate our results on the NIH chestX-ray14 dataset that consists of 112,120 images on 30,805 unique patients including 14 kinds of lung diseases.
Link: https://arxiv.org/abs/1808.05744
====================================================
Efficient Single-Shot Multibox Detector for Construction Site Monitoring (Viral Thakar - 19 August, 2018)
Asset monitoring in construction sites is an intricate, manually intensive task, that can highly benefit from automated solutions engineered using deep neural networks. In our attempts, we have been able to improve the mean average precision of SSD by 3.77% on custom dataset consist of images from construction sites and by 1.67% on PASCAL VOC Challenge.
Link: https://arxiv.org/abs/1808.05730
====================================================
BlockQNN: Efficient Block-wise Neural Network Architecture Generation (Zhao Zhong - 16 August, 2018)
Convolutional neural networks have gained a remarkable success in computer vision. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35% top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. The best network achieves very competitive accuracy of 82.0% top-1 and 96.0% top-5 on ImageNet.
Link: https://arxiv.org/abs/1808.05584
====================================================
LARNN: Linear Attention Recurrent Neural Network (Guillaume Chevalier - 16 August, 2018)
This neural architecture yields better results than the vanilla LSTM cells. It can obtain results of 91.92% for the test accuracy, compared to the previously attained 91.65% using vanilla LSTM cells. Note that this is not to compare to other research, where up to 93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3 cells as analyzed here
Link: https://arxiv.org/abs/1808.05578
====================================================
Anatomy Of High-Performance Deep Learning Convolutions On SIMD Architectures (Evangelos Georganas - 20 August, 2018)
Convolution layers are prevalent in many classes of deep neural networks, including Convolutional Neural Networks (CNNs) which provide state-of-the-art results for tasks like image recognition, neural machine translation and speech recognition. In this paper, we introduce direct convolution kernels for x86 architectures, in particular for Xeon and XeonPhi systems, which are implemented via a dynamic compilation approach
Link: https://arxiv.org/abs/1808.05567
====================================================
Overview of the CLEF-2018 CheckThat! Lab on Automatic Identification and Verification of Political Claims. Task 1: Check-Worthiness (Pepa Atanasova - 8 August, 2018)
The most successful approaches used by the participants relied on recurrent and multi-layer neural networks, as well as on combinations of distributional representations, on matchings claims' vocabulary against lexicons, and on measures of syntactic dependency. The best systems achieved mean average precision of 0.18 and 0.15 on the English and on the Arabic test datasets, respectively
Link: https://arxiv.org/abs/1808.05542
====================================================
An Experimental Evaluation of Covariates Effects on Unconstrained Face Verification (Boyu Lu - 16 August, 2018)
In this paper, we comprehensively study two covariate related problems for unconstrained face verification: first, how covariates affect the performance of deep neural networks on the large-scale unconstrained face verification problem; second, how to utilize covariates to improve verification performance. After retraining the face recognition model using the curated data, performance improvement is observed at low False Acceptance Rates (FARs) (FAR=$10^{-5}$, $10^{-6}$, $10^{-7}$).
Link: https://arxiv.org/abs/1808.05508
====================================================
CBinfer: Exploiting Frame-to-Frame Locality for Faster Convolutional Network Inference on Video Streams (Lukas Cavigelli - 15 August, 2018)
The last few years have brought advances in computer vision at an amazing pace, grounded on new findings in deep neural network construction and training as well as the availability of large labeled datasets. This optimized inference procedure resulted in an average speed-up of 9.1x over cuDNN on the Tegra X2 platform at a negligible accuracy loss of <0.1% and no retraining of the network for a semantic segmentation application. Similarly, an average speed-up of 7.0x has been achieved for a pose detection DNN on static camera video surveillance data. These throughput gains combined with a lower power consumption result in an energy efficiency of 511 GOp/s/W compared to 70 GOp/s/W for the baseline.
Link: https://arxiv.org/abs/1808.05488
====================================================
Transfer Learning for Brain-Computer Interfaces: An Euclidean Space Data Alignment Approach (He He - 8 August, 2018)
Almost all EEG-based brain-computer interfaces (BCIs) need some labeled subject-specific data to calibrate a new subject, as neural responses are different across subjects to even the same stimulus. It has three desirable properties: 1) the aligned trial lie in the Euclidean space, which can be used by any Euclidean space signal processing and machine learning approach; 2) it can be computed very efficiently; and, 3) it does not need any labeled trials from the new subject
Link: https://arxiv.org/abs/1808.05464
====================================================
Egocentric Gesture Recognition for Head-Mounted AR devices (Tejo Chalasani - 16 August, 2018)
In this work we propose a data driven end-to-end deep learning approach to address the problem of egocentric gesture recognition, which combines an ego-hand encoder network to find ego-hand features, and a recurrent neural network to discern temporally discriminating features. In addition we publish a dataset of 10 gestures performed in a natural fashion in front of a green screen for training and the same 10 gestures performed in different natural scenes without green screen for validation
Link: https://arxiv.org/abs/1808.05380
====================================================
Neural Networks Assist Crowd Predictions in Discerning the Veracity of Emotional Expressions (Zhenyue Qin - 16 August, 2018)
Neural networks can achieve an accuracy to 99.69% by aggregating participants' answers. We find that our neural networks do not require a large number of participants, particularly, 30 randomly selected, to achieve high accuracy predictions, better than any individual participant
Link: https://arxiv.org/abs/1808.05359
====================================================
Tool Breakage Detection using Deep Learning (Guang Li - 16 August, 2018)
In addition, we use a BP neural network to understand the reliability of the CNN. The results show that our CNN approach can detect tool breakage with an accuracy of 93%, while the best performance of BP is 80%.
Link: https://arxiv.org/abs/1808.05347
====================================================
DNN Feature Map Compression using Learned Representation over GF(2) (Denis A. Gudovskiy - 15 August, 2018)
In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Compared to prior approaches, the conducted experiments show a factor of 2 decrease in memory requirements with minor degradation in accuracy while adding only bitwise computations.
Link: https://arxiv.org/abs/1808.05285
====================================================
Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks (Penghang Yin - 29 August, 2018)
We introduce the notion of coarse derivative and propose the blended coarse gradient descent (BCGD) algorithm, for training fully quantized neural networks. In full quantization of ResNet-18 for ImageNet classification task, BCGD gives 64.36% top-1 accuracy with binary weights across all layers and 4-bit adaptive activation. If the weights in the first and last layers are kept in full precision, this number increases to 65.46%
Link: https://arxiv.org/abs/1808.05240
====================================================
Deep Learning using K-space Based Data Augmentation for Automated Cardiac MR Motion Artefact Detection (Ilkay Oksuz - 31 August, 2018)
Our method is based on 3D spatio-temporal Convolutional Neural Networks, and is able to detect 2D+time short axis images with motion artefacts in less than 1ms. We test our algorithm on a subset of the UK Biobank dataset consisting of 3465 CMR images and achieve not only high accuracy in detection of motion artefacts, but also high precision and recall
Link: https://arxiv.org/abs/1808.05130
====================================================
Deep RTS: A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games (Per-Arne Andersen - 15 August, 2018)
This success is primarily due to the vast capabilities of convolutional neural networks, that can extract useful features from noisy and complex data. It supports accelerated learning, meaning that it can learn at a magnitude of 50 000 times faster compared to existing RTS games. Using Deep RTS, we show that a Deep Q-Network agent beats random-play agents over 70% of the time
Link: https://arxiv.org/abs/1808.05032
====================================================
Pairwise Relational Networks for Face Recognition (Bong-Nam Kang - 15 August, 2018)
Existing face recognition using deep neural networks is difficult to know what kind of features are used to discriminate the identities of face images clearly. Experimental results on the LFW show that the PRN using only pairwise relations achieved 99.65% accuracy and the PRN using both pairwise relations and face identity state feature achieved 99.76% accuracy. On the YTF, both the PRN using only pairwise relations and the PRN using pairwise relations and the face identity state feature achieved the state-of-the-art (95.7% and 96.3%)
Link: https://arxiv.org/abs/1808.04976
====================================================
Multiple Character Embeddings for Chinese Word Segmentation (Jingkang Wang - 1 October, 2018)
Chinese word segmentation (CWS) is often regarded as a character-based sequence labeling task in most current works which have achieved great performance by leveraging powerful neural networks. Specifically, we achieve the state-of-the-art performance in AS and CityU datasets with F1 scores 96.9 and 97.3, respectively without leveraging any external resources.
Link: https://arxiv.org/abs/1808.04963
====================================================
A framework for automatic question generation from text using deep reinforcement learning (Vishwajeet Kumar - 15 August, 2018)
Recent neural network-based approaches represent the state-of-the-art in this task, but they are not without shortcomings
Link: https://arxiv.org/abs/1808.04961
====================================================
Neural Collaborative Ranking (Bo Song - 14 August, 2018)
For example, a recent model, NeuMF, first projects users and items into some shared low-dimensional latent feature space, and then employs neural nets to model the interaction between the user and item latent features to obtain state-of-the-art performance on the recommendation tasks
Link: https://arxiv.org/abs/1808.04957
====================================================
Convolutional Neural Networks on 3D Surfaces Using Parallel Frames (Hao Pan - 14 August, 2018)
We extend Convolutional Neural Networks (CNNs) on flat and regular domains (e.g. We define surface convolution on tangent spaces of a surface domain, where the convolution has two desirable properties: 1) the distortion of surface domain signals is locally minimal when being projected to the tangent space, and 2) the translation equi-variance property holds locally, by aligning tangent spaces with the canonical parallel transport that preserves metric
Link: https://arxiv.org/abs/1808.04952
====================================================
Two Local Models for Neural Constituent Parsing (Zhiyang Teng - 28 August, 2018)
We investigate two conceptually simple local neural models for constituent parsing, which make local decisions to constituent spans and CFG rules, respectively. Consistent with previous findings along the line, our best model gives highly competitive results, achieving the labeled bracketing F1 scores of 92.4% on PTB and 87.3% on CTB 5.1.
Link: https://arxiv.org/abs/1808.04850
====================================================
KGCleaner : Identifying and Correcting Errors Produced by Information Extraction Systems (Ankur Padia - 15 August, 2018)
For credibility classification, parameter efficient simple shallow neural network can achieve an absolute performance gain of 30 $F_1$ points on Wikidata and comparable performance on TAC
Link: https://arxiv.org/abs/1808.04816
====================================================
A Survey on Methods and Theories of Quantized Neural Networks (Yunhui Guo - 13 August, 2018)
Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition
Link: https://arxiv.org/abs/1808.04752
====================================================
DeepNeuro: an open-source deep learning toolbox for neuroimaging (Andrew Beers - 14 August, 2018)
We show how this framework can be used to both design and train neural network architectures, as well as modify state-of-the-art architectures in a flexible and intuitive way
Link: https://arxiv.org/abs/1808.04589
====================================================
Automatic Airway Segmentation in chest CT using Convolutional Neural Networks (A. Garcia-Uceda Juarez - 14 August, 2018)
Recently, deep convolutional neural networks (CNNs) have become the state-of-the-art for many segmentation tasks, and in particular the so-called Unet architecture for biomedical images. The method is trained on a dataset composed of 12 CTs, and tested on another 6 CTs. We evaluate the influence of different loss functions and data augmentation techniques, and reach an average dice coefficient of 0.8 between the ground-truth and our automated segmentations.
Link: https://arxiv.org/abs/1808.04576
====================================================
Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding (Tianshui Chen - 13 August, 2018)
In this work, we investigate simultaneously predicting categories of different levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical Semantic Embedding (HSE) framework. To evaluate the proposed framework, we organize the 200 bird species of the Caltech-UCSD birds dataset with the four-level category hierarchy and construct a large-scale butterfly dataset that also covers four level categories
Link: https://arxiv.org/abs/1808.04505
====================================================
ScarGAN: Chained Generative Adversarial Networks to Simulate Pathological Tissue on Cardiovascular MR Scans (Felix Lau - 13 August, 2018)
Medical images with specific pathologies are scarce, but a large amount of data is usually required for a deep convolutional neural network (DCNN) to achieve good accuracy. Our novel approach factorizes the simulation process into 3 steps: 1) a mask generator to simulate the shape of the scar tissue; 2) a domain-specific heuristic to produce the initial simulated scar tissue from the simulated shape; 3) a refining generator to add details to the simulated scar tissue. Training a U-Net with additional scans with scar tissue simulated by ScarGAN increases the percentage of scar pixels correctly included in LV myocardium prediction from 75.9% to 80.5%.
Link: https://arxiv.org/abs/1808.04500
====================================================
Generative Invertible Networks (GIN): Pathophysiology-Interpretable Feature Mapping and Virtual Patient Generation (Jialei Chen - 13 August, 2018)
Combining a convolutional neural network (CNN) and generative adversarial networks (GAN), GIN discovers the pathophysiologic meaning of the feature space. Moreover, a test of predicting the surgical outcome directly using the selected features results in a high accuracy of 81.55%, which suggests little pathophysiologic information has been lost while conducting the feature selection
Link: https://arxiv.org/abs/1808.04495
====================================================
Fast Convergence for Object Detection by Learning how to Combine Error Functions (Benjamin Schnieders - 13 August, 2018)
In this paper, we introduce an innovative method to improve the convergence speed and accuracy of object detection neural networks. Our experiments show that adding an optimally weighted Euclidean distance loss to a network trained on the commonly used Intersection over Union (IoU) metric reduces the convergence time by 42.48%. The estimated pickup rate is improved by 39.90%. Compared to state-of-the-art task weighting methods, the improvement is 24.5% in convergence, and 15.8% on the estimated pickup rate.
Link: https://arxiv.org/abs/1808.04480
====================================================
Multimodal Deep Neural Networks using Both Engineered and Learned Representations for Biodegradability Prediction (Garrett B. Goh - 13 September, 2018)
In this work, we develop a novel multimodal CNN-MLP neural network architecture that utilizes both domain-specific feature engineering as well as learned representations from raw data. DeepBioD, a multimodal CNN-MLP network is more accurate than either standalone network designs, and achieves an error classification rate of 0.125 that is 27% lower than the current state-of-the-art
Link: https://arxiv.org/abs/1808.04456
====================================================
Smart Device based Initial Movement Detection of Cyclists using Convolutional Neuronal Networks (Jan Schneegans - 8 August, 2018)
Convolutional Neural Networks prove to be the state-of-the-art solution for many problems with an ever increasing range of applications
Link: https://arxiv.org/abs/1808.04451
====================================================
RoadNet-v2: A 10 ms Road Segmentation Using Spatial Sequence Layer (Yecheng Lyu - 10 August, 2018)
In this paper, we explore the usage of recurrent neural network (RNN) in image processing and propose an efficient network layer named spatial sequence. We claim the proposed network achieves comparable accuracy to the existing road segmentation algorithms but much faster processing speed, 10 ms per frame.
Link: https://arxiv.org/abs/1808.04450
====================================================
Out of the Black Box: Properties of deep neural networks and their applications (Nizar Ouarti - 10 August, 2018)
In this study, we formalize some properties shared by eight state-of-the-art deep neural networks in order to grasp the principles allowing a given deep neural network to classify an image. We obtain for the VGG-VDD-19 neural network a fooling ratio of 88\%
Link: https://arxiv.org/abs/1808.04433
====================================================
Murmur Detection Using Parallel Recurrent & Convolutional Neural Networks (Shahnawaz Alam - 13 August, 2018)
Set of acoustic features are presented to our proposed deep neural network to discriminate between Normal and Murmur class. The proposed method was evaluated on a large dataset using 5-fold cross-validation, resulting in a sensitivity and specificity of 96 +- 0.6 % , 100 +- 0 % respectively and F1 Score of 98 +- 0.3 %.
Link: https://arxiv.org/abs/1808.04411
====================================================
A Domain Guided CNN Architecture for Predicting Age from Structural Brain Images (Pascal Sturmfels - 11 August, 2018)
Given the wide success of convolutional neural networks (CNNs) applied to natural images, researchers have begun to apply them to neuroimaging data. Applied to the task of brain age prediction, our network achieves a mean absolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline that achieves a MAE of 1.6 years
Link: https://arxiv.org/abs/1808.04362
====================================================
RedSync : Reducing Synchronization Traffic for Distributed Deep Learning (Jiarui Fang - 13 August, 2018)
Data parallelism has already become a dominant method to scale Deep Neural Network (DNN) training to multiple computation nodes. Among several recent proposed compression algorithms, Residual Gradient Compression (RGC) is one of the most successful approaches---it can significantly compress the message size (0.1% of the original size) and still preserve accuracy
Link: https://arxiv.org/abs/1808.04357
====================================================
Design Flow of Accelerating Hybrid Extremely Low Bit-width Neural Network in Embedded FPGA (Junsong Wang - 31 July, 2018)
Results show that our design can deliver very high performance peaking at 10.3 TOPS and classify up to 325.3 image/s/watt while running large-scale neural networks for less than 5W using embedded FPGA
Link: https://arxiv.org/abs/1808.04311
====================================================
iNNvestigate neural networks! (Maximilian Alber - 13 August, 2018)
To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.
Link: https://arxiv.org/abs/1808.04260
====================================================
Rapid Adaptation of Neural Machine Translation to New Languages (Graham Neubig - 13 August, 2018)
This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings
Link: https://arxiv.org/abs/1808.04189
====================================================
Parsimonious HMMs for Offline Handwritten Chinese Text Recognition (Wenchao Wang - 13 August, 2018)
The proposed parsimonious HMMs with both Gaussian mixture models (GMMs) and deep neural networks (DNNs) as the emission distributions not only lead to a compact model but also improve the recognition accuracy via the data sharing for the tied states and the confusion decreasing among state classes. Tested on ICDAR-2013 competition database, in the best configured case, the new parsimonious DNN-HMM can yield a relative character error rate (CER) reduction of 6.2%, 25% reduction of model size and 60% reduction of decoding time over the conventional DNN-HMM. In the compact setting case of average 1-state HMM, our parsimonious DNN-HMM significantly outperforms the conventional DNN-HMM with a relative CER reduction of 35.5%.
Link: https://arxiv.org/abs/1808.04138
====================================================
DenseRAN for Offline Handwritten Chinese Character Recognition (Wenchao Wang - 13 August, 2018)
Then a decoder based on recurrent neural networks is employed, aiming at generating captions of Chinese characters by detecting radicals and two-dimensional structures through attention mechanism. Evaluated on ICDAR-2013 competition database, the proposed approach significantly outperforms whole-character modeling approach with a relative character error rate (CER) reduction of 18.54%. Meanwhile, for the case of recognizing 3277 unseen Chinese characters in CASIA-HWDB1.2 database, DenseRAN can achieve a character accuracy of about 41% while the traditional whole-character method has no capability to handle them.
Link: https://arxiv.org/abs/1808.04134
====================================================
Confidence penalty, annealing Gaussian noise and zoneout for biLSTM-CRF networks for named entity recognition (Antonio Jimeno Yepes - 13 August, 2018)
A bidirectional LSTM (long short term memory) encoder with a neural conditional random fields (CRF) decoder (biLSTM-CRF) is the state of the art methodology. Results show that the optimization methods improve the performance of the biLSTM-CRF NER baseline system, setting a new state of the art performance for the CoNLL-2003 Spanish set with an F1 of 87.18.
Link: https://arxiv.org/abs/1808.04029
====================================================
Sequence Labeling: A Practical Approach (Adnan Akhundov - 12 August, 2018)
To this end, we utilize a universal end-to-end Bi-LSTM-based neural sequence labeling model applicable to a wide range of NLP tasks and languages. We observe state-of-the-art results on four of them: CoNLL-2012 (English NER), CoNLL-2002 (Dutch NER), GermEval 2014 (German NER), Tiger Corpus (German POS-tagging), and competitive performance on the rest.
Link: https://arxiv.org/abs/1808.03926
====================================================
Sample Mixed-Based Data Augmentation for Domestic Audio Tagging (Shengyun Wei - 11 August, 2018)
We apply a convolutional recurrent neural network (CRNN) with attention module with log-scaled mel spectrum as a baseline system. In our experiments, we achieve an state-of-the-art of equal error rate (EER) of 0.10 on DCASE 2016 task4 dataset with mixup approach, outperforming the baseline system without data augmentation.
Link: https://arxiv.org/abs/1808.03883
====================================================
Self-Supervised Model Adaptation for Multimodal Semantic Segmentation (Abhinav Valada - 11 August, 2018)
Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on relevant complementary information for fusion. In addition, we propose a computationally efficient unimodal segmentation architecture termed AdapNet++ that incorporates a new encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a larger effective receptive field with more than 10x fewer parameters, complemented with a strong decoder with a multi-resolution supervision scheme that recovers high-resolution details
Link: https://arxiv.org/abs/1808.03833
====================================================
Automatically Designing CNN Architectures Using Genetic Algorithm for Image Classification (Yanan Sun - 11 August, 2018)
Convolutional Neural Networks (CNNs) have gained a remarkable success on many real-world problems in recent years. Furthermore, the proposed algorithm also shows the competitive classification accuracy to the semi-automatic peer competitors, while reducing 10 times of the parameters
Link: https://arxiv.org/abs/1808.03818
====================================================
A Full End-to-End Semantic Role Labeler, Syntax-agnostic Over Syntax-aware? (Jiaxun Cai - 13 August, 2018)
For the first time, this paper introduces an end-to-end neural model which unifiedly tackles the predicate disambiguation and the argument labeling in one shot. Though the proposed model is syntax-agnostic with local decoder, it outperforms the state-of-the-art syntax-aware SRL systems on the CoNLL-2008, 2009 benchmarks for both English and Chinese
Link: https://arxiv.org/abs/1808.03815
====================================================
Document Informed Neural Autoregressive Topic Models (Pankaj Gupta - 11 August, 2018)
Here, we extend a neural autoregressive topic model to exploit the full context information around words in a document in a language modeling fashion. With the learned representations, we show on an average a gain of 9.6% (0.57 Vs 0.52) in precision at retrieval fraction 0.02 and 7.2% (0.582 Vs 0.543) in F1 for text categorization.
Link: https://arxiv.org/abs/1808.03793
====================================================
From POS tagging to dependency parsing for biomedical event extraction (Dat Quoc Nguyen - 10 August, 2018)
In this paper, we perform an empirical study comparing state-of-the-art traditional feature-based and neural network-based models for two core NLP tasks of POS tagging and dependency parsing on two benchmark biomedical corpora, GENIA and CRAFT
Link: https://arxiv.org/abs/1808.03731
====================================================
Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning (Noah Frazier-Logue - 10 August, 2018)
Multi-layer neural networks have lead to remarkable performance on many kinds of benchmark tasks in text, speech and image processing. One approach to this overfitting and related problems (local minima, colinearity, feature discovery etc.) is called dropout (Srivastava, et al 2014, Baldi et al 2016). In this paper we will show that Dropout is a special case of a more general model published originally in 1990 called the stochastic delta rule ( SDR, Hanson, 1990). Tests on standard benchmarks (CIFAR) using a modified version of DenseNet shows the SDR outperforms standard dropout in error by over 50% and in loss by over 50%. Furthermore, the SDR implementation converges on a solution much faster, reaching a training error of 5 in just 15 epochs with DenseNet-40 compared to standard DenseNet-40's 94 epochs.
Link: https://arxiv.org/abs/1808.03578
====================================================
Densely Connected Convolutional Networks for Speech Recognition (Chia Yu Li - 10 August, 2018)
DenseN-ets are very deep, compact convolutional neural networks, which have demonstrated incredible improvements over the state-of-the-art results on several data sets in computer vision
Link: https://arxiv.org/abs/1808.03570
====================================================
Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors in Deep Learning (Adam A. Kohan - 9 August, 2018)
We show experimentally that recurrent neural networks with two and three hidden layers can be trained using Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving $1.90\%$ and $11\%$ generalization errors respectively.
Link: https://arxiv.org/abs/1808.03357
====================================================
3D Shape Perception from Monocular Vision, Touch, and Shape Priors (Shaoxiong Wang - 9 August, 2018)
We use vision first, applying neural networks with learned shape priors to predict an object's 3D shape from a single-view color image. Our method efficiently builds the 3D shape of common objects from a color image and a small number of tactile explorations (around 10)
Link: https://arxiv.org/abs/1808.03247
====================================================
Building a Kannada POS Tagger Using Machine Learning and Neural Network Models (Ketan Kumar Todi - 9 August, 2018)
We present a statistical POS tagger for Kannada using different machine learning and neural network models. Our Kannada POS tagger outperforms the state-of-the-art Kannada POS tagger by 6%
Link: https://arxiv.org/abs/1808.03175
====================================================
Image Inspired Poetry Generation in XiaoIce (Wen-Feng Cheng - 9 August, 2018)
Finally, verses are generated gradually from the keywords using recurrent neural networks trained on existing poems. By deploying our proposed approach, XiaoIce has already generated more than 12 million poems for users since its release in July 2017
Link: https://arxiv.org/abs/1808.03090
====================================================
Controllable Image-to-Video Translation: A Case Study on Facial Expression Generation (Lijie Fan - 8 August, 2018)
To this end, we design a novel neural network architecture that can incorporate the user input into its skip connections and propose several improvements to the adversarial training method for the neural network. Especially, we would like to highlight that even for the face images in the wild (downloaded from the Web and the authors' own photos), our model can generate high-quality facial expression videos of which about 50\% are labeled as real by Amazon Mechanical Turk workers.
Link: https://arxiv.org/abs/1808.02992
====================================================
An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets (Valentin Vielzeuf - 8 August, 2018)
This paper presents a light-weight and accurate deep neural model for audiovisual emotion recognition. We also highlight the inherent challenges of the AFEW dataset and the difficulty of model selection with as few as 383 validation  equences. The proposed real-time emotion classifier achieved a state-of-the-art accuracy of 60.64 % on the test set of AFEW, and ranked 4th at  he Emotion in the Wild 2018 challenge.
Link: https://arxiv.org/abs/1808.02668
====================================================
Question-Guided Hybrid Convolution for Visual Question Answering (Peng Gao - 8 August, 2018)
Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features.To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage
Link: https://arxiv.org/abs/1808.02632
====================================================
SchiNet: Automatic Estimation of Symptoms of Schizophrenia from Facial Behaviour Analysis (Mina Bishay - 7 August, 2018)
We automatically analyse the facial behaviour of 91 out-patients - this is almost 3 times the number of patients in other studies - and propose SchiNet, a novel neural network architecture that estimates expression-related symptoms in two different assessment interviews
Link: https://arxiv.org/abs/1808.02531
====================================================
Device-directed Utterance Detection (Sri Harish Mallidi - 7 August, 2018)
A feed-forward deep neural network (DNN) is then trained to combine the acoustic and 1-best embeddings, derived from the LSTMs, with features from the ASR decoder. Experimental results show that ASR decoder, acoustic embeddings, and 1-best embeddings yield an equal-error-rate (EER) of $9.3~\%$, $10.9~\%$ and $20.1~\%$, respectively. Combination of the features resulted in a $44~\%$ relative improvement and a final EER of $5.2~\%$.
Link: https://arxiv.org/abs/1808.02504
====================================================
Spinal Cord Gray Matter-White Matter Segmentation on Magnetic Resonance AMIRA Images with MD-GRU (Antal Horvath - 7 August, 2018)
We use a recurrent neural network (RNN) with multi-dimensional gated recurrent units (MD-GRU) to train segmentation models on the AMIRA dataset of 855 slices
Link: https://arxiv.org/abs/1808.02408
====================================================
Motorcycle detection and classification in urban Scenarios using a model based on Faster R-CNN (Jorge E. Espinosa - 8 August, 2018)
This paper introduces a Deep Learning Convolutional Neural Network model based on Faster-RCNN for motorcycle detection and classification on urban environments. The model is evaluated in occluded scenarios where more than 60% of the vehicles present a degree of occlusion. For training and evaluation, we introduce a new dataset of 7500 annotated images, captured under real traffic scenes, using a drone mounted camera. Several tests were carried out to design the network, achieving promising results of 75% in average precision (AP), even with the high number of occluded motorbikes, the low angle of capture and the moving camera. The model is also evaluated on low occlusions datasets, reaching results of up to 92% in AP.
Link: https://arxiv.org/abs/1808.02299
====================================================
Efficient Fusion of Sparse and Complementary Convolutions (Chun-Fu Chen - 10 September, 2018)
We propose a new method to create compact convolutional neural networks (CNNs) by exploiting sparse convolutions. For object detection, our approach leads to a VGG-16-based Faster RCNN detector that is 12.4$\times$ smaller and about 3$\times$ faster than the baseline.
Link: https://arxiv.org/abs/1808.02167
====================================================
Did you take the pill? - Detecting Personal Intake of Medicine from Twitter (Debanjan Mahata - 2 August, 2018)
We train a stacked ensemble of shallow convolutional neural network (CNN) models on an annotated dataset. Our system produces state-of-the-art result, with a micro-averaged F-score of 0.693
Link: https://arxiv.org/abs/1808.02082
====================================================
Deep Reinforcement Learning for Online Offloading in Wireless Powered Mobile-Edge Computing Networks (Liang Huang - 6 August, 2018)
To tackle this problem, we propose in this paper a Deep Reinforcement learning-based Online Offloading (DROO) framework that implements a deep neural network to generate offloading decisions. For example, the complexity is reduced from several seconds to less than $0.1$ second in a $30$-user network, making real-time and optimal offloading design truly viable even in a fast fading environment.
Link: https://arxiv.org/abs/1808.01977
====================================================
Adversarial Vision Challenge (Wieland Brendel - 6 August, 2018)
This document is an updated version of our competition proposal that was accepted in the competition track of 32nd Conference on Neural Information Processing Systems (NIPS 2018).
Link: https://arxiv.org/abs/1808.01976
====================================================
Audio Tagging With Connectionist Temporal Classification Model Using Sequential Labelled Data (Yuanbo Hou - 6 August, 2018)
To utilize SLD in audio tagging, we propose a Convolutional Recurrent Neural Network followed by a Connectionist Temporal Classification (CRNN-CTC) objective function to map from an audio clip spectrogram to SLD. Experiments show that CRNN-CTC obtains an Area Under Curve (AUC) score of 0.986 in audio tagging, outperforming the baseline CRNN of 0.908 and 0.815 with Max Pooling and Average Pooling, respectively
Link: https://arxiv.org/abs/1808.01935
====================================================
Residual Memory Networks: Feed-forward approach to learn long temporal dependencies (Murali Karthick Baskar - 6 August, 2018)
In this paper we propose a residual memory neural network (RMN) architecture to model short-time dependencies using deep feed-forward layers having residual and time delayed connections. Recognition performance of RMN trained with 300 hours of Switchboard corpus is compared with various state-of-the-art LVCSR systems. The results indicate that RMN and BRMN gains 6 % and 3.8 % relative improvement over LSTM and BLSTM networks.
Link: https://arxiv.org/abs/1808.01916
====================================================
Defense Against Adversarial Attacks with Saak Transform (Sibo Song - 6 August, 2018)
Deep neural networks (DNNs) are known to be vulnerable to adversarial perturbations, which imposes a serious threat to DNN-based decision systems. Therefore, we propose a Saak transform based preprocessing method with three steps: 1) transforming an input image to a joint spatial-spectral representation via the forward Saak transform, 2) apply filtering to its high-frequency components, and, 3) reconstructing the image via the inverse Saak transform
Link: https://arxiv.org/abs/1808.01785
====================================================
Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the Robustness of 18 Deep Image Classification Models (Dong Su - 5 August, 2018)
However, recent studies have highlighted the lack of robustness in well-trained deep neural networks to adversarial examples. To demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 ImageNet models using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models
Link: https://arxiv.org/abs/1808.01688
====================================================
Missing Value Imputation Based on Deep Generative Models (Hongbao Zhang - 5 August, 2018)
These distributions are parameterized by deep neural networks (DNNs) which possess high approximation power and can capture the nonlinear relationships between missing entries and the observed values. We conducted extensive evaluation on 13 datasets and compared with 11 baselines methods, where our methods largely outperforms the baselines.
Link: https://arxiv.org/abs/1808.01684
====================================================
A Multi-task Framework for Skin Lesion Detection and Segmentation (Sulaiman Vesal - 5 August, 2018)
A `Faster region-based convolutional neural network' (Faster-RCNN) which comprises a region proposal network (RPN), is used to generate bounding boxes/region proposals, for lesion localization in each image. We trained and evaluated the performance of our network, using the ISBI 2017 challenge and the PH2 datasets, and compared it with the state-of-the-art, using the official test data released as part of the challenge for the former. Our approach outperformed others in terms of Dice coefficients ($>0.93$), Jaccard index ($>0.88$), accuracy ($>0.96$) and sensitivity ($>0.95$), across five-fold cross validation experiments.
Link: https://arxiv.org/abs/1808.01676
====================================================
Dilated Convolutions in Neural Networks for Left Atrial Segmentation in 3D Gadolinium Enhanced-MRI (Sulaiman Vesal - 5 August, 2018)
Recently, deep convolutional neural networks (CNNs) have gained tremendous traction and achieved state-of-the-art results in medical image segmentation
Link: https://arxiv.org/abs/1808.01673
====================================================
Combining Graph-based Dependency Features with Convolutional Neural Network for Answer Triggering (Deepak Gupta - 5 August, 2018)
In this paper, we present a hybrid deep learning model for answer triggering, which combines several dependency graph based alignment features, namely graph edit distance, graph-based similarity and dependency graph coverage, with dense vector embeddings from a Convolutional Neural Network (CNN). Comparative study on WikiQA dataset shows 5.86% absolute F-score improvement at the question level.
Link: https://arxiv.org/abs/1808.01650
====================================================
Classification of Dermoscopy Images using Deep Learning (Nithin D Reddy - 5 August, 2018)
We trained a convolutional neural network based on the ResNet50 architecture to accurately classify dermoscopy images of skin lesions into one of seven disease categories. Using our custom model, we obtained a balanced accuracy of 91% on the validation dataset.
Link: https://arxiv.org/abs/1808.01607
====================================================
LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation (Pankaj Gupta - 5 August, 2018)
Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.
Link: https://arxiv.org/abs/1808.01591
====================================================
Designing Adaptive Neural Networks for Energy-Constrained Image Classification (Dimitrios Stamoulis - 6 August, 2018)
As convolutional neural networks (CNNs) enable state-of-the-art computer vision applications, their high energy consumption has emerged as a key impediment to their deployment on embedded and mobile devices
Link: https://arxiv.org/abs/1808.01550
====================================================
Adversarial Examples: Attacks on Machine Learning-based Malware Visualization Detection Methods (Xinbo Liu - 4 August, 2018)
As the threat of malicious software (malware) becomes urgently serious, automatic malware detection techniques have received increasing attention recently, where the machine learning (ML)-based visualization detection plays a significant role.However, this leads to a fundamental problem whether such detection methods can be robust enough against various potential attacks.Even though ML algorithms show superiority to conventional ones in malware detection in terms of high efficiency and accuracy, this paper demonstrates that such ML-based malware detection methods are vulnerable to adversarial examples (AE) attacks.We propose the first AE-based attack framework, named Adversarial Texture Malware Perturbation Attacks (ATMPA), based on the gradient descent or L-norm optimization method.By introducing tiny perturbations on the transformed dataset, ML-based malware detection methods completely fail.The experimental results on the MS BIG malware dataset show that a small interference can reduce the detection rate of convolutional neural network (CNN), support vector machine (SVM) and random forest(RF)-based malware detectors down to 0 and the attack transferability can achieve up to 88.7% and 74.1% on average in different ML-based detection methods.
Link: https://arxiv.org/abs/1808.01546
====================================================
Learning Multi-scale Features for Foreground Segmentation (Long Ang Lim - 4 August, 2018)
In this work, we propose a novel robust encoder-decoder structure neural network that can be trained end-to-end using only a few training examples. Our method outperforms all existing state-of-the-art methods in CDnet2014 dataset by an average overall F-Measure of 0.9847
Link: https://arxiv.org/abs/1808.01477
====================================================
code2seq: Generating Sequences from Structured Representations of Code (Uri Alon - 10 October, 2018)
Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to $16$M examples
Link: https://arxiv.org/abs/1808.01400
====================================================
Large Scale Language Modeling: Converging on 40GB of Text in Four Hours (Raul Puri - 10 August, 2018)
Following [Radford 2017], in this work, we demonstrate similar scalability and transfer for Recurrent Neural Networks (RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and a 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to train a character-level 4096-dimension multiplicative LSTM (mLSTM) for unsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews dataset in four hours. Since our model converges over the Amazon Reviews dataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while substantial, is commercially available, this work opens up large scale unsupervised NLP training to most commercial applications and deep learning researchers
Link: https://arxiv.org/abs/1808.01371
====================================================
A Deep Learning based Joint Segmentation and Classification Framework for Glaucoma Assesment in Retinal Color Fundus Images (Arunava Chakravarty - 29 July, 2018)
In this work, we present a Multi-task Convolutional Neural Network (CNN) that jointly segments the Optic Disc (OD), Optic Cup (OC) and predicts the presence of glaucoma in color fundus images. The cross-testing performance of the proposed method on an independent validation set acquired using a different camera and image resolution was found to be good with an average dice score of 0.92 for OD, 0.84 for OC and AUC of 0.95 on the task of glaucoma classification illustrating its potential as a mass screening tool for the early detection of glaucoma.
Link: https://arxiv.org/abs/1808.01355
====================================================
CornerNet: Detecting Objects as Paired Keypoints (Hei Law - 3 August, 2018)
We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. Experiments show that CornerNet achieves a 42.1% AP on MS COCO, outperforming all existing one-stage detectors.
Link: https://arxiv.org/abs/1808.01244
====================================================
Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions (Konda Reddy Mopuri - 3 August, 2018)
Experimental evaluation demonstrates that the learned generative model, (i) readily crafts UAPs via simple feed-forwarding through neural network layers, and (ii) achieves state-of-the-art success rates for data-free scenario and closer to that for data-driven setting without actually utilizing any data samples.
Link: https://arxiv.org/abs/1808.01153
====================================================
Integrated System for Malicious Node Discovery and Self-destruction in Wireless Sensor Networks (Madalin Plastoi - 3 August, 2018)
Basically, we will compare every sensor reading with its estimated values provided by two predictors: an autoregressive predictor [1] that uses past values provided by the sensor under investigation and a neural predictor that uses past values provided by adjacent nodes
Link: https://arxiv.org/abs/1808.01108
====================================================
CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images (Sheng Guo - 19 September, 2018)
We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled raw from the Internet by using text queries, without any human annotation. With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on the WebVision challenge for 1000-category classification. This result was the top performance by a wide margin, outperforming second place by a nearly 50% relative error rate
Link: https://arxiv.org/abs/1808.01097
====================================================
Online Illumination Invariant Moving Object Detection by Generative Neural Network (Fateme Bahri - 2 August, 2018)
In this paper, we propose an extension of a state-of-the-art batch MOD method (ILISD) to an online/incremental MOD using unsupervised and generative neural networks, which use illumination invariant image representations
Link: https://arxiv.org/abs/1808.01066
====================================================
Impacts of Weather Conditions on District Heat System (Jiyang Xie - 2 August, 2018)
Based on an Elman neural network (ENN), this paper investigates the impact of direct solar irradiance and wind speed on predicting the heat demand of a district heating network. Results show that including wind speed can generally result in a lower overall mean absolute percentage error (MAPE) (6.43%) than including direct solar irradiance (6.47%); while including direct solar irradiance can achieve a lower maximum absolute deviation (71.8%) than including wind speed (81.53%)
Link: https://arxiv.org/abs/1808.00961
====================================================
SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait Detection (Vaibhav Kumar - 2 August, 2018)
Finally, this representation is passed through a neural network to obtain a score for the headline. We test our model over 2538 posts (having trained it on 17000 records) and achieve an accuracy of 83.49% outscoring previous state-of-the-art approaches.
Link: https://arxiv.org/abs/1808.00957
====================================================
Weakly Supervised Localisation for Fetal Ultrasound Images (Nicolas Toussaint - 2 August, 2018)
We examine the use of convolutional neural network architectures coupled with soft proposal layers. Detection achieves an average accuracy of 90\% on individual regions, and show that the proposal maps correlate well with relevant anatomical structures
Link: https://arxiv.org/abs/1808.00793
====================================================
DCASE 2018 Challenge Surrey Cross-Task convolutional neural network baseline (Qiuqiang Kong - 29 September, 2018)
In this paper, we create a cross-task baseline system for all five tasks based on a convlutional neural network (CNN): a "CNN Baseline" system. We implemented CNNs with 4 layers and 8 layers originating from AlexNet and VGG from computer vision. Experiments show that deeper CNN with 8 layers performs better than CNN with 4 layers on all tasks except Task 1. Using CNN with 8 layers, we achieve an accuracy of 0.680 on Task 1, an accuracy of 0.895 and a mean average precision (MAP) of 0.928 on Task 2, an accuracy of 0.751 and an area under the curve (AUC) of 0.854 on Task 3, a sound event detection F1 score of 20.8% on Task 4, and an F1 score of 87.75% on Task 5
Link: https://arxiv.org/abs/1808.00773
====================================================
Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation (Maximilian Jaritz - 31 August, 2018)
Convolutional neural networks are designed for dense data, but vision data is often sparse (stereo depth, point clouds, pen stroke, etc.). Our method even works with densities as low as 0.8% (8 layer lidar), and outperforms all published state-of-the-art on the Kitti depth completion benchmark.
Link: https://arxiv.org/abs/1808.00769
====================================================
Deeply Self-Supervising Edge-to-Contour Neural Network Applied to Liver Segmentation (Minyoung Chung - 6 August, 2018)
To guide a neural network to accurately delineate the target liver object, we apply self-supervising scheme with respect to edge and contour responses. We used 160 abdominal CT images for training and validation. The result showed that our method successfully segmented liver more accurately than any other state-of-the-art methods without expanding or deepening the neural network
Link: https://arxiv.org/abs/1808.00739
====================================================
Binary Weighted Memristive Analog Deep Neural Network for Near-Sensor Edge Processing (Olga Krestinskaya - 2 August, 2018)
In this work, we propose the design of an analog deep neural network with binary weight update through backpropagation algorithm using binary state memristive devices. The proposed network was benchmarked for MNIST handwritten digits recognition achieving an accuracy of approximately 90%.
Link: https://arxiv.org/abs/1808.00737
====================================================
Approximate Probabilistic Neural Networks with Gated Threshold Logic (Olga Krestinskaya - 2 August, 2018)
Probabilistic Neural Network (PNN) is a feed-forward artificial neural network developed for solving classification problems. In particular, the proposed algorithm performs normalization of the training weights, and quantization into 16 levels which significantly reduces the complexity of the circuit.
Link: https://arxiv.org/abs/1808.00733
====================================================
Deep Learning for Radio Resource Allocation in Multi-Cell Networks (K. I. Ahmed - 2 August, 2018)
Starting with a brief overview of a deep neural network (DNN) as a DL model, relevant DNN architectures and the data training procedure, we provide an overview of existing state-of-the-art applying DL in the context of radio resource allocation. Simulation results show that the trained DL model is able to provide the desired optimal solution 86.3% of time.
Link: https://arxiv.org/abs/1808.00667
====================================================
Classification of Building Information Model (BIM) Structures with Deep Learning (Francesco Lomio - 1 August, 2018)
The accuracy achieved is 57% for the HOG + SVM model, and above 89% for the neural networks.
Link: https://arxiv.org/abs/1808.00601
====================================================
SlimNets: An Exploration of Deep Model Compression and Acceleration (Ini Oguntola - 1 August, 2018)
With increased focus on deploying deep neural networks on resource constrained devices like smart phones, there has been a push to evaluate why these models are so resource hungry and how they can be made more efficient. We show that by combining pruning and knowledge distillation methods we can create a compressed network 85 times smaller than the original, all while retaining 96% of the original model's accuracy.
Link: https://arxiv.org/abs/1808.00496
====================================================
Low-Latency Neural Speech Translation (Jan Niehues - 1 August, 2018)
Through the development of neural machine translation, the quality of machine translation systems has been improved significantly. After adaptation, we are able to reduce the number of corrections displayed during incremental output construction by 45%, without a decrease in translation quality.
Link: https://arxiv.org/abs/1808.00491
====================================================
Towards a Semantic Perceptual Image Metric (Troy Chinen - 1 August, 2018)
We present a full reference, perceptual image metric based on VGG-16, an artificial neural network trained on object classification. The resulting metric shows competitive performance on TID 2013, a database widely used to assess image quality assessments methods
Link: https://arxiv.org/abs/1808.00447
====================================================
Efficient Progressive Neural Architecture Search (Juan-Manuel Perez-Rua - 1 August, 2018)
We propose a method that aggregates two main results of the previous state-of-the-art in neural architecture search. Sequential search has previously demonstrated its capabilities to find state-of-the-art neural architectures for image classification
Link: https://arxiv.org/abs/1808.00391
====================================================
Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs (Francisco M. Castro - 1 August, 2018)
Within this context, Convolutional Neural Network (CNN) models constitute a representative example of success on a wide set of complex applications, particularly on datasets where the target can be represented through a hierarchy of local features of increasing semantic complexity. We evaluate energy consumption on four different networks based on the two most popular ones (ResNet/AlexNet): ResNet (167 layers), a 2D CNN (15 layers), a CaffeNet (25 layers) and a ResNetIm (94 layers) using batch sizes of 64, 128 and 256, and then correlate those with speed-up and accuracy to determine optimal settings. Experimental results on a multi-GPU server endowed with twin Maxwell and twin Pascal Titan X GPUs demonstrate that energy correlates with performance and that Pascal may have up to 40% gains versus Maxwell
Link: https://arxiv.org/abs/1808.00286
====================================================
Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm (Zechun Liu - 28 September, 2018)
In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. Experiments on ImageNet show that the Bi-Real net with the proposed training algorithm achieves 56.4% and 62.2% top-1 accuracy with 18 layers and 34 layers, respectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves up to 10% higher top-1 accuracy with more memory saving and lower computational cost
Link: https://arxiv.org/abs/1808.00278
====================================================
Recurrent neural networks for aortic image sequence segmentation with sparse annotations (Wenjia Bai - 1 August, 2018)
In this work, we propose an image sequence segmentation algorithm by combining a fully convolutional network with a recurrent neural network, which incorporates both spatial and temporal information into the segmentation task. It achieves an average Dice metric of 0.960 for the ascending aorta and 0.953 for the descending aorta.
Link: https://arxiv.org/abs/1808.00273
====================================================
Binarized Convolutional Neural Networks for Efficient Inference on GPUs (Mir Khan - 1 August, 2018)
In this paper convolutional neural network binarization is implemented on GPU-based platforms for real-time inference on resource constrained devices. In binarized networks, all weights and intermediate computations between layers are quantized to +1 and -1, allowing multiplications and additions to be replaced with bit-wise operations between 32-bit words. Our implementation achieves a maximum speed up of 7. 4X with only 4.4% loss in accuracy compared to a reference implementation.
Link: https://arxiv.org/abs/1808.00209
====================================================
Reinforced Evolutionary Neural Architecture Search (Yukang Chen - 6 September, 2018)
Our method integrates reinforced mutation into an evolution algorithm for neural architecture exploration, in which a mutation controller to learn the effects of slight modifications and make mutation actions. We conduct the proposed search method on CIFAR-10 with 4 GPUs (Titan Xp) across 1.5 days and discover a powerful network architecture
Link: https://arxiv.org/abs/1808.00193
====================================================
Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network (Xinjing Cheng - 31 July, 2018)
Specifically, we adopt an efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). NYU v2 and KITTI, where we show that our proposed approach improves in not only quality (e.g., 30% more reduction in depth error), but also speed (e.g., 2 to 5 times faster) than prior SOTA methods.
Link: https://arxiv.org/abs/1808.00150
====================================================
FMCode: A 3D In-the-Air Finger Motion Based User Login Framework for Gesture Interface (Duo Lu - 31 July, 2018)
In addition, we use deep neural network approaches to efficiently identify the user based on his or her in-air-handwriting, which avoids expansive account database search methods employed by existing work. On a dataset collected by us with over 100 users, our prototype system achieves 0.1% and 0.5% best Equal Error Rate (EER) for user authentication, as well as 96.7% and 94.3% accuracy for user identification, using two types of gesture input devices
Link: https://arxiv.org/abs/1808.00130
====================================================
WeedMap: A large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming (Inkyu Sa - 6 September, 2018)
We present a novel weed segmentation and mapping framework that processes multispectral images obtained from an unmanned aerial vehicle (UAV) using a deep neural network (DNN). Compared to our baseline model (i.e., SegNet with 3 channel RGB inputs) yielding an area under the curve (AUC) of [background=0.607, crop=0.681, weed=0.576], our proposed model with 9 input channels achieves [0.839, 0.863, 0.782]. Additionally, we provide an extensive analysis of 20 trained models, both qualitatively and quantitatively, in order to evaluate the effects of varying input channels and tunable network hyperparameters
Link: https://arxiv.org/abs/1808.00100
====================================================
News Session-Based Recommendations using Deep Neural Networks (Gabriel de Souza P. Moreira - 16 September, 2018)
This architecture is composed of two modules, the first responsible to learn news articles representations, based on their text and metadata, and the second module aimed to provide session-based recommendations using Recurrent Neural Networks. Experiments with an extensive number of session-based recommendation methods were performed and the proposed instantiation of CHAMELEON meta-architecture obtained a significant relative improvement in top-n accuracy and ranking metrics (10% on Hit Rate and 13% on MRR) over the best benchmark methods.
Link: https://arxiv.org/abs/1808.00076
====================================================
Modeling Task Effects in Human Reading with Neural Attention (Michael Hahn - 31 July, 2018)
We propose a neural architecture that combines an attention module (deciding whether to skip words) and a task module (memorizing the input). We show that our model predicts human skipping behavior, while also modeling reading times well, even though it skips 40% of the input
Link: https://arxiv.org/abs/1808.00054
====================================================
Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition (Swathikiran Sudhakaran - 31 July, 2018)
In this paper we propose an end-to-end trainable deep neural network model for egocentric activity recognition. Nonetheless, on standard egocentric activity benchmarks our model surpasses by up to +6% points recognition accuracy the currently best performing method that leverages hand segmentation and object location strong supervision for training
Link: https://arxiv.org/abs/1807.11794
====================================================
Scale equivariance in CNNs with vector fields (Diego Marcos - 31 July, 2018)
We study the effect of injecting local scale equivariance into Convolutional Neural Networks. We show that this improves the performance of the model by over $20\%$ in the scale equivariant task of regressing the scaling factor applied to randomly scaled MNIST digits
Link: https://arxiv.org/abs/1807.11783
====================================================
Compact Convolutional Neural Networks for Multi-Class, Personalised, Closed-Loop EEG-BCI (Pablo Ortega - 31 July, 2018)
Since the system is devised to be used in domestic environments in a user-friendly way, we selected non-invasive electroencephalographic (EEG) signals and convolutional neural networks (CNNs), known for their ability to find the optimal features in classification tasks. Our preliminary results show that an efficient architecture (SmallNet), with only one convolutional layer, can classify 4 mental activities chosen by the user. It is kept up-to-date through the use of newly collected signals along playing, reaching an online accuracy of 47.6% where most approaches only report results obtained offline. Compared to our previous decoder of physiological signals relying on blinks, we increased by a factor 2 the amount of states among which the user can transit, bringing the opportunity for finer control of specific subtasks composing natural grasping in a self-paced way
Link: https://arxiv.org/abs/1807.11752
====================================================
Gender Bias in Neural Natural Language Processing (Kaiji Lu - 31 July, 2018)
Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark datasets finds significant gender bias in how models view occupations
Link: https://arxiv.org/abs/1807.11714
====================================================
Multimodal Deep Domain Adaptation (Silvia Bucci - 31 July, 2018)
With this purpose some of the state-of-the-art DA methods are selected: Deep Adaptation Network (DAN), Domain Adversarial Training of Neural Network (DANN), Automatic Domain Alignment Layers (AutoDIAL) and Adversarial Discriminative Domain Adaptation (ADDA)
Link: https://arxiv.org/abs/1807.11697
====================================================
Deep Belief Networks Based Feature Generation and Regression for Predicting Wind Power (Asifullah Khan - 31 July, 2018)
Deep Neural Networks (DNN) mimics hierarchical learning in the human brain and thus possesses hierarchical, distributed, and multi-task learning capabilities. The proposed prediction system based on DBN, achieves mean values of RMSE, MAE and SDE as 0.124, 0.083 and 0.122, respectively
Link: https://arxiv.org/abs/1807.11682
====================================================
MnasNet: Platform-Aware Neural Architecture Search for Mobile (Mingxing Tan - 30 July, 2018)
In this paper, we propose an automated neural architecture search approach for designing resource-constrained mobile CNN models. On the ImageNet classification task, our model achieves 74.0% top-1 accuracy with 76ms latency on a Pixel phone, which is 1.5x faster than MobileNetV2 (Sandler et al. 2018) and 2.4x faster than NASNet (Zoph et al. 2018) with the same top-1 accuracy
Link: https://arxiv.org/abs/1807.11626
====================================================
Testing the Efficient Network TRaining (ENTR) Hypothesis: initially reducing training image size makes Convolutional Neural Network training for image recognition tasks more efficient (Thomas Cherico Wanger - 30 July, 2018)
Convolutional Neural Networks (CNN) for image recognition tasks are seeing rapid advances in the available architectures and how networks are trained based on large computational infrastructure and standard datasets with millions of images. We test this Efficient Network TRaining (ENTR) Hypothesis by training pre-trained Residual Network (ResNet) models (ResNet18, 34, & 50) on three small datasets (steel microstructures, bee images, and geographic aerial images) with a free cloud GPU
Link: https://arxiv.org/abs/1807.11583
====================================================
A Hierarchical Approach to Neural Context-Aware Modeling (Patrick Huber - 6 August, 2018)
We present a new recurrent neural network topology to enhance state-of-the-art machine learning systems by incorporating a broader context. The performance measures on the error detection task show the advantage of the hierarchical context-aware topologies, improving the baseline by 12.75% relative for unsupervised models and 20.37% relative for supervised models.
Link: https://arxiv.org/abs/1807.11582
====================================================
Deep Recurrent Neural Networks for ECG Signal Denoising (Karol Antczak - 17 August, 2018)
We present a novel approach to denoise electrocardiographic signals (ECG), utilizing deep recurrent neural network built of Long-Short Term Memory (LSTM) units. The results show that a 10-layer DRNN has a mean squared error as low as 0.179 for denoising real signals with white noise of amplitude 0.2 mV, making it a viable alternative for other commonly used methods
Link: https://arxiv.org/abs/1807.11551
====================================================
Improving Transferability of Deep Neural Networks (Parijat Dube - 30 July, 2018)
One of the important parameters is the learning rate for the layers of the neural network. We show through experiments on the ImageNet22k and Oxford Flowers datasets that improvements in accuracy in range of 127% can be obtained by proper choice of learning rates
Link: https://arxiv.org/abs/1807.11459
====================================================
Extreme Network Compression via Filter Group Approximation (Bo Peng - 31 July, 2018)
In this paper we propose a novel decomposition method based on filter group approximation, which can significantly reduce the redundancy of deep convolutional neural networks (CNNs) while maintaining the majority of feature representation. For several commonly used CNN models, including VGG and ResNet, our method can reduce over 80% floating-point operations (FLOPs) with less accuracy drop than state-of-the-art methods on various image classification datasets
Link: https://arxiv.org/abs/1807.11254
====================================================
Improving Electron Micrograph Signal-to-Noise with an Atrous Convolutional Encoder-Decoder (Jeffrey M. Ede - 30 July, 2018)
Our neural network was trained end-to-end to remove Poisson noise applied to low-dose ($\ll$ 300 counts ppx) micrographs created from a new dataset of 17267 2048$\times$2048 high-dose ($>$ 2500 counts ppx) micrographs and then fine-tuned for ordinary doses (200-2500 counts ppx). Our network outperforms their best mean squared error and structural similarity index performances by 24.6% and 9.6% for low doses and by 43.7% and 5.5% for ordinary doses
Link: https://arxiv.org/abs/1807.11234
====================================================
Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes (Xianyan Jia - 30 July, 2018)
Synchronized stochastic gradient descent (SGD) optimizers with data parallelism are widely used in training large-scale deep neural networks. (3) We propose highly optimized all-reduce algorithms that achieve up to 3x and 11x speedup on AlexNet and ResNet-50 respectively than NCCL-based training on a cluster with 1024 Tesla P40 GPUs. On training ResNet-50 with 90 epochs, the state-of-the-art GPU-based system with 1024 Tesla P100 GPUs spent 15 minutes and achieved 74.9\% top-1 test accuracy, and another KNL-based system with 2048 Intel KNLs spent 20 minutes and achieved 75.4\% accuracy. Our training system can achieve 75.8\% top-1 test accuracy in only 6.6 minutes using 2048 Tesla P40 GPUs. When training AlexNet with 95 epochs, our system can achieve 58.7\% top-1 test accuracy within 4 minutes, which also outperforms all other existing systems.
Link: https://arxiv.org/abs/1807.11205
====================================================
Multi-Fiber Networks for Video Recognition (Yunpeng Chen - 18 September, 2018)
In this paper, we aim to reduce the computational cost of spatio-temporal deep neural networks, making them run as fast as their 2D counterparts while preserving state-of-the-art accuracy on video recognition benchmarks. Our proposed model requires over 9x and 13x less computations than the I3D and R(2+1)D models, respectively, yet providing higher accuracy.
Link: https://arxiv.org/abs/1807.11195
====================================================
Leveraging Medical Sentiment to Understand Patients Health on Social Media (Shweta Yadav - 30 July, 2018)
We propose an effective architecture that uses a Convolutional Neural Network (CNN) as a data-driven feature extractor and a Support Vector Machine (SVM) as a classifier. In addition to our dataset, we also evaluate our approach on the benchmark "CLEF eHealth 2014" corpora and show that our model outperforms the state-of-the-art techniques.
Link: https://arxiv.org/abs/1807.11172
====================================================
Neural Mesh: Introducing a Notion of Space and Conservation of Energy to Neural Networks (Jacob Beck - 29 July, 2018)
In this project, we wanted to relax the simplifying assumptions of a traditional neural network by making a model that more closely emulates the low level interactions of neurons. However, unlike an RNN, our state consists of a 2 dimensional matrix, rather than a 1 dimensional vector, thereby introducing a concept of distance to other neurons within the state
Link: https://arxiv.org/abs/1807.11121
====================================================
Spiking Neural Networks for Early Prediction in Human Robot Collaboration (Tian Zhou - 29 July, 2018)
This paper introduces the Turn-Taking Spiking Neural Network (TTSNet), which is a cognitive model to perform early turn-taking prediction about human or agent's intentions. It was found to reach a F1 score of 0.683 given 10% of completed action, and a F1 score of 0.852 at 50% and 0.894 at 100% of the completed action. This performance outperformed multiple state-of-the-art algorithms, and surpassed human performance when limited partial observation is given (< 40%)
Link: https://arxiv.org/abs/1807.11096
====================================================
ADAM-ADMM: A Unified, Systematic Framework of Structured Weight Pruning for DNNs (Tianyun Zhang - 29 July, 2018)
Weight pruning methods of deep neural networks (DNNs) have been demonstrated to achieve a good model pruning ratio without loss of accuracy, thereby alleviating the significant computation/storage requirements of large-scale DNNs. However, the pruning ratio (degree of sparsity) and GPU acceleration are limited (to less than 50%) when accuracy needs to be maintained.
Link: https://arxiv.org/abs/1807.11091
====================================================
Joint Representation and Truncated Inference Learning for Correlation Filter based Tracking (Yingjie Yao - 29 July, 2018)
By modeling the representor as convolutional neural network (CNN), we truncate the alternating direction method of multipliers (ADMM) and interpret it as a deep network of updater, resulting in our model for learning representation and truncated inference (RTINet). Experiments demonstrate that our RTINet tracker achieves favorable tracking accuracy against the state-of-the-art trackers and its rapid version can run at a real-time speed of 24 fps
Link: https://arxiv.org/abs/1807.11071
====================================================
Towards Good Practices on Building Effective CNN Baseline Model for Person Re-identification (Fu Xiong - 29 July, 2018)
To address this, most of the state-of-the-art approaches are proposed based on deep convolutional neural network (CNN), being leveraged by its strong feature learning power and classification boundary fitting capacity. To answer this open question, we propose 3 good practices in this paper from the perspectives of adjusting CNN architecture and training procedure. The extensive experiments on 3 widely-used benchmark datasets demonstrate that, our propositions essentially facilitate the CNN baseline model to achieve the state-of-the-art performance without any other high-level domain knowledge or low-level technical trick.
Link: https://arxiv.org/abs/1807.11042
====================================================
Efficient Uncertainty Estimation for Semantic Segmentation in Videos (Po-Yu Huang - 29 July, 2018)
Some literature proposes the Bayesian neural network which can estimate the uncertainty by Monte Carlo Dropout (MC dropout). Our RTA method with Tiramisu backbone is 10x faster than the MC dropout with Tiramisu backbone ($N=5$)
Link: https://arxiv.org/abs/1807.11037
====================================================
RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumours (Raghav Mehta - 28 July, 2018)
In this paper, we present an end-to-end 3D convolution neural network that takes a set of acquired MR image sequences (e.g. Experiments on the BraTS 2015 and 2017 datasets [1] show that: (1) the proposed method gives better performance than state-of-the-art methods in terms of established global evaluation metrics (e.g. The system further provides uncertainty estimates based on Monte Carlo (MC) dropout [11] for the synthesized volume at each voxel, permitting quantification of the system's confidence in the output at each location.
Link: https://arxiv.org/abs/1807.10972
====================================================
PROPEL: Probabilistic Parametric Regression Loss for Convolutional Neural Networks (Muhammad Asad - 28 July, 2018)
Recently, Convolutional Neural Networks (CNNs) have dominated the field of computer vision. Our experimental results indicate that PROPEL significantly improves the performance of a CNN, while reducing model parameters by 10x as compared to the existing state-of-the-art.
Link: https://arxiv.org/abs/1807.10937
====================================================
Bike Flow Prediction with Multi-Graph Convolutional Networks (Di Chai - 28 July, 2018)
We propose a new multi-graph convolutional neural network model to predict the bike flow at station-level, where the key novelty is viewing the bike sharing system from the graph perspective. Using New York City and Chicago bike sharing data for experiments, our model can outperform state-of-the-art station-level prediction models by reducing 25.1% and 17.0% of prediction error in New York City and Chicago, respectively.
Link: https://arxiv.org/abs/1807.10934
====================================================
TBI Contusion Segmentation from MRI using Convolutional Neural Networks (Snehashis Roy - 27 July, 2018)
In this paper, we propose a fully convolutional neural network (CNN) model to segment contusions and lesions from brain magnetic resonance (MR) images of patients with TBI. When compared with two recent TBI lesion segmentation methods, one based on CNN (called DeepMedic) and another based on random forests, the proposed algorithm showed improved segmentation accuracy on images of 18 patients with mild to severe TBI. Using a leave-one-out cross validation, the proposed model achieved a median Dice of 0.75, which was significantly better (p<0.01) than the two competing methods.
Link: https://arxiv.org/abs/1807.10839
====================================================
A Case Study of Spherical Parallel Manipulators Fabricated via Laminate Processes (Mohammad Sharifzadeh - 24 September, 2018)
We discuss one method for compensating position uncertainty via an experimental identification technique which uses a neural network to create a forward kinematic model
Link: https://arxiv.org/abs/1810.05131
====================================================
Crack-pot: Autonomous Road Crack and Pothole Detection (Sukhad Anand - 9 September, 2018)
The approach is based on a deep neural net architecture which detects cracks and potholes using texture and spatial features
Link: https://arxiv.org/abs/1810.05107
====================================================
Simple and Effective Text Simplification Using Semantic and Neural Methods (Elior Sulem - 11 October, 2018)
In particular, we show that neural Machine Translation can be effectively used in this situation. Extensive automatic and human evaluation shows that the proposed method compares favorably to the state-of-the-art in combined lexical and structural simplification.
Link: https://arxiv.org/abs/1810.05104
====================================================
Neural Relation Extraction Within and Across Sentence Boundaries (Pankaj Gupta - 11 October, 2018)
iDepNN models the shortest and augmented dependency paths via recurrent and recursive neural networks to extract relationships within (intra-) and across (inter-) sentence boundaries. Compared to SVM and neural network baselines, iDepNN is more robust to false positives in relationships spanning sentences.
Link: https://arxiv.org/abs/1810.05102
====================================================
Multi-Strategy Coevolving Aging Particle Optimization (Giovanni Iacca - 11 October, 2018)
To demonstrate the applicability of the approach, we also test MS-CAP to train a Feedforward Neural Network modelling the kinematics of an 8-link robot manipulator. The numerical results show that MS-CAP, for the setting considered in this study, tends to outperform the state-of-the-art optimization algorithms on a large set of problems, thus resulting in a robust and versatile optimizer.
Link: https://arxiv.org/abs/1810.05018
====================================================
One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL (Tom Le Paine - 11 October, 2018)
MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task
Link: https://arxiv.org/abs/1810.05017
====================================================
Online Visual Robot Tracking and Identification using Deep LSTM Networks (Hafez Farazi - 11 October, 2018)
Unlike previous works addressing robot tracking and identification, we use a data-driven approach based on recurrent neural networks to learn relations between sequential inputs and outputs
Link: https://arxiv.org/abs/1810.04941
====================================================
Location Dependency in Video Prediction (Niloofar Azizi - 11 October, 2018)
Deep convolutional neural networks are used to address many computer vision problems, including video prediction. Convolutional neural networks are spatially invariant, though, which prevents them from modeling location-dependent patterns
Link: https://arxiv.org/abs/1810.04937
====================================================
Novel Cascaded Gaussian Mixture Model-Deep Neural Network Classifier for Speaker Identification in Emotional Talking Environments (Ismail Shahin - 11 October, 2018)
This research is an effort to present an effective approach to enhance text-independent speaker identification performance in emotional talking environments based on novel classifier called cascaded Gaussian Mixture Model-Deep Neural Network (GMM-DNN). Our current work focuses on proposing, implementing and evaluating a new approach for speaker identification in emotional talking environments based on cascaded Gaussian Mixture Model-Deep Neural Network as a classifier
Link: https://arxiv.org/abs/1810.04908
====================================================
Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word- vs. Character-based Processing and Output Diversity (Glorianna Jagfeld - 11 October, 2018)
In a controlled experiment with synthetic training data generated from templates, we demonstrate the ability of neural models to learn novel combinations of the templates and thereby generalize beyond the linguistic structures they were trained on.
Link: https://arxiv.org/abs/1810.04864
====================================================
End-to-End Content and Plan Selection for Data-to-Text Generation (Sebastian Gehrmann - 10 October, 2018)
Learning to generate fluent natural language from structured data with neural networks has become an common approach for NLG
Link: https://arxiv.org/abs/1810.04700
====================================================
Structured Argument Extraction of Korean Question and Command (Won Ik Cho - 10 October, 2018)
In order to suggest a guideline to this problem, inspired by the neural summarization systems introduced recently, we propose a structured annotation scheme for Korean questions/commands which is widely applicable to the field of argument extraction
Link: https://arxiv.org/abs/1810.04631
====================================================
Temporal Cross-Media Retrieval with Soft-Smoothing (David Semedo - 10 October, 2018)
Building on such knowledge, we propose a novel temporal cross-media neural architecture, that departs from standard cross-media methods, by explicitly accounting for the temporal dimension through temporal subspace learning
Link: https://arxiv.org/abs/1810.04547
====================================================
A Deep Learning Approach to the Inversion of Borehole Resistivity Measurements (M. Shahriari - 5 October, 2018)
In this work, we explore the possibility of using Deep Neural Network (DNN) to perform a rapid inversion of borehole resistivity measurements
Link: https://arxiv.org/abs/1810.04522
====================================================
Let's take a Walk on Superpixels Graphs: Deformable Linear Objects Segmentation and Model Estimation (Daniele De Gregorio - 10 October, 2018)
The algorithm is initialized by a Convolutional Neural Networks that detects the DLO's endcaps
Link: https://arxiv.org/abs/1810.04461
====================================================
Global Search with Bernoulli Alternation Kernel for Task-oriented Grasping Informed by Simulation (Rika Antonova - 10 October, 2018)
Our further contribution is a neural network architecture and training pipeline that use experience from grasping objects in simulation to learn grasp stability scores
Link: https://arxiv.org/abs/1810.04438
====================================================
Improving Neural Text Simplification Model with Simplified Corpora (Jipeng Qiang - 10 October, 2018)
But different from neural machine translation, we cannot obtain enough ordinary and simplified sentence pairs for TS, which are expensive and time-consuming to build. We train encoder-decoder model using synthetic sentence pairs and original sentence pairs, which can obtain substantial improvements on the available WikiLarge data and WikiSmall data compared with the state-of-the-art methods.
Link: https://arxiv.org/abs/1810.04428
====================================================
Response to Comment on "All-optical machine learning using diffractive deep neural networks" (Deniz Mengu - 10 October, 2018)
(arXiv:1809.08360v1 [cs.LG]) claim that our original interpretation of Diffractive Deep Neural Networks (D2NN) represent a mischaracterization of the system due to linearity and passivity
Link: https://arxiv.org/abs/1810.04384
====================================================
Deep supervised feature selection using Stochastic Gates (Yutaro Yamada - 9 October, 2018)
Using these tools we present a general neural network that simultaneously minimizes a loss function while selecting relevant features
Link: https://arxiv.org/abs/1810.04247
====================================================
Deep clustering: On the link between discriminative models and K-means (Mohammed Jabi - 9 October, 2018)
These models learn a deep discriminative neural network classifier in which the labels are latent. Our theoretical analysis not only connects directly several recent state-of-the-art discriminative models to K-means, but also leads to a new soft and regularized deep K-means algorithm, which yields competitive performance on several image clustering benchmarks.
Link: https://arxiv.org/abs/1810.04246
====================================================
Penetrating the Fog: the Path to Efficient CNN Models (Kun Wan - 10 October, 2018)
With the increasing demand to deploy convolutional neural networks (CNNs) on mobile platforms, the sparse kernel approach was proposed, which could save more parameters than the standard convolution while maintaining accuracy
Link: https://arxiv.org/abs/1810.04231
====================================================
Event Coreference Resolution Using Neural Network Classifiers (Arun Pandian - 9 October, 2018)
This paper presents a neural network classifier approach to detecting both within- and cross- document event coreference effectively using only event mention based features. Experimental results on the ECB+ dataset show that our approach produces F1 scores that significantly outperform the state-of-the-art methods for both within-document and cross-document event coreference resolution when we use B3 and CEAFe evaluation measures, but gets worse F1 score with the MUC measure
Link: https://arxiv.org/abs/1810.04216
====================================================
The Computational Complexity of Training ReLU(s) (Pasin Manurangsi - 9 October, 2018)
We consider the computational complexity of training depth-2 neural networks composed of rectified linear units (ReLUs)
Link: https://arxiv.org/abs/1810.04207
====================================================
Optimized Gated Deep Learning Architectures for Sensor Fusion (Myung Seok Shim - 8 October, 2018)
Deep neural networks have been adopted for sensor fusion in a body of recent studies. Among these, the so-called netgated architecture was proposed, which has demonstrated improved performances over the conventional convolutional neural networks (CNN)
Link: https://arxiv.org/abs/1810.04160
====================================================
Seeing Beyond Appearance - Mapping Real Images into Geometrical Domains for Unsupervised CAD-based Recognition (Benjamin Planche - 9 October, 2018)
While convolutional neural networks are dominating the field of computer vision, one usually does not have access to the large amount of domain-relevant data needed for their training
Link: https://arxiv.org/abs/1810.04158
====================================================
Learning One-hidden-layer Neural Networks under General Input Distributions (Weihao Gao - 9 October, 2018)
Significant advances have been made recently on training neural networks, where the main challenge is in solving an optimization problem with abundant critical points. Our loss function design bridges the notion of score functions with the topic of neural network optimization
Link: https://arxiv.org/abs/1810.04133
====================================================
Person-Job Fit: Adapting the Right Talent for the Right Job with Joint Representation Learning (Chen Zhu - 8 October, 2018)
To this end, in this paper, we propose a novel end-to-end data-driven model based on Convolutional Neural Network (CNN), namely Person-Job Fit Neural Network (PJFNN), for matching a talent qualification to the requirements of a job. To be specific, PJFNN is a bipartite neural network which can effectively learn the joint representation of Person-Job fitness from historical job applications
Link: https://arxiv.org/abs/1810.04040
====================================================
Understanding and Improving Recurrent Networks for Human Activity Recognition by Continuous Attention (Ming Zeng - 7 October, 2018)
Deep neural networks, including recurrent networks, have been successfully applied to human activity recognition. We evaluate the approaches on three datasets and obtain state-of-the-art results
Link: https://arxiv.org/abs/1810.04038
====================================================
Learning Converged Propagations with Deep Prior Ensemble for Image Enhancement (Risheng Liu - 9 October, 2018)
In the past few years, both knowledge-driven maximum a posterior (MAP) with prior modelings and fully data-dependent convolutional neural network (CNN) techniques have been investigated to address specific enhancement tasks. Experimental results demonstrate that the proposed DPE outperforms state-of-the-arts on a variety of image enhancement tasks in terms of both quantitative measure and visual perception quality.
Link: https://arxiv.org/abs/1810.04012
====================================================
Learning Noun Cases Using Sequential Neural Networks (Sina Ahmadi - 9 October, 2018)
This research proposal seeks to address the degree to which Recurrent Neural Networks (RNNs) are efficient in learning to decline noun cases. Given the challenge of data sparsity in processing morphologically rich languages and also, the flexibility of sentence structures in such languages, we believe that modeling morphological dependencies can improve the performance of neural network models
Link: https://arxiv.org/abs/1810.03996
====================================================
DeepNIS: Deep Neural Network for Nonlinear Electromagnetic Inverse Scattering (Lianlin Li - 4 October, 2018)
To tackle these difficulties, we, for the first time to our best knowledge, exploit a connection between the deep neural network (DNN) architecture and the iterative method of nonlinear EM inverse scattering. The proposed DeepNIS consists of a cascade of multi-layer complexvalued residual convolutional neural network (CNN) modules
Link: https://arxiv.org/abs/1810.03990
====================================================
Image-to-Video Person Re-Identification by Reusing Cross-modal Embeddings (Zhongwei Xie - 4 October, 2018)
In this paper,we propose an end-to-end neural network framework for image-to-video person reidentification by leveraging cross-modal embeddings learned from extra information.Concretely speaking,cross-modal embeddings from image captioning and video captioning models are reused to help learned features be projected into a coordinated space,where similarity can be directly computed
Link: https://arxiv.org/abs/1810.03989
====================================================
Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks (Reinhard Heckel - 2 October, 2018)
Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.
Link: https://arxiv.org/abs/1810.03982
====================================================
Collective evolution of weights in wide neural networks (Dmitry Yarotsky - 9 October, 2018)
We derive a nonlinear integro-differential transport equation describing collective evolution of weights under gradient descent in large-width neural-network-like models
Link: https://arxiv.org/abs/1810.03974
====================================================
Adaptive Image Stream Classification via Convolutional Neural Network with Intrinsic Similarity Metrics (Yang Gao - 27 September, 2018)
Particularly, we utilize a convolution neural network layer that aids in the learning of a latent feature space suitable for novel class detection
Link: https://arxiv.org/abs/1810.03966
====================================================
Fixing Variational Bayes: Deterministic Variational Inference for Bayesian Neural Networks (Anqi Wu - 9 October, 2018)
We fix VB and turn it into a robust inference tool for Bayesian neural networks. We achieve this with two innovations: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel empirical Bayes procedure for automatically selecting prior variances
Link: https://arxiv.org/abs/1810.03958
====================================================
textTOvec: Deep Contextualized Neural Autoregressive Models of Language with Distributed Compositional Prior (Pankaj Gupta - 10 October, 2018)
We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the word-topic mapping on a smaller and/or short-text corpus
Link: https://arxiv.org/abs/1810.03947
====================================================
Convolutional Neural Networks In Convolution (Xiaobo Huang - 9 October, 2018)
Currently, increasingly deeper neural networks have been applied to improve their accuracy. In contrast, We propose a novel wider Convolutional Neural Networks (CNN) architecture, motivated by the Multi-column Deep Neural Networks and the Network In Network(NIN), aiming for higher accuracy without input data transmutation
Link: https://arxiv.org/abs/1810.03946
====================================================
Analyzing the Noise Robustness of Deep Neural Networks (Mengchen Liu - 9 October, 2018)
Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples
Link: https://arxiv.org/abs/1810.03913
====================================================
Functionally Modular and Interpretable Temporal Filtering for Robust Segmentation (JÃ¶rg Wagner - 9 October, 2018)
Deep neural networks have greatly improved vision-based perception systems but still fail in challenging situations, e.g
Link: https://arxiv.org/abs/1810.03867
====================================================
The Adversarial Attack and Detection under the Fisher Information Metric (Chenxiao Zhao - 9 October, 2018)
By considering the data space as a non-linear space with the Fisher information metric induced from a neural network, we first propose an adversarial attack algorithm termed one-step spectral attack (OSSA)
Link: https://arxiv.org/abs/1810.03806
====================================================
What made you do this? Understanding black-box decisions with sufficient input subsets (Brandon Carter - 9 October, 2018)
We demonstrate the utility of our interpretation method on various neural network models trained on text, image, and genomic data.
Link: https://arxiv.org/abs/1810.03805
====================================================
The Outer Product Structure of Neural Network Derivatives (Craig Bakker - 8 October, 2018)
In this paper, we show that feedforward and recurrent neural networks exhibit an outer product derivative structure but that convolutional neural networks do not. This structure makes it possible to use higher-order information without needing approximations or infeasibly large amounts of memory, and it may also provide insights into the geometry of neural network optima
Link: https://arxiv.org/abs/1810.03798
====================================================
Average Margin Regularization for Classifiers (Matt Olfat - 8 October, 2018)
In this paper, we propose and then study a new regularization for any margin classifier or deep neural network. We conclude by using both synthetic and real data to empirically show that AM regularization can strictly improve both accuracy and robustness for support vector machine's (SVM's) and deep neural networks, relative to unregularized classifiers and adversarially trained classifiers.
Link: https://arxiv.org/abs/1810.03773
====================================================
Neural Networks Models for Analyzing Magic: the Gathering Cards (Felipe Zilio - 8 October, 2018)
This work aims to apply neural networks models, including Convolutional Neural Networks and Recurrent Neural Networks, in order to analyze Magic: the Gathering cards, both in terms of card text and illustrations; the card images and texts are used to train the networks in order to be able to classify them into multiple categories
Link: https://arxiv.org/abs/1810.03744
====================================================
Efficient Two-Step Adversarial Defense for Deep Neural Networks (Ting-Jui Chang - 8 October, 2018)
In recent years, deep neural networks have demonstrated outstanding performance in many machine learning tasks. However, researchers have discovered that these state-of-the-art models are vulnerable to adversarial examples: legitimate examples added by small perturbations which are unnoticeable to human eyes
Link: https://arxiv.org/abs/1810.03739
====================================================
Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos (Yang Wang - 8 October, 2018)
Learning depth and optical flow via deep neural networks by watching videos has made significant progress recently. Specifically, given two consecutive stereo image pairs from a video, we first estimate depth, camera ego-motion and optical flow from three neural networks
Link: https://arxiv.org/abs/1810.03654
====================================================
End-to-End Text Classification via Image-based Embedding using Character-level Networks (Shunsuke Kitada - 10 October, 2018)
In this paper, we propose a CE-CLCNN, character-level convolutional neural networks using a character encoder to tackle these problems. Through various experiments, we found and confirmed that our CE-CLCNN captured closely embedded features for visually and semantically similar characters and achieves state-of-the-art results on several open document classification tasks
Link: https://arxiv.org/abs/1810.03595
====================================================
Zero-Resource Multilingual Model Transfer: Learning What to Share (Xilun Chen - 8 October, 2018)
Modern natural language processing and understanding applications have enjoyed a great boost utilizing neural networks models
Link: https://arxiv.org/abs/1810.03552
====================================================
Meta-Learning: A Survey (Joaquin Vanschoren - 8 October, 2018)
Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.
Link: https://arxiv.org/abs/1810.03548
====================================================
Combinatorial Attacks on Binarized Neural Networks (Elias B. Khalil - 8 October, 2018)
Concurrently, it has been shown that neural networks may be overly sensitive to "attacks" - tiny adversarial changes in the input - which may be detrimental to their use in safety-critical domains. Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks
Link: https://arxiv.org/abs/1810.03538
====================================================
Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks (Sanghyun Hong - 8 October, 2018)
Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary's capability to conduct black-box attacks against the model
Link: https://arxiv.org/abs/1810.03487
====================================================
On Breiman's Dilemma in Neural Networks: Phase Transitions of Margin Dynamics (Weizhi Zhu - 8 October, 2018)
In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed spectrally normalized margins
Link: https://arxiv.org/abs/1810.03389
====================================================
Detecting Memorization in ReLU Networks (Edo Collins - 8 October, 2018)
We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets
Link: https://arxiv.org/abs/1810.03372
====================================================
Empirical Bounds on Linear Regions of Deep Rectifier Networks (Thiago Serra - 8 October, 2018)
One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled
Link: https://arxiv.org/abs/1810.03370
====================================================
Practical Implementation of Memristor-Based Threshold Logic Gates (Georgios Papandroulidakis - 8 October, 2018)
This is achieved by varying memristive weights arbitrarily for shaping the classification decision boundary, thus showing promise as an alternative hardware-friendly implementation of Artificial Neural Networks (ANNs)
Link: https://arxiv.org/abs/1810.03333
====================================================
Sanity Checks for Saliency Maps (Julius Adebayo - 8 October, 2018)
Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.
Link: https://arxiv.org/abs/1810.03292
====================================================
Query Tracking for E-commerce Conversational Search: A Machine Comprehension Perspective (Yunlun Yang - 8 October, 2018)
We also propose a self attention based neural network to handle the task in a machine comprehension perspective
Link: https://arxiv.org/abs/1810.03274
====================================================
Diagnosing Convolutional Neural Networks using their Spectral Response (Victor Stamatescu - 7 October, 2018)
Convolutional Neural Networks (CNNs) are a class of artificial neural networks whose computational blocks use convolution, together with other linear and non-linear operations, to perform classification or regression
Link: https://arxiv.org/abs/1810.03241
====================================================
Rethinking Recurrent Latent Variable Model for Music Composition (Eunjeong Stella Koh - 7 October, 2018)
We present a model for capturing musical features and creating novel sequences of music, called the Convolutional Variational Recurrent Neural Network. We compare the performance of our proposed model with other types of Neural Networks using the criteria of Information Rate that is implemented by Variable Markov Oracle, a method that allows statistical characterization of musical information dynamics and detection of motifs in a song
Link: https://arxiv.org/abs/1810.03226
====================================================
Principled Deep Neural Network Training through Linear Programming (Daniel Bienstock - 7 October, 2018)
Unfortunately, while very powerful, Deep Learning is not well understood theoretically and in particular only recently results for the complexity of training deep neural networks have been obtained. In this work we show that large classes of deep neural networks with various architectures (e.g., DNNs, CNNs, Binary Neural Networks, and ResNets), activation functions (e.g., ReLUs and leaky ReLUs), and loss functions (e.g., Hinge loss, Euclidean loss, etc) can be trained to near optimality with desired target accuracy using linear programming in time that is exponential in the size of the architecture and polynomial in the size of the data set; this is the best one can hope for due to the NP-Hardness of the problem and in line with previous work
Link: https://arxiv.org/abs/1810.03218
====================================================
Image Completion on CIFAR-10 (Mason Swofford - 7 October, 2018)
This project performed image completion on CIFAR-10, a dataset of 60,000 32x32 RGB images, using three different neural network architectures: fully convolutional networks, convolutional networks with fully connected layers, and encoder-decoder convolutional networks
Link: https://arxiv.org/abs/1810.03213
====================================================
Pre-Synaptic Pool Modification (PSPM): A Supervised Learning Procedure for Spiking Neural Networks (Bryce Bagley - 10 October, 2018)
Inspired by synaptic scaling, our pre-synaptic pool modification (PSPM) algorithm outputs deterministic, fully recurrent spiking neural networks that can provide a novel generative model for given spike trains. Our results establish that learning rules based on synaptic homeostasis can be used to represent input-output relationships in fully recurrent spiking neural networks.
Link: https://arxiv.org/abs/1810.03199
====================================================
Finding Correspondences for Optical Flow and Disparity Estimations using a Sub-pixel Convolution-based Encoder-Decoder Network (Juan Luis Gonzalez - 7 October, 2018)
Deep convolutional neural networks (DCNN) have recently shown promising results in low-level computer vision problems such as optical flow and disparity estimation, but still, have much room to further improve their performance
Link: https://arxiv.org/abs/1810.03155
====================================================
Assessing Crosslingual Discourse Relations in Machine Translation (Karin Sim Smith - 7 October, 2018)
While significant progress has been achieved, especially recently with neural models, automatically evaluating the output of such systems is still an open problem
Link: https://arxiv.org/abs/1810.03148
====================================================
Coronary Artery Centerline Extraction in Cardiac CT Angiography Using a CNN-Based Orientation Classifier (Jelmer M. Wolterink - 7 October, 2018)
We propose an algorithm that extracts coronary artery centerlines in CCTA using a convolutional neural network (CNN).
Link: https://arxiv.org/abs/1810.03143
====================================================
Control of uniflagellar soft robots at low Reynolds number using buckling instability (Mojtaba Forghani - 7 October, 2018)
We also show that the complexity of the dynamics of the helical filament can be captured using a deep neural network, from which we identify the input-output functional relationship between the control inputs and the trajectory of the robot
Link: https://arxiv.org/abs/1810.03113
====================================================
Training Convolutional Neural Networks and Compressed Sensing End-to-End for Microscopy Cell Detection (Yao Xue - 6 October, 2018)
In this paper, we design a new cell detection and localization algorithm that combines deep convolutional neural network (CNN) and compressed sensing (CS) or sparse coding (SC) for end-to-end training
Link: https://arxiv.org/abs/1810.03075
====================================================
Graph Classification with Geometric Scattering (Feng Gao - 6 October, 2018)
One of the most notable contributions of deep learning is the application of convolutional neural networks (ConvNets) to structured signal classification, and in particular image classification
Link: https://arxiv.org/abs/1810.03068
====================================================
Deep Model-Based 6D Pose Refinement in RGB (Fabian Manhardt - 6 October, 2018)
Building on the established idea of contour-based pose tracking, we teach a deep neural network to predict a translational and rotational update
Link: https://arxiv.org/abs/1810.03065
====================================================
CSI-Net: Unified Human Body Characterization and Action Recognition (Fei Wang - 6 October, 2018)
To address these issues, we build CSI-Net, a unified Deep Neural Network (DNN), that fully utilizes the strength of deep feature representation and the power of existing DNN architectures for CSI-based human sensing problems
Link: https://arxiv.org/abs/1810.03064
====================================================
Over-parameterization Improves Generalization in the XOR Detection Problem (Alon Brutzkus - 6 October, 2018)
Empirical evidence suggests that neural networks with ReLU activations generalize better with over-parameterization
Link: https://arxiv.org/abs/1810.03037
====================================================
Team NimbRo at MBZIRC 2017: Autonomous Valve Stem Turning using a Wrench (Max Schwarz - 6 October, 2018)
An object detection architecture based on deep neural networks is used to find and select the correct tool from grayscale images
Link: https://arxiv.org/abs/1810.02997
====================================================
Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences (Yi Tay - 6 October, 2018)
This paper proposes Co-Stack Residual Affinity Networks (CSRAN), a new and universal neural architecture for this problem. We conduct extensive experiments on six well-studied text sequence matching datasets, achieving state-of-the-art performance on all.
Link: https://arxiv.org/abs/1810.02938
====================================================
Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning (Fabio Pardo - 5 October, 2018)
Considering that a significant number of RL environments can support spatial coordinates as goals, such as on-screen location of the character in ATARI or SNES games, we propose a novel goal-oriented agent called Q-map that utilizes an autoencoder-like neural network to predict the minimum number of steps towards each coordinate in a single forward pass
Link: https://arxiv.org/abs/1810.02927
====================================================
Entity Tracking Improves Cloze-style Reading Comprehension (Luong Hoang - 5 October, 2018)
Recent work has shown that relatively simple neural methods such as the Attention Sum-Reader can perform well on these tasks; however, these systems still significantly trail human performance. We show that these simple modifications improve performance both independently and in combination, and we outperform the previous state of the art on the LAMBADA dataset, particularly on difficult entity examples.
Link: https://arxiv.org/abs/1810.02891
====================================================
Physics Guided Recurrent Neural Networks For Modeling Dynamical Systems: Application to Monitoring Water Temperature And Quality In Lakes (Xiaowei Jia - 5 October, 2018)
In this paper, we introduce a novel framework for combining scientific knowledge within physics-based models and recurrent neural networks to advance scientific discovery in many dynamical systems. Then, we further incorporate physical knowledge in real-world dynamical systems as additional constraints for training recurrent neural networks
Link: https://arxiv.org/abs/1810.02880
====================================================
Generic Model-Agnostic Convolutional Neural Network for Single Image Dehazing (Zheng Liu - 5 October, 2018)
It is based on designing a fully convolutional neural network to recognize haze structures in input images and restore clear, haze-free images. Somewhat surprisingly, it achieves superior performance relative to all existing state-of-the-art methods for image dehazing even on SOTS outdoor images, which are synthesized using the atmosphere scattering model.
Link: https://arxiv.org/abs/1810.02862
====================================================
Deep Probabilistic Video Compression (Jun Han - 5 October, 2018)
Our model uses advances in variational autoencoders (VAEs) for sequential data and combines it with recent work on neural image compression
Link: https://arxiv.org/abs/1810.02845
====================================================
ResumeNet: A Learning-based Framework for Automatic Resume Quality Assessment (Yong Luo - 5 October, 2018)
Then a neural-network model is designed to predict the quality of each resume, where some text processing techniques are incorporated
Link: https://arxiv.org/abs/1810.02832
====================================================
Interpretable Convolutional Neural Networks via Feedforward Design (C. -C. Jay Kuo - 5 October, 2018)
The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP)
Link: https://arxiv.org/abs/1810.02786
====================================================
TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation (Pengcheng Yin - 5 October, 2018)
We present TRANX, a transition-based neural semantic parser that maps natural language (NL) utterances into formal meaning representations (MRs). Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.
Link: https://arxiv.org/abs/1810.02720
====================================================
Hows and Whys of Artificial Intelligence for Public Sector Decisions: Explanation and Evaluation (Alun Preece - 28 September, 2018)
Recent advances in machine learning (ML) enabled by deep neural networks has exacerbated the challenge of evaluating such software due to the opaque nature of these ML-based artifacts
Link: https://arxiv.org/abs/1810.02689
====================================================
Local Interpretable Model-agnostic Explanations of Bayesian Predictive Models via Kullback-Leibler Projections (Tomi Peltola - 5 October, 2018)
We demonstrate the method in explaining MNIST digit classifications made by a Bayesian deep convolutional neural network.
Link: https://arxiv.org/abs/1810.02678
====================================================
ReTiCaM: Real-time Human Performance Capture from Monocular Video (Marc Habermann - 5 October, 2018)
In the first stage, a skinned template model is jointly fitted to background subtracted input video, 2D and 3D skeleton joint positions found using a deep neural network, and a set of sparse facial landmark detections
Link: https://arxiv.org/abs/1810.02648
====================================================
PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation (Perttu HÃ¤mÃ¤lÃ¤inen - 8 October, 2018)
This is implemented by using separate neural networks for policy mean and variance and training the mean and variance in separate passes
Link: https://arxiv.org/abs/1810.02541
====================================================
Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods (Peter Henderson - 5 October, 2018)
In deep reinforcement learning (Deep RL), such optimization methods are often used for training neural networks via the temporal difference error or policy gradient
Link: https://arxiv.org/abs/1810.02525
====================================================
AutoLoss: Learning Discrete Schedules for Alternate Optimization (Haowen Xu - 4 October, 2018)
We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT)
Link: https://arxiv.org/abs/1810.02442
====================================================
The Dynamics of Differential Learning I: Information-Dynamics and Task Reachability (Alessandro Achille - 4 October, 2018)
We study the topology of the space of learning tasks, which is critical to understanding transfer learning whereby a model such as a deep neural network is pre-trained on a task, and then used on a different one after some fine-tuning. Our analysis also explains more complex phenomena where semantically similar tasks may be unreachable from one another, a phenomenon called Information Plasticity and observed in diverse learning systems such as animals and deep neural networks.
Link: https://arxiv.org/abs/1810.02440
====================================================
SNIP: Single-shot Network Pruning based on Connection Sensitivity (Namhoon Lee - 4 October, 2018)
Pruning large neural networks while maintaining the performance is often highly desirable due to the reduced space and time complexity
Link: https://arxiv.org/abs/1810.02340
====================================================
A Practical Approach to Sizing Neural Networks (Gerald Friedland - 4 October, 2018)
Second, we introduce and experimentally validate a heuristic method to estimate the neural network capacity requirement for a given dataset and labeling. This allows an estimate of the required size of a neural network for a given problem
Link: https://arxiv.org/abs/1810.02328
====================================================
Learning Compressed Transforms with Low Displacement Rank (Anna T. Thomas - 4 October, 2018)
We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters.
Link: https://arxiv.org/abs/1810.02309
====================================================
Multi-directional Geodesic Neural Networks via Equivariant Convolution (Adrien Poulenard - 1 October, 2018)
As a result, rather than trying to fix a canonical orientation or only keeping the maximal response across all alignments of a 2D template at every point of the surface, as done in previous works, we show how information across all rotations can be kept across different layers of the neural network
Link: https://arxiv.org/abs/1810.02303
====================================================
Brain2Object: Printing Your Mind from Brain Signals with Spatial Correlation Embedding (Xiang Zhang - 4 October, 2018)
We propose a unified training framework which combines multi-class Common Spatial Pattern and deep Convolutional Neural Networks to support the backend computation. The experiment results show that our approach consistently outperforms a wide range of baseline and state-of-the-art approaches
Link: https://arxiv.org/abs/1810.02223
====================================================
Neural Networks for Cross-lingual Negation Scope Detection (Federico Fancellu - 4 October, 2018)
Could a model that detects negation scope be applied to a language that it hasn't been trained on? We develop neural models that learn from cross-lingual word embeddings or universal dependencies in English, and test them on Chinese, showing that they work surprisingly well
Link: https://arxiv.org/abs/1810.02156
====================================================
Dual Convolutional Neural Network for Graph of Graphs Link Prediction (Shonosuke Harada - 4 October, 2018)
The recent advances in graph neural networks have made automatic and flexible feature extraction from graphs possible and have improved the predictive performance significantly. We propose a dual convolutional neural network that extracts node representations by combining the external and internal graph structures in an end-to-end manner
Link: https://arxiv.org/abs/1810.02080
====================================================
Finding Solutions to Generative Adversarial Privacy (Dae Hyun Kim - 4 October, 2018)
We present heuristics for solving the maximin problem induced by the generative adversarial privacy setting for linear and convolutional neural network (CNN) adversaries. In the CNN adversary setting, we present a method of hiding selected information from the adversary while preserving the others through alternately optimizing the goals of the privatizer and the adversary using neural network backpropagation
Link: https://arxiv.org/abs/1810.02069
====================================================
Gradient Descent Provably Optimizes Over-parameterized Neural Networks (Simon S. Du - 4 October, 2018)
This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.
Link: https://arxiv.org/abs/1810.02054
====================================================
Learning Bidirectional LSTM Networks for Synthesizing 3D Mesh Animation Sequences (Yi-Ling Qiao - 3 October, 2018)
In this paper, we present a novel method for learning to synthesize 3D mesh animation sequences with long short-term memory (LSTM) blocks and mesh-based convolutional neural networks (CNNs). To address these, we utilize convolutional neural networks defined on triangular meshes along with a shape deformation representation to extract useful features, followed by LSTM cells that iteratively process the features
Link: https://arxiv.org/abs/1810.02042
====================================================
Detecting DGA domains with recurrent neural networks and side information (Ryan R. Curtin - 3 October, 2018)
We then describe our new modeling approach, which is a combination of a novel recurrent neural network architecture with domain registration side information. The model's performance compared to the state of the art is best for DGA families that resemble English words
Link: https://arxiv.org/abs/1810.02023
====================================================
Transfer Incremental Learning using Data Augmentation (Ghouthi Boukli Hacene - 3 October, 2018)
Combining the outstanding performances of Deep Neural Networks (DNNs) with the flexibility of incremental learning techniques is a promising venue of research
Link: https://arxiv.org/abs/1810.02020
====================================================
Image and Encoded Text Fusion for Multi-Modal Classification (Ignazio Gallo - 3 October, 2018)
Deep neural networks have been successfully employed for these approaches. To learn feature representations of resulting images, standard Convolutional Neural Networks (CNNs) are employed for the classification task
Link: https://arxiv.org/abs/1810.02001
====================================================
Verification for Machine Learning, Autonomy, and Neural Networks Survey (Weiming Xiang - 3 October, 2018)
Autonomy in CPS is enabling by recent advances in artificial intelligence (AI) and machine learning (ML) through approaches such as deep neural networks (DNNs), embedded in so-called learning enabled components (LECs) that accomplish tasks from classification to control
Link: https://arxiv.org/abs/1810.01989
====================================================
PADDIT: Probabilistic Augmentation of Data using Diffeomorphic Image Transformation (Mauricio Orbes Arteaga - 3 October, 2018)
For proper generalization performance of convolutional neural networks (CNNs) in medical image segmentation, the learnt features should be invariant under particular non-linear shape variations of the input
Link: https://arxiv.org/abs/1810.01928
====================================================
Combining Natural Gradient with Hessian Free Methods for Sequence Training (Adnan Haider - 3 October, 2018)
This paper presents a new optimisation approach to train Deep Neural Networks (DNNs) with discriminative sequence criteria. The efficacy of the method is shown within a Hessian Free (HF) style optimisation framework to sequence train both standard fully-connected DNNs and Time Delay Neural Networks as speech recognition acoustic models
Link: https://arxiv.org/abs/1810.01873
====================================================
Inhibited Softmax for Uncertainty Estimation in Neural Networks (Marcin MoÅ¼ejko - 3 October, 2018)
We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output
Link: https://arxiv.org/abs/1810.01861
====================================================
GINN: Geometric Illustration of Neural Networks (Luke N. Darlow - 2 October, 2018)
This informal technical report details the geometric illustration of decision boundaries for ReLU units in a three layer fully connected neural network. The Geometric Illustration of Neural Networks (GINN) tool was built to visualise and track the points at which ReLU units switch from being active to off (or vice versa) as the network undergoes training
Link: https://arxiv.org/abs/1810.01860
====================================================
Weighted Sigmoid Gate Unit for an Activation Function of Deep Neural Network (Masayuki Tanaka - 3 October, 2018)
An activation function has crucial role in a deep neural network.
Link: https://arxiv.org/abs/1810.01829
====================================================
A characterization of the Edge of Criticality in Binary Echo State Networks (Pietro Verzelli - 3 October, 2018)
Echo State Networks (ESNs) are simplified recurrent neural network models composed of a reservoir and a linear, trainable readout layer
Link: https://arxiv.org/abs/1810.01742
====================================================
A Comparative Study of Neural Network Models for Sentence Classification (Phuong Le-Hong - 3 October, 2018)
We also show the superiority of convolutional neural network models on a Vietnamese newspaper sentence dataset over strong baseline models. Our experimental results suggest some good practices for applying neural network models in sentence classification.
Link: https://arxiv.org/abs/1810.01656
====================================================
Optimization Algorithm Inspired Deep Neural Network Structure Design (Huan Li - 3 October, 2018)
In this paper, we propose the hypothesis that the neural network structure design can be inspired by optimization algorithms and a faster optimization algorithm may lead to a better neural network structure. Specifically, we prove that the propagation in the feedforward neural network with the same linear transformation in different layers is equivalent to minimizing some function using the gradient descent algorithm
Link: https://arxiv.org/abs/1810.01638
====================================================
Primitive Fitting Using Deep Boundary Aware Geometric Segmentation (Duanshun Li - 3 October, 2018)
Inspired by the corresponding human recognition process, and benefiting from the recent advancements in image semantic segmentation using deep neural networks, we propose BAGSFit as a new framework addressing this problem. Firstly, through a fully convolutional neural network, the input point cloud is point-wisely segmented into multiple classes divided by jointly detected instance boundaries without any geometric fitting
Link: https://arxiv.org/abs/1810.01604
====================================================
Deep Fundamental Matrix Estimation without Correspondences (Omid Poursaeed - 2 October, 2018)
In this paper, we propose novel neural network architectures to estimate fundamental matrices in an end-to-end manner without relying on point correspondences
Link: https://arxiv.org/abs/1810.01575
====================================================
Representation Flow for Action Recognition (AJ Piergiovanni - 2 October, 2018)
Our representation flow layer is a fully-differentiable layer designed to optimally capture the `flow' of any representation channel within a convolutional neural network
Link: https://arxiv.org/abs/1810.01455
====================================================
GLAD: GLocalized Anomaly Detection via Active Feature Space Suppression (Shubhomoy Das - 8 October, 2018)
The key idea is to place a uniform prior over the input feature space for each member of the anomaly detection ensemble via a neural network trained on unlabeled instances, and tune the weights of the neural network to adjust the local relevance of each ensemble member using all labeled instances
Link: https://arxiv.org/abs/1810.01403
====================================================
Multi-scale Convolution Aggregation and Stochastic Feature Reuse for DenseNets (Mingjie Wang - 2 October, 2018)
Recently, Convolution Neural Networks (CNNs) obtained huge success in numerous vision tasks
Link: https://arxiv.org/abs/1810.01373
====================================================
FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models (Will Grathwohl - 3 October, 2018)
The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.
Link: https://arxiv.org/abs/1810.01367
====================================================
Learning with Random Learning Rates (LÃ©onard Blier - 3 October, 2018)
We present the 'All Learning Rates At Once' (Alrao) optimization method for neural networks: each unit or feature in the network gets its own learning rate sampled from a random distribution spanning several orders of magnitude
Link: https://arxiv.org/abs/1810.01322
====================================================
A Lightweight Music Texture Transfer System (Xutan Peng - 27 September, 2018)
However, present methods for music feature transfer using neural networks are far from practical application
Link: https://arxiv.org/abs/1810.01248
====================================================
Predicate learning in neural systems: Discovering latent generative structures (Andrea E. Martin - 2 October, 2018)
During the process of predicate learning, an artificial neural network exploits the naturally occurring dynamic properties of distributed computing across neuronal assemblies in order to learn predicates, but also to combine them compositionally, two computational aspects which appear to be necessary for human behavior as per formal theories in multiple domains. We describe how predicates can be combined generatively using neural oscillations to achieve human-like extrapolation and compositionality in an artificial neural network
Link: https://arxiv.org/abs/1810.01127
====================================================
Robust Optimization through Neuroevolution (Paolo Pagliuca - 2 October, 2018)
The comparison of different algorithms indicates that the CMA-ES and xNES methods, that operate by optimizing a distribution of parameters, represent the best options for the evolution of robust neural network controllers.
Link: https://arxiv.org/abs/1810.01125
====================================================
The Dreaming Variational Autoencoder for Reinforcement Learning Environments (Per-Arne Andersen - 2 October, 2018)
There are several challenges in the current state-of-the-art reinforcement learning algorithms that prevent them from converging towards the global optima. This paper presents The Dreaming Variational Autoencoder (DVAE), a neural network based generative modeling architecture for exploration in environments with sparse feedback
Link: https://arxiv.org/abs/1810.01112
====================================================
Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning (Charles H. Martin - 2 October, 2018)
Random Matrix Theory (RMT) is applied to analyze weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems
Link: https://arxiv.org/abs/1810.01075
====================================================
Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing Power Devices (Zhengyi Luo - 2 October, 2018)
By offloading the computation work to the cloud, no dedicated hardware is needed to enable deep neural networks on existing low computing power devices
Link: https://arxiv.org/abs/1810.01069
====================================================
Improving Sentence Representations with Multi-view Frameworks (Shuai Tang - 2 October, 2018)
In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model
Link: https://arxiv.org/abs/1810.01064
====================================================
PhotoSafer: Content-Based and Context-Aware Private Photo Protection for Smartphones (Ang Li - 1 October, 2018)
PhotoSafer can detect private photos based on photo content with a well-trained deep convolutional neural network, and control access to photos based on system status (e.g., screen locked or not) and app-running status (e.g., app in the background)
Link: https://arxiv.org/abs/1810.01046
====================================================
Large batch size training of neural networks with adversarial training and second-order information (Zhewei Yao - 1 October, 2018)
Stochastic Gradient Descent (SGD) methods using randomly selected batches are widely-used to train neural network (NN) models
Link: https://arxiv.org/abs/1810.01021
====================================================
Distributional Semantics Approach to Detect Intent in Twitter Conversations on Sexual Assaults (Rahul Pandey - 1 October, 2018)
We then present and evaluate a malicious intent classification model for a Twitter post using semantic features of the intent senses learned with the help of convolutional neural networks
Link: https://arxiv.org/abs/1810.01012
====================================================
Inertial-aided Motion Deblurring with Deep Networks (Janne Mustaniemi - 1 October, 2018)
We propose an inertial-aided deblurring method that incorporates gyroscope measurements into a convolutional neural network (CNN). The evaluation shows a clear improvement in visual quality over the state-of-the-art while achieving real-time performance
Link: https://arxiv.org/abs/1810.00986
====================================================
Neural Regression Trees (Shahan Ali Memon - 1 October, 2018)
We propose a neural regression tree model for RvC. We empirically show the validity of our model by testing it on two challenging regression tasks where we establish the state of the art.
Link: https://arxiv.org/abs/1810.00974
====================================================
Error correction in quantum cryptography based on artificial neural networks (Marcin Niemiec - 1 October, 2018)
It has been shown that the synchronization process in the new solution is much faster than in the analogous scenario used in neural cryptography. This feature significantly increases the level of security because a potential eavesdropper cannot effectively synchronize their own artificial neural networks in order to obtain information about the key
Link: https://arxiv.org/abs/1810.00957
====================================================
Extending Stan for Deep Probabilistic Programming (Javier Burroni - 30 September, 2018)
Deep probabilistic programming combines deep neural networks (for automatic hierarchical representation learning) with probabilistic models (for principled handling of uncertainty). To ease this task, we extend Stan, a popular high-level probabilistic programming language, to use deep neural networks written in PyTorch
Link: https://arxiv.org/abs/1810.00873
====================================================
Training Machine Learning Models by Regularizing their Explanations (Andrew Slavin Ross - 29 September, 2018)
Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves
Link: https://arxiv.org/abs/1810.00869
====================================================
ProxQuant: Quantized Neural Networks via Proximal Operators (Yu Bai - 8 October, 2018)
To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping
Link: https://arxiv.org/abs/1810.00861
====================================================
Dynamic Sparse Graph for Efficient Deep Learning (Liu Liu - 1 October, 2018)
We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference
Link: https://arxiv.org/abs/1810.00859
====================================================
Multimodal Interactive Learning of Primitive Actions (Tuan Do - 1 October, 2018)
In our previous work, we have used demonstrations captured from humans performing actions as training samples for a neural network-based trajectory model of actions to be performed by a computational agent in novel setups
Link: https://arxiv.org/abs/1810.00838
====================================================
How Powerful are Graph Neural Networks? (Keyulu Xu - 1 October, 2018)
Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.
Link: https://arxiv.org/abs/1810.00826
====================================================
Set Transformer (Juho Lee - 1 October, 2018)
We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set
Link: https://arxiv.org/abs/1810.00825
====================================================
Deep sequential models for sampling-based planning (Yen-Ling Kuo - 1 October, 2018)
The neural-network-based models avoid manual feature engineering by co-training a convolutional network which processes map features and observations from sensors
Link: https://arxiv.org/abs/1810.00804
====================================================
Human Motion Prediction using Adaptable Neural Networks (Yujiao Cheng - 1 October, 2018)
Recursive least square parameter adaptation algorithm (RLS-PAA) is adopted for online parameter adaptation of the neural network and for uncertainty estimation. Experiments on several human motion datasets verify that the proposed method outperforms the state-of-the-art approach with a significant improvement in terms of prediction accuracy and computation efficiency.
Link: https://arxiv.org/abs/1810.00781
====================================================
Throughput Optimizations for FPGA-based Deep Neural Network Inference (ThorbjÃ¶rn Posewsky - 28 September, 2018)
Deep neural networks are an extremely successful and widely used technique for various pattern recognition and machine learning tasks. In this paper, we propose novel architectures for the inference of previously learned and arbitrary deep neural networks on FPGA-based SoCs that are able to overcome these limitations
Link: https://arxiv.org/abs/1810.00722
====================================================
Part-Level Convolutional Neural Networks for Pedestrian Detection Using Saliency and Boundary Box Alignment (Inyong Yun - 1 October, 2018)
To address it, we propose part-level convolutional neural networks (CNN) for pedestrian detection using saliency and boundary box alignment in this paper. Experimental results on various datasets demonstrate that the proposed method remarkably improves accuracy in pedestrian detection and outperforms existing state-of-the-arts in terms of log average miss rate at false position per image (FPPI).
Link: https://arxiv.org/abs/1810.00689
====================================================
Learning Robust, Transferable Sentence Representations for Text Classification (Wasi Uddin Ahmad - 28 September, 2018)
Despite deep recurrent neural networks (RNNs) demonstrate strong performance in text classification, training RNN models are often expensive and requires an extensive collection of annotated data which may not be available
Link: https://arxiv.org/abs/1810.00681
====================================================
Direct optimization of F-measure for retrieval-based personal question answering (Rasool Fakoor - 27 September, 2018)
The problem is framed as a neural retrieval based question answering system where answers are selected from previously stored user memories
Link: https://arxiv.org/abs/1810.00679
====================================================
Attention-based Encoder-Decoder Networks for Spelling and Grammatical Error Correction (Sina Ahmadi - 21 September, 2018)
In particular, we investigate sequence-to-sequence and attention-based models which have recently shown a higher performance than the state-of-the-art of many language processing problems. We demonstrate that neural machine translation models can be successfully applied to the task of error correction.
Link: https://arxiv.org/abs/1810.00660
====================================================
Perfect Match: A Simple Method for Learning Representations For Counterfactual Inference With Neural Networks (Patrick Schwab - 3 October, 2018)
Here, we present Perfect Match (PM), a method for training neural networks for counterfactual inference that is easy to implement, compatible with any architecture, does not add computational complexity or hyperparameters, and extends to any number of treatments. Our experiments demonstrate that PM outperforms a number of more complex state-of-the-art methods in inferring counterfactual outcomes across several real-world and semi-synthetic datasets.
Link: https://arxiv.org/abs/1810.00656
====================================================
Elastic Neural Networks for Classification (Yi Zhou - 5 October, 2018)
In this work we propose a framework for improving the performance of any deep neural network that may suffer from vanishing gradients. The framework - which we name Elastic network - is tested with several well-known networks on CIFAR10 and CIFAR100 datasets, and the experimental results show that the proposed framework improves the accuracy on both shallow networks (e.g., MobileNet) and deep convolutional neural networks (e.g., DenseNet)
Link: https://arxiv.org/abs/1810.00589
====================================================
End-To-End Alzheimer's Disease Diagnosis and Biomarker Identification (Soheil Esmaeilzadeh - 1 October, 2018)
In this paper, we propose a simple 3D Convolutional Neural Networks and exploit its model parameters to tailor the end-to-end architecture for the diagnosis of Alzheimer's disease (AD). Our model can diagnose AD with an accuracy of 94.1\% on the popular ADNI dataset using only MRI data, which outperforms the previous state-of-the-art
Link: https://arxiv.org/abs/1810.00523
====================================================
One Network to Solve All ROIs: Deep Learning CT for Any ROI using Differentiated Backprojection (Yoseob Han - 30 September, 2018)
In this paper, two types of neural networks are designed. Experimental results show that the new type of neural network significantly outperforms the existing iterative methods for any ROI size in spite of significantly reduced run-time complexity
Link: https://arxiv.org/abs/1810.00500
====================================================
Interactive Learning with Corrective Feedback for Policies based on Deep Neural Networks (Rodrigo PÃ©rez-Dattari - 30 September, 2018)
Deep Reinforcement Learning (DRL) has become a powerful strategy to solve complex decision making problems based on Deep Neural Networks (DNNs)
Link: https://arxiv.org/abs/1810.00466
====================================================
Efficient Sequence Labeling with Actor-Critic Training (Saeed Najafi - 30 September, 2018)
Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks
Link: https://arxiv.org/abs/1810.00428
====================================================
Graph Spectral Regularization for Neural Network Interpretability (Alexander Tong - 1 October, 2018)
Deep neural networks can learn meaningful representations of data. Neural networks are able to learn and benefit from much higher dimensional representationsm but these are not visually interpretable because neurons have arbitrary ordering within a layer
Link: https://arxiv.org/abs/1810.00424
====================================================
Nth Absolute Root Mean Error (Siddhartha Dhar Choudhury - 2 October, 2018)
Neural network training process takes long time when the size of training data is huge, without the large set of training values the neural network is unable to learn features. To reduce the time for training a regression model using neural network we introduce a loss function called Nth Absolute Root Mean Error (NARME)
Link: https://arxiv.org/abs/1810.00421
====================================================
Deep, Skinny Neural Networks are not Universal Approximators (Jesse Johnson - 30 September, 2018)
In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate
Link: https://arxiv.org/abs/1810.00393
====================================================
Pixel and Feature Level Based Domain Adaption for Object Detection in Autonomous Driving (Yuhu Shan - 30 September, 2018)
Annotating large scale datasets to train modern convolutional neural networks is prohibitively expensive and time-consuming for many real tasks
Link: https://arxiv.org/abs/1810.00345
====================================================
Learning to Progressively Plan (Xinyun Chen - 2 October, 2018)
The rewriting policy employs a neural network trained with reinforcement learning
Link: https://arxiv.org/abs/1810.00337
====================================================
Multi-Level Contextual Network for Biomedical Image Segmentation (Amirhossein Dadashzadeh - 30 September, 2018)
In this paper, we consider the problem of biomedical image segmentation using deep convolutional neural networks
Link: https://arxiv.org/abs/1810.00327
====================================================
Tree2Tree Neural Translation Model for Learning Source Code Changes (Saikat Chakraborty - 30 September, 2018)
Thus, deploying state-of-the-art NMT models without domain adaptation may poorly serve the purpose. To this end, in this work, we propose a novel Tree2Tree Neural Machine Translation system to model source code changes and learn code change patterns from the wild
Link: https://arxiv.org/abs/1810.00314
====================================================
Posture recognition using an RGB-D camera : exploring 3D body modeling and deep learning approaches (Mohamed El Amine Elforaici - 29 September, 2018)
Convolutional Neural Networks (CNNs) are trained to recognize human postures using transfer learning on RGB and depth images
Link: https://arxiv.org/abs/1810.00308
====================================================
Correlation Propagation Networks for Scene Text Detection (Zichuan Liu - 29 September, 2018)
It is an end-to-end trainable framework engined by advanced Convolutional Neural Networks
Link: https://arxiv.org/abs/1810.00304
====================================================
Pruned and Structurally Sparse Neural Networks (Simon Alford - 29 September, 2018)
Advances in designing and training deep neural networks have led to the principle that the large and deeper a network is, the better it can perform. In this paper we experiment with training on sparse neural network topologies
Link: https://arxiv.org/abs/1810.00299
====================================================
To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression (Yiren Zhao - 29 September, 2018)
As deep neural networks (DNNs) become widely used, pruned and quantised models are becoming ubiquitous on edge devices; such compressed DNNs are popular for lowering computational requirements
Link: https://arxiv.org/abs/1810.00208
====================================================
NICE: Noise Injection and Clamping Estimation for Neural Network Quantization (Chaim Baskin - 2 October, 2018)
The \uniqname method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy. This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and activations
Link: https://arxiv.org/abs/1810.00162
====================================================
Auto-conditioned Recurrent Mixture Density Networks for Complex Trajectory Generation (Hejia Zhang - 1 October, 2018)
Recent advancements in machine learning research have given rise to recurrent neural networks that are able to synthesize high-dimensional motion sequences over long time horizons
Link: https://arxiv.org/abs/1810.00146
====================================================
Interpreting Adversarial Robustness: A View from Decision Surface in Input Space (Fuxun Yu - 29 September, 2018)
One popular hypothesis of neural network generalization is that the flat local minima of loss surface in parameter space leads to good generalization. We then propose an adversarial robustness indicator, which can evaluate a neural network's intrinsic robustness property without testing its accuracy under adversarial attacks
Link: https://arxiv.org/abs/1810.00144
====================================================
Knowledge-guided Semantic Computing Network (Guangming Shi - 28 September, 2018)
Besides, to enhance the recognition ability of the semantic tree in aspects of the diversity, randomicity and variability, we use the traditional neural network to aid the semantic tree to learn some indescribable features. Especially, Our model also has better adversarial robustness than traditional neural network with the help of human knowledge.
Link: https://arxiv.org/abs/1810.00139
====================================================
Visual Object Tracking based on Adaptive Siamese and Motion Estimation Network (Hossein Kashiani - 28 September, 2018)
Recently, convolutional neural network (CNN) has attracted much attention in different areas of computer vision, due to its powerful abstract feature representation. Evaluation results on well-known benchmark datasets (OTB100, OTB50 and OTB2013) prove that the proposed tracker outperforms the state-of-the-art competitors.
Link: https://arxiv.org/abs/1810.00119
====================================================
DeepSSM: A Deep Learning Framework for Statistical Shape Modeling from Raw Images (Riddhish Bhalodia - 28 September, 2018)
DeepSSM uses a convolutional neural network (CNN) that simultaneously localizes the biological structure of interest, establishes correspondences, and projects these points onto a low-dimensional shape representation in the form of PCA loadings within a point distribution model
Link: https://arxiv.org/abs/1810.00111
====================================================
Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout (Kun Wan - 28 September, 2018)
Recently convolutional neural networks (CNNs) achieve great accuracy in visual recognition tasks. Experimental results show that DenseNets with our specialized dropout method yield better accuracy compared to vanilla DenseNet and state-of-the-art CNN models, and such accuracy boost increases with the model depth.
Link: https://arxiv.org/abs/1810.00091
====================================================
Adversarial Domain Adaptation for Stable Brain-Machine Interfaces (Ali Farshchian - 28 September, 2018)
Here, we introduce a new computational approach that decodes movement intent from a low-dimensional latent representation of the neural data. However, implementation of an Adversarial Domain Adaptation Network trained to match the empirical probability distribution of the residuals of the reconstructed neural signals outperforms the two methods based on latent variables, while requiring remarkably few data points to solve the domain adaptation problem.
Link: https://arxiv.org/abs/1810.00045
====================================================
Channel-wise and Spatial Feature Modulation Network for Single Image Super-Resolution (Yanting Hu - 28 September, 2018)
On the other hand, as the depth of neural networks grows, the long-term information coming from preceding layers is easy to be weaken or lost in late layers, which is adverse to super-resolving image. Extensive quantitative and qualitative evaluations on benchmark datasets illustrate the superiority of our proposed method over the state-of-the-art methods.
Link: https://arxiv.org/abs/1809.11130
====================================================
Learning to Remember, Forget and Ignore using Attention Control in Memory (T. S. Jayram - 28 September, 2018)
Typical neural networks with external memory do not effectively separate capacity for episodic and working memory as is required for reasoning in humans. As it shows the same functional characteristics as working memory, it robustly learns psychology inspired tasks and converges faster than comparable state-of-the-art models
Link: https://arxiv.org/abs/1809.11087
====================================================
Spoken Pass-Phrase Verification in the i-vector Space (Hossein Zeinali - 28 September, 2018)
In this paper, we build on our previous work on i-vector based text-dependent speaker verification, where we have shown that i-vectors extracted using phrase specific Hidden Markov Models (HMMs) or using Deep Neural Network (DNN) based bottle-neck (BN) features help to reject utterances with wrong pass-phrases
Link: https://arxiv.org/abs/1809.11068
====================================================
Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels (Bo Han - 28 September, 2018)
To handle these issues, by using the memorization effects of deep neural networks, we may train deep neural networks on the whole dataset only the first few iterations. However, in such training procedures, deep neural networks inevitably memorize some noisy labels, which will degrade their generalization
Link: https://arxiv.org/abs/1809.11008
====================================================
Deep Adaptive Learning for Writer Identification based on Single Handwritten Word Images (Sheng He - 28 September, 2018)
Our proposed method transfers the benefits of the learned features of a convolutional neural network from an auxiliary task such as explicit content recognition to the main task of writer identification in a single procedure. A multi-task neural network with one or several adaptive convolutional layers is trained end-to-end, to exploit robust generic features for a specific main task, i.e., writer identification
Link: https://arxiv.org/abs/1809.10954
====================================================
HyperST-Net: Hypernetworks for Spatio-Temporal Forecasting (Zheyi Pan - 28 September, 2018)
Then, we design a general form of HyperST layer as well as different forms for several basic layers in neural networks, including the dense layer (HyperST-Dense) and the convolutional layer (HyperST-Conv). Experiments on three types of real-world tasks demonstrate that the predictive models integrated with our framework achieve significant improvements, and outperform the state-of-the-art baselines as well.
Link: https://arxiv.org/abs/1809.10889
====================================================
Confidence Calibration in Deep Neural Networks through Stochastic Inferences (Seonguk Seo - 1 October, 2018)
The proposed loss function enables us to learn deep neural networks that predict confidence calibrated scores using a single inference. Our algorithm presents outstanding confidence calibration performance and improves classification accuracy with two popular stochastic regularization techniques---stochastic depth and dropout---in multiple models and datasets; it alleviates overconfidence issue in deep neural networks significantly by training networks to achieve prediction accuracy proportional to confidence of prediction.
Link: https://arxiv.org/abs/1809.10877
====================================================
Characterizing Audio Adversarial Examples Using Temporal Dependency (Zhuolin Yang - 28 September, 2018)
Recent studies have highlighted adversarial examples as a ubiquitous threat to different neural network models and many downstream applications
Link: https://arxiv.org/abs/1809.10875
====================================================
The Rule of Three: Abstractive Text Summarization in Three Bullet Points (Tomonori Kodaira - 28 September, 2018)
Neural network-based approaches have become widespread for abstractive text summarization. Therefore, we use a dataset containing summaries with only three bullet points, and propose a neural network-based abstractive summarization model that considers the information structures of the generated summaries
Link: https://arxiv.org/abs/1809.10867
====================================================
Embedded-State Latent Conditional Random Fields for Sequence Labeling (Dung Thai - 27 September, 2018)
Recently, it has become common to locally parametrize these models using rich features extracted by recurrent neural networks (such as LSTM), while enforcing consistent outputs through a simple linear-chain model, representing Markovian dependencies between successive labels
Link: https://arxiv.org/abs/1809.10835
====================================================
A theoretical framework for deep locally connected ReLU network (Yuandong Tian - 27 September, 2018)
Understanding theoretical properties of deep and locally connected nonlinear network, such as deep convolutional neural network (DCNN), is still a hard problem despite its empirical success
Link: https://arxiv.org/abs/1809.10829
====================================================
Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped (Tianyu Li - 27 September, 2018)
This has several advantages, as the structure preserves the intuitive nature of the policy, and the neural network improves the performance of the hand-designed policy. In this way, we propose a way of using neural networks to improve expert designed controllers, while maintaining ease of understanding.
Link: https://arxiv.org/abs/1809.10811
====================================================
Complexity of Training ReLU Neural Network (Digvijay Boob - 27 September, 2018)
We show that it is NP-hard to train a two- hidden layer feedforward ReLU neural network. We also show that if sufficient over-parameterization is provided in the first hidden layer of ReLU neural network then there is a polynomial time algorithm which finds weights such that output of the over-parameterized ReLU neural network matches with the output of the given data
Link: https://arxiv.org/abs/1809.10787
====================================================
On the loss landscape of a class of deep neural networks with no bad local valleys (Quynh Nguyen - 27 September, 2018)
We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero
Link: https://arxiv.org/abs/1809.10749
====================================================
Introducing Noise in Decentralized Training of Neural Networks (Linara Adilova - 27 September, 2018)
We investigate the effects of noise injection into the neural networks during a decentralized training process. However for non-linear neural networks we empirically show that noise injection substantially improves model quality helping to reach a generalization ability of a local model close to the serial baseline.
Link: https://arxiv.org/abs/1809.10678
====================================================
Predictive Embeddings for Hate Speech Detection on Twitter (Rohan Kshirsagar - 27 September, 2018)
We present a neural-network based approach to classifying online hate speech in general, as well as racist and sexist speech in particular. Our models match or outperform state of the art F1 performance on all three datasets using significantly fewer parameters and minimal feature preprocessing compared to previous methods.
Link: https://arxiv.org/abs/1809.10644
====================================================
Generative replay with feedback connections as a general strategy for continual learning (Gido M. van de Ven - 27 September, 2018)
Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning problematic
Link: https://arxiv.org/abs/1809.10635
====================================================
Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural Networks (Andrew Anderson - 27 September, 2018)
Quantization of weights and activations in Deep Neural Networks (DNNs) is a powerful technique for network compression, and has enjoyed significant attention and success
Link: https://arxiv.org/abs/1809.10572
====================================================
Cross-Layer Effects on Training Neural Algorithms for Video Streaming (Pablo Gil Pereira - 27 September, 2018)
In this paper, we demonstrate that the performance of the trained ABR algorithms depends on the implementation of the simulated environment used to train the neural network
Link: https://arxiv.org/abs/1809.10537
====================================================
The Convergence of Sparsified Gradient Methods (Dan Alistarh - 27 September, 2018)
Distributed training of massive machine learning models, in particular deep neural networks, via Stochastic Gradient Descent (SGD) is becoming commonplace
Link: https://arxiv.org/abs/1809.10505
====================================================
Learning to Train a Binary Neural Network (Joseph Bethge - 27 September, 2018)
However, understanding binary neural networks and training accurate models for practical applications remains a challenge. Within this framework, we systematically evaluated different network architectures and hyperparameters to provide useful insights on how to train a binary neural network
Link: https://arxiv.org/abs/1809.10463
====================================================
Deformable Object Tracking with Gated Fusion (Wenxi Liu - 27 September, 2018)
The tracking-by-detection framework receives growing attentions through the integration with the Convolutional Neural Network (CNN). Extensive experiments on the standard benchmarks show that the proposed tracker performs favorably against state-of-the-art methods.
Link: https://arxiv.org/abs/1809.10417
====================================================
3D Face Synthesis Driven by Personality Impression (Yining Lang - 27 September, 2018)
In the first step, we train classifiers using deep convolutional neural networks on a dataset of images with personality impression annotations, which are capable of predicting the personality impression of a face
Link: https://arxiv.org/abs/1809.10402
====================================================
Smooth Inter-layer Propagation of Stabilized Neural Networks for Classification (Jingfeng Zhang - 28 September, 2018)
Recent work has studied the reasons for the remarkable performance of deep neural networks in image classification
Link: https://arxiv.org/abs/1809.10315
====================================================
Adding Neural Network Controllers to Behavior Trees without Destroying Performance Guarantees (Christopher Iliffe Sprague - 26 September, 2018)
In this paper, we show how controllers created using data driven designs, such as neural networks, can be used together with model based controllers in a way that combines the performance guarantees of the model based controllers with the efficiency of the data driven controllers
Link: https://arxiv.org/abs/1809.10283
====================================================
Cylindrical Transform: 3D Semantic Segmentation of Kidneys With Limited Annotated Images (Hojjat Salehinejad - 24 September, 2018)
This approach enables us to train contemporary classification deep convolutional neural networks (DCNNs) instead of fully convolutional networks (FCNs) for semantic segmentation
Link: https://arxiv.org/abs/1809.10245
====================================================
Autonomously and Simultaneously Refining Deep Neural Network Parameters by a Bi-Generative Adversarial Network Aided Genetic Algorithm (Yantao Lu - 24 September, 2018)
Our proposed approach can be used to autonomously refine the number of convolutional layers and dense layers, number and size of kernels, and the number of neurons for the dense layers; choose the type of the activation function; and decide whether to use dropout and batch normalization or not, to improve the accuracy of different deep neural network architectures. The results show that the presented approach can simultaneously and successfully optimize multiple neural network parameters, and achieve higher accuracy even with shallower networks.
Link: https://arxiv.org/abs/1809.10244
====================================================
Leveraging Transfer Learning for Segmenting Lesions and their Attributes in Dermoscopy Images (Navid Alemi Koohbanani - 23 September, 2018)
In this paper, we propose a framework by incorporating transfer learning for segmenting lesions and their attributes based on the convolutional neural networks
Link: https://arxiv.org/abs/1809.10243
====================================================
Compressing the Input for CNNs with the First-Order Scattering Transform (Edouard Oyallon - 27 September, 2018)
We study the first-order scattering transform as a candidate for reducing the signal processed by a convolutional neural network (CNN)
Link: https://arxiv.org/abs/1809.10200
====================================================
Recent progress in semantic image segmentation (Xiaolong Liu - 19 September, 2018)
Since the emergence of Deep Neural Network (DNN), segmentation has made a tremendous progress
Link: https://arxiv.org/abs/1809.10198
====================================================
Photometric Depth Super-Resolution (Bjoern Haefner - 26 September, 2018)
It is then shown that this dependency upon a specific reflectance model can be relaxed by focusing on a specific class of objects (e.g., faces), and delegate reflectance estimation to a deep neural network
Link: https://arxiv.org/abs/1809.10097
====================================================
Unsupervised Adversarial Invariance (Ayush Jaiswal - 26 September, 2018)
We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation
Link: https://arxiv.org/abs/1809.10083
====================================================
Rediscovering Deep Neural Networks in Finite-State Distributions (Amir Emad Marvasti - 26 September, 2018)
We propose a new way of thinking about deep neural networks, in which the linear and non-linear components of the network are naturally derived and justified in terms of principles in probability theory. Unlike existing designs that rely on heuristics, the proposed framework restricts subjective interpretations of CNNs and sheds light on the functionality of neural networks from a completely new perspective.
Link: https://arxiv.org/abs/1809.10073
====================================================
Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis (Kelly W. Zhang - 26 September, 2018)
Recent work using auxiliary prediction task classifiers to investigate the properties of LSTM representations has begun to shed light on why pretrained representations, like ELMo (Peters et al., 2018) and CoVe (McCann et al., 2017), are so beneficial for neural language understanding models
Link: https://arxiv.org/abs/1809.10040
====================================================
Using Neural Networks to Generate Information Maps for Mobile Sensors (Louis Dressel - 26 September, 2018)
We propose using convolutional neural networks to generate information maps from a target estimate and sensor model in real-time
Link: https://arxiv.org/abs/1809.10012
====================================================
Hierarchy-based Image Embeddings for Semantic Image Retrieval (BjÃ¶rn Barz - 26 September, 2018)
Deep neural networks trained for classification have been found to learn powerful image representations, which are also often used for other tasks such as comparing images w.r.t
Link: https://arxiv.org/abs/1809.09924
====================================================
Night-to-Day Image Translation for Retrieval-based Localization (Asha Anoosheh - 25 September, 2018)
day and night, is still a problem with unsatisfactory results, even in this age of powerful neural models. A recent class of neural models allows for realistic translation of images among visual domains with relatively little training data and, most importantly, without ground-truth pairings.
Link: https://arxiv.org/abs/1809.09767
====================================================
PhotoShape: Photorealistic Materials for Large-Scale Shape Collections (Keunhong Park - 25 September, 2018)
By generating a large number of synthetic renderings, we train a convolutional neural network to classify materials in real photos, and employ 3D-2D alignment techniques to transfer materials to different parts of each shape model
Link: https://arxiv.org/abs/1809.09761
====================================================
A Gradient-Based Split Criterion for Highly Accurate and Transparent Model Trees (Klaus Broelemann - 25 September, 2018)
State-of-the-art methods, such as neural networks and ensemble methods, often result in highly complex models that offer little transparency.
Link: https://arxiv.org/abs/1809.09703
====================================================
BanditSum: Extractive Summarization as a Contextual Bandit (Yue Dong - 25 September, 2018)
In this work, we propose a novel method for training neural networks to perform single-document extractive summarization without heuristically-generated extractive labels. We perform a series of experiments demonstrating that BanditSum is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches
Link: https://arxiv.org/abs/1809.09672
====================================================
Non-native children speech recognition through transfer learning (Marco Matassoni - 25 September, 2018)
This work deals with non-native children's speech and investigates both multi-task and transfer learning approaches to adapt a multi-language Deep Neural Network (DNN) to speakers, specifically children, learning a foreign language
Link: https://arxiv.org/abs/1809.09658
====================================================
Deep Neural Networks for Pattern Recognition (Kyongsik Yun - 25 September, 2018)
This chapter introduces the basic structure of deep neural networks that simulate human neural networks. Finally, recent developments in training strategies for effective learning of complex deep neural networks are addressed.
Link: https://arxiv.org/abs/1809.09645
====================================================
Automatic brain tumor grading from MRI data using convolutional neural networks and quality assessment (Sergio Pereira - 25 September, 2018)
In this paper, we propose to use Convolutional Neural Networks for predicting tumor grade directly from imaging data
Link: https://arxiv.org/abs/1809.09468
====================================================
Temporal Relational Ranking for Stock Prediction (Fuli Feng - 25 September, 2018)
With the recent success of deep neural networks in modeling sequential data, deep learning has become a promising choice for stock prediction.
Link: https://arxiv.org/abs/1809.09441
====================================================
TTMF: A Triple Trustworthiness Measurement Frame for Knowledge Graphs (Shengbin Jia - 25 September, 2018)
The framework is a crisscrossing neural network structure
Link: https://arxiv.org/abs/1809.09414
====================================================
Supervised Neural Models Revitalize the Open Relation Extraction (Shengbin Jia - 25 September, 2018)
A hybrid neural sequence tagging model (NST) is proposed which combines BiLSTM, CNN and CRF to capture the contextual temporal information, local spatial information, and sentence level tag information of the sequence by using the word and part-of-speech embeddings. Experiments on multiple datasets show that our method is better than most of the existing pattern-based methods and other neural networks based models.
Link: https://arxiv.org/abs/1809.09408
====================================================
Hypergraph Neural Networks (Yifan Feng - 25 September, 2018)
In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-the-art methods
Link: https://arxiv.org/abs/1809.09401
====================================================
Non-Iterative Knowledge Fusion in Deep Convolutional Neural Networks (Mikhail Iu. Leontev - 25 September, 2018)
One method is based on a simple operation of summation of weights of constituent neural networks. The efficiency of the methods is quantified on several publicly available data sets in classification tasks both for shallow and deep neural networks.
Link: https://arxiv.org/abs/1809.09399
====================================================
Scenic: Language-Based Scene Generation (Daniel J. Fremont - 24 September, 2018)
Synthetic data has proved increasingly useful in both training and testing machine learning models such as neural networks. We implement an improviser for Scenic scenarios and apply it in a case study generating synthetic data sets for a convolutional neural network designed to detect cars in road images
Link: https://arxiv.org/abs/1809.09310
====================================================
No Multiplication? No Floating Point? No Problem! Training Networks for Efficient Inference (Shumeet Baluja - 28 September, 2018)
For successful deployment of deep neural networks on highly--resource-constrained devices (hearing aids, earbuds, wearables), we must simplify the types of operations and the memory/power resources used during inference
Link: https://arxiv.org/abs/1809.09244
====================================================
Zoom-RNN: A Novel Method for Person Recognition Using Recurrent Neural Networks (Sina Mokhtarzadeh Azar - 25 September, 2018)
Hence, we present Zoom-RNN, a novel method based on recurrent neural networks for combining evidence extracted from the whole body, upper body, and head regions
Link: https://arxiv.org/abs/1809.09189
====================================================
Text Summarization as Tree Transduction by Top-Down TreeLSTM (Davide Bacciu - 24 September, 2018)
Motivated by this, we introduce a deep neural model for learning structure-to-substructure tree transductions by extending the standard Long Short-Term Memory, considering the parent-child relationships in the structural recursion. The proposed model can achieve state of the art performance on sentence compression benchmarks, both in terms of accuracy and compression rate.
Link: https://arxiv.org/abs/1809.09096
====================================================
Sparse-to-Continuous: Enhancing Monocular Depth Estimation using Occupancy Maps (NÃ­colas Rosa - 24 September, 2018)
This paper addresses the problem of single image depth estimation (SIDE), focusing on improving the accuracy of deep neural network predictions. Rather than modifying the neural network structure to deal with sparse depth maps, this paper introduces a novel technique for the densification of depth maps based on the Hilbert Maps framework
Link: https://arxiv.org/abs/1809.09061
====================================================
On The Utility of Conditional Generation Based Mutual Information for Characterizing Adversarial Subspaces (Chia-Yi Hsu - 24 September, 2018)
The robustness of neural networks has been studied extensively in the context of adversary detection, which compares a metric that exhibits strong discriminate power between natural and adversarial examples
Link: https://arxiv.org/abs/1809.08986
====================================================
Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings (Shafiq Joty - 24 September, 2018)
We use deep neural networks (DNNs) to learn meaningful task-specific embeddings, which we then incorporate into a conditional random field (CRF) model for the multitask setting, performing joint learning over a complex graph structure
Link: https://arxiv.org/abs/1809.08928
====================================================
Understanding Compressive Adversarial Privacy (Xiao Chen - 2 October, 2018)
We then build a more realistic data releasing mechanism that can rely on a nonlinear compression model while the attacker uses a neural network
Link: https://arxiv.org/abs/1809.08911
====================================================
Language Identification with Deep Bottleneck Features (Zhanyu Ma - 18 September, 2018)
In this paper we proposed an end-to-end short utterances speech language identification(SLD) approach based on a Long Short Term Memory (LSTM) neural network which is special suitable for SLD application in intelligent vehicles. Bottle-neck features of a deep neural network (DNN) which are trained for mandarin acoustic-phonetic classification are used for LSTM training
Link: https://arxiv.org/abs/1809.08909
====================================================
Neural network approach to classifying alarming student responses to online assessment (Christopher M. Ormerod - 20 September, 2018)
Our neural network models have been designed to help identify these anomalous responses from a large collection of typical responses that students give. The responses identified by the neural network can be assessed for urgency, severity, and validity more quickly by a team of reviewers than otherwise possible
Link: https://arxiv.org/abs/1809.08899
====================================================
Deformable Stacked Structure for Named Entity Recognition (Shuyang Cao - 28 September, 2018)
Currently, the dominating architecture consists of a bi-directional recurrent neural network (RNN) as the encoder and a conditional random field (CRF) as the decoder. Our model achieves the state-of-the-art performances on the OntoNotes dataset.
Link: https://arxiv.org/abs/1809.08730
====================================================
Deep Knowledge Tracing and Dynamic Student Classification for Knowledge Tracing (Sein Minn - 23 September, 2018)
In this paper, we propose a novel model for knowledge tracing that i) captures students' learning ability and dynamically assigns students into distinct groups with similar ability at regular time intervals, and ii) combines this information with a Recurrent Neural Network architecture known as Deep Knowledge Tracing. Experimental results confirm that the proposed model is significantly better at predicting student performance than well known state-of-the-art techniques for student modelling.
Link: https://arxiv.org/abs/1809.08713
====================================================
Monolingual sentence matching for text simplification (Yonghui Huang - 19 September, 2018)
We introduce a convolutional neural network structure to model similarity between two sentences
Link: https://arxiv.org/abs/1809.08703
====================================================
Unsupervised Learning of Dense Optical Flow and Depth from Sparse Event Data (Chengxi Ye - 23 September, 2018)
To tackle this low level vision task, we use a novel encoder-decoder neural network architecture that aggregates multi-level features and addresses the problem at multiple resolutions
Link: https://arxiv.org/abs/1809.08625
====================================================
Neural Arithmetic Expression Calculator (Kaiyu Chen - 23 September, 2018)
This paper presents a pure neural solver for arithmetic expression calculation (AEC) problem. Previous work utilizes the powerful capabilities of deep neural networks and attempts to build an end-to-end model to solve this problem
Link: https://arxiv.org/abs/1809.08590
====================================================
Exponential Convergence Time of Gradient Descent for One-Dimensional Deep Linear Neural Networks (Ohad Shamir - 27 September, 2018)
In this note, we study the dynamics of gradient descent on objective functions of the form $f(\prod_{i=1}^{k} w_i)$ (with respect to scalar parameters $w_1,\ldots,w_k$), which arise in the context of training depth-$k$ linear neural networks. This highlights a potential obstacle in understanding the convergence of gradient-based methods for deep linear neural networks, where $k$ is large.
Link: https://arxiv.org/abs/1809.08587
====================================================
A Learning Framework for Robust Bin Picking by Customized Grippers (Yongxiang Fan - 23 September, 2018)
The high-level learning-based explorer trains a region-based convolutional neural network (R-CNN) to propose good optimization regions, which avoids ISF getting stuck in bad local optima and improves the collision avoidance performance
Link: https://arxiv.org/abs/1809.08546
====================================================
Artistic Instance-Aware Image Filtering by Convolutional Neural Networks (Milad Tehrani - 22 September, 2018)
Exploitation of neural network vision technologies like object detection and semantic segmentation could be a new viewpoint in this area. In this paper, we utilize an instance segmentation neural network to obtain a class mask for separately filtering the background and foreground of an image
Link: https://arxiv.org/abs/1809.08448
====================================================
Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach (Ravi Kiran Raman - 22 September, 2018)
This framework guarantees not only verifiability of final results, but also the validity of local computations, and its cost-benefit tradeoffs are studied using a synthetic example of training a neural network.
Link: https://arxiv.org/abs/1809.08438
====================================================
Medical Knowledge Embedding Based on Recursive Neural Network for Multi-Disease Diagnosis (Jingchi Jiang - 22 September, 2018)
In this paper, we propose recursive neural knowledge network (RNKN), which combines medical knowledge based on first-order logic with recursive neural network for multi-disease diagnosis
Link: https://arxiv.org/abs/1809.08422
====================================================
Differentiable Unbiased Online Learning to Rank (Harrie Oosterhuis - 22 September, 2018)
Their approaches do not extend well to non-linear models such as neural networks. Our results show that using a neural network leads to even better performance at convergence than a linear model
Link: https://arxiv.org/abs/1809.08415
====================================================
RPNet: an End-to-End Network for Relative Camera Pose Estimation (Sovann En - 22 September, 2018)
This paper addresses the task of relative camera pose estimation from raw image pixels, by means of deep neural networks. While state-of-the-art systems based on SIFT + RANSAC, are able to recover the translation vector only up to scale, RPNet is trained to produce the full translation vector, in an end-to-end way
Link: https://arxiv.org/abs/1809.08402
====================================================
Understanding Fake Faces (Ryota Natsume - 22 September, 2018)
Face recognition research is one of the most active topics in computer vision (CV), and deep neural networks (DNN) are now filling the gap between human-level and computer-driven performance levels in face verification algorithms. This database has two configurations: (i) false positive face detections produced using both the Viola Jones (VJ) method and convolutional neural networks (CNN), and (ii) simulacra that have fundamental characteristics that resemble faces but are completely artificial
Link: https://arxiv.org/abs/1809.08391
====================================================
A Byte-sized Approach to Named Entity Recognition (Emily Sheng - 22 September, 2018)
We introduce a novel, subword approach for named entity recognition (NER) that uses byte-pair encodings (BPE) in combination with convolutional and recurrent neural networks to produce byte-level tags of entities
Link: https://arxiv.org/abs/1809.08386
====================================================
Focus On What's Important: Self-Attention Model for Human Pose Estimation (Guanxiong Sun - 21 September, 2018)
We named it attention convolutional neural network (ACNN)
Link: https://arxiv.org/abs/1809.08371
====================================================
Comment on All-optical machine learning using diffractive deep neural networks (Haiqing Wei - 21 September, 2018)
But interpreting the multilayer diffractive setup as a deep neural network and advocating it as an all-optical deep learning framework are not well justified and represent a mischaracterization of the system by overlooking its defining characteristics of perfect linearity and strict passivity.
Link: https://arxiv.org/abs/1809.08360
====================================================
CPDist: Deep Siamese Networks for Learning Distances Between Structured Preferences (Andrea Loreggia - 21 September, 2018)
We present CPDist, a novel neural network to address the problem of learning to measure the distance between structured preference representations. We use the popular CP-net formalism to represent preferences and then leverage deep neural networks to learn a recently proposed metric function that is computationally hard to compute directly
Link: https://arxiv.org/abs/1809.08350
====================================================
Image Denoising and Super-Resolution using Residual Learning of Deep Convolutional Network (Rohit Pardasani - 21 September, 2018)
Our model nearly replicates the architecture of existing state-of-the-art deep learning models for super-resolution and denoising. We use the proven strategy of residual learning, as supported by state-of-the-art networks in this domain
Link: https://arxiv.org/abs/1809.08229
====================================================
Learning Recommender Systems from Multi-Behavior Data (Chen Gao - 21 September, 2018)
We develop a neural network model to capture the complicated and multi-type interactions between users and items. Extensive experiments on two real-world datasets demonstrate that NMTR significantly outperforms state-of-the-art recommender systems that are designed to learn from both single-behavior data and multi-behavior data
Link: https://arxiv.org/abs/1809.08161
====================================================
Human activity recognition based on time series analysis using U-Net (Yong Zhang - 20 September, 2018)
The experimental results show that compared with Support Vector Machine (SVM), k-Nearest Neighbor (kNN), Decision Tree(DT), Quadratic Discriminant Analysis (QDA), Convolutional Neural Network (CNN) and Fully Convolutional Networks (FCN) methods, our proposal has the highest accuracy and F1-socre in each dataset, and has stable performance and high robustness
Link: https://arxiv.org/abs/1809.08113
====================================================
Power Market Price Forecasting via Deep Learning (Yongli Zhu - 18 September, 2018)
As one of the most successful deep learning frameworks, the LSTM (Long short-term memory) neural network is utilized. The forecasted results demonstrate that, the LSTM deep neural network can outperform the others under different application settings in this problem.
Link: https://arxiv.org/abs/1809.08092
====================================================
Short-term Cognitive Networks, Flexible Reasoning and Nonsynaptic Learning (Gonzalo NÃ¡poles - 16 September, 2018)
Fuzzy Cognitive Maps (FCMs) are neural networks that can be exploited towards this goal because of their flexibility to handle external knowledge. In this paper, we propose a neural network system named Short-term Cognitive Networks that tackle some of these limitations
Link: https://arxiv.org/abs/1809.08085
====================================================
Understanding Convolutional Neural Networks for Text Classification (Alon Jacovi - 21 September, 2018)
We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text
Link: https://arxiv.org/abs/1809.08037
====================================================
LIDAR-Camera Fusion for Road Detection Using Fully Convolutional Neural Networks (Luca Caltagirone - 21 September, 2018)
Several fully convolutional neural networks (FCNs) are then trained to carry out road detection, either by using data from a single sensor, or by using three fusion strategies: early, late, and the newly proposed cross fusion
Link: https://arxiv.org/abs/1809.07941
====================================================
Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes (Peng-Shuai Wang - 20 September, 2018)
We present an Adaptive Octree-based Convolutional Neural Network (Adaptive O-CNN) for efficient 3D shape encoding and decoding
Link: https://arxiv.org/abs/1809.07917
====================================================
Biological plausibility and stochasticity in scalable VO2 active memristor neurons (Wei Yi - 20 September, 2018)
Scalable and biomimetic active memristor neurons and passive memristor synapses form a self-sufficient basis for a transistorless neural network
Link: https://arxiv.org/abs/1809.07867
====================================================
FFT Convolutions are Faster than Winograd on Modern CPUs, Here is Why (Aleksandar Zlateski - 20 September, 2018)
Winograd-based convolution has quickly gained traction as a preferred approach to implement convolutional neural networks (ConvNet) on various hardware platforms because it requires fewer floating point operations than FFT-based or direct convolutions.
Link: https://arxiv.org/abs/1809.07851
====================================================
LSTM-based Whisper Detection (Zeynab Raeesy - 20 September, 2018)
The proposed system consists of a long-short term memory (LSTM) neural network trained on log-filterbank energy (LFBE) acoustic features
Link: https://arxiv.org/abs/1809.07832
====================================================
Recurrent Neural Networks based Obesity Status Prediction Using Activity Data (Qinghan Xue - 20 September, 2018)
While existing machine learning methods such as Recurrent Neural Networks (RNNs) can provide exceptional results, it is challenging to discover hidden patterns of the sequential data due to the irregular observation time instances
Link: https://arxiv.org/abs/1809.07828
====================================================
Dynamic Weights in Multi-Objective Deep Reinforcement Learning (Axel Abels - 20 September, 2018)
However, this earlier work is not feasible for reinforcement learning settings in which the input is high-dimensional, necessitating the use of function approximators, such as neural networks
Link: https://arxiv.org/abs/1809.07803
====================================================
Implementing Adaptive Separable Convolution for Video Frame Interpolation (Mart KartaÅ¡ev - 20 September, 2018)
As Deep Neural Networks are becoming more popular, much of the attention is being devoted to Computer Vision problems that used to be solved with more traditional approaches
Link: https://arxiv.org/abs/1809.07759
====================================================
Symbolic Priors for RNN-based Semantic Parsing (Chunyang Xiao - 20 September, 2018)
Seq2seq models based on Recurrent Neural Networks (RNNs) have recently received a lot of attention in the domain of Semantic Parsing for Question Answering
Link: https://arxiv.org/abs/1809.07721
====================================================
Deep HMResNet Model for Human Activity-Aware Robotic Systems (Hazem Abdelkawy - 1 October, 2018)
The proposed Multichannel 1D Deep Residual Network model is, at the features level, combined with a Bottleneck MLP neural network to automatically extract robust features regardless of the hardware configuration and, at the decision level, is fully connected with an MLP neural network to recognize daily human activities
Link: https://arxiv.org/abs/1809.07624
====================================================
Machine Learning for semi linear PDEs (Quentin Chan-Wai-Nam - 20 September, 2018)
Recent machine learning algorithms dedicated to solving semi-linear PDEs are improved by using different neural network architectures and different parameterizations
Link: https://arxiv.org/abs/1809.07609
====================================================
MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer (Gino Brunner - 20 September, 2018)
We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.
Link: https://arxiv.org/abs/1809.07600
====================================================
Time is of the Essence: Machine Learning-based Intrusion Detection in Industrial Time Series Data (Simon Duque Anton - 20 September, 2018)
Long Short Term Memory-based neural networks perform mediocre while requiring a high training- and parameterisation effort.
Link: https://arxiv.org/abs/1809.07500
====================================================
Learning a Local Feature Descriptor for 3D LiDAR Scans (Ayush Dewan - 20 September, 2018)
The descriptor is learned using a Convolutional Neural Network (CNN)
Link: https://arxiv.org/abs/1809.07494
====================================================
Building Context-aware Clause Representations for Situation Entity Type Classification (Zeyu Dai - 20 September, 2018)
Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph by extensively modeling context influences and inter-dependencies of clauses. Experimental results show that our model achieves the state-of-the-art performance for clause-level situation entity classification on the genre-rich MASC+Wiki corpus, which approaches human-level performance.
Link: https://arxiv.org/abs/1809.07483
====================================================
Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest X-ray Images (Chengsheng Mao - 19 September, 2018)
Unlike the traditional deterministic classifier, a deep generative classifier has a distribution middle layer in the deep neural network. We implemented our deep generative classifiers based on a number of well-known deterministic neural network architectures, and tested our models on the chest X-ray14 dataset
Link: https://arxiv.org/abs/1809.07436
====================================================
Deep Part Induction from Articulated Object Pairs (Li Yi - 19 September, 2018)
Our method learns a neural network architecture with three modules that respectively propose correspondences, estimate 3D deformation flows, and perform segmentation. Our results demonstrate that our method significantly outperforms state-of-the-art techniques in the task of discovering articulated parts of objects
Link: https://arxiv.org/abs/1809.07417
====================================================
Learning, Planning, and Control in a Monolithic Neural Event Inference Architecture (Martin V. Butz - 19 September, 2018)
In a first implementation, a recurrent neural network (RNN) is trained to learn a temporal forward model, which predicts the sensorimotor contingencies of different simulated dynamic vehicles. Meanwhile, the system evaluates the encountered sensorimotor contingencies retrospectively, adapting its neural hidden states for maintaining model coherence
Link: https://arxiv.org/abs/1809.07412
====================================================
Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems (Yu Yao - 19 September, 2018)
We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixel-level observations for future vehicle localization
Link: https://arxiv.org/abs/1809.07408
====================================================
A Dataset for Document Grounded Conversations (Kangyan Zhou - 19 September, 2018)
We describe two neural architectures that provide benchmark performance on the task of generating the next response
Link: https://arxiv.org/abs/1809.07358
====================================================
A Generalized Representer Theorem for Hilbert Space - Valued Functions (Sanket Diwale - 19 September, 2018)
Representer theorems involving explicit basis functions and Reproducing Kernels are a common occurrence in various machine learning algorithms like generalized least squares, support vector machines, Gaussian process regression and kernel based deep neural networks to name a few. The implications of the theorem are presented with examples of multi input-multi output regression, kernel based deep neural networks, stochastic regression and sparsity learning problems as being special cases in this unified view.
Link: https://arxiv.org/abs/1809.07347
====================================================
Accelerating Flash Calculation through Deep Learning Methods (Yu Li - 29 September, 2018)
A detailed introduction from artificial neural networks to deep learning methods is provided here with the authors' own remarks
Link: https://arxiv.org/abs/1809.07311
====================================================
Audio Based Disambiguation Of Music Genre Tags (Romain Hennequin - 19 September, 2018)
These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system
Link: https://arxiv.org/abs/1809.07256
====================================================
Learning Long-range Perception using Self-Supervision from Short-Range Sensors and Odometry (Mirko Nava - 19 September, 2018)
We instantiate and implement the approach on a small mobile robot to detect obstacles at various distances using the video stream of the robot's forward-pointing camera, by training a convolutional neural network on automatically-acquired datasets
Link: https://arxiv.org/abs/1809.07207
====================================================
Capacity Control of ReLU Neural Networks by Basis-path Norm (Shuxin Zheng - 19 September, 2018)
It has been shown that the generalization error bound in terms of the path norm explains the empirical generalization behaviors of the ReLU neural networks better than that of other capacity measures. Motivated by this, we propose a new norm \emph{Basis-path Norm} based on a group of linearly independent paths to measure the capacity of neural networks more accurately
Link: https://arxiv.org/abs/1809.07122
====================================================
Dual Reconstruction Nets for Image Super-Resolution with Gradient Sensitive Loss (Yong Guo - 19 September, 2018)
Deep neural networks have exhibited promising performance in image super-resolution (SR) due to the power in learning the non-linear mapping from low-resolution (LR) images to high-resolution (HR) images
Link: https://arxiv.org/abs/1809.07099
====================================================
Counting the uncountable: deep semantic density estimation from Space (Andres C. Rodriguez - 20 September, 2018)
To distinguish objects of different classes, our approach combines density estimation with semantic segmentation in an end-to-end learnable convolutional neural network (CNN)
Link: https://arxiv.org/abs/1809.07091
====================================================
Detect, anticipate and generate: Semi-supervised recurrent latent variable models for human activity modeling (Judith BÃ¼tepage - 19 September, 2018)
In this work we introduce semi-supervised variational recurrent neural networks which are able to a) model temporal distributions over latent factors and the observable feature space, b) incorporate discrete labels such as activity type when available, and c) generate possible future action sequences on both feature and label level. Our model outperforms state-of-the-art approaches in both activity and affordance detection and anticipation
Link: https://arxiv.org/abs/1809.07075
====================================================
Latent Topic Conversational Models (Tsung-Hsien Wen - 19 September, 2018)
In this paper, we propose Latent Topic Conversational Model (LTCM) which augments seq2seq with a neural latent topic component to better guide response generation and make training easier. The neural topic component encodes information from the source sentence to build a global "topic" distribution over words, which is then consulted by the seq2seq model at each generation step
Link: https://arxiv.org/abs/1809.07070
====================================================
Adversarial Training Towards Robust Multimedia Recommender System (Jinhui Tang - 19 September, 2018)
Owing to the success of deep neural networks in representation learning, recent advance on multimedia recommendation has largely focused on exploring deep learning methods to improve the recommendation accuracy
Link: https://arxiv.org/abs/1809.07062
====================================================
NAIS: Neural Attentive Item Similarity Model for Recommendation (Xiangnan He - 19 September, 2018)
While extensive efforts have been made to use shallow linear models for learning item similarities, there has been relatively less work exploring nonlinear neural network models for item-based CF.
Link: https://arxiv.org/abs/1809.07053
====================================================
Removing the Feature Correlation Effect of Multiplicative Noise (Zijun Zhang - 19 September, 2018)
Multiplicative noise, including dropout, is widely used to regularize deep neural networks (DNNs), and is shown to be effective in a wide range of architectures and tasks
Link: https://arxiv.org/abs/1809.07023
====================================================
Light Field Neural Network (Yuchi Huo - 28 September, 2018)
We introduce an optical neural network system made by off-the-shelf components
Link: https://arxiv.org/abs/1809.07009
====================================================
Improving Moderation of Online Discussions via Interpretable Neural Models (Andrej Å vec - 18 September, 2018)
We propose a neural network based method that partially automates the moderation process
Link: https://arxiv.org/abs/1809.06906
====================================================
SilhoNet: An RGB Method for 3D Object Pose Estimation and Grasp Planning (Gideon Billings - 18 September, 2018)
We use a Convolutional Neural Network (CNN) pipeline that takes in ROI proposals to simultaneously predict an intermediate silhouette representation for objects with an associated occlusion mask
Link: https://arxiv.org/abs/1809.06893
====================================================
Better Conversations by Modeling,Filtering,and Optimizing for Coherence and Diversity (Xinnuo Xu - 18 September, 2018)
Experiments on the OpenSubtitles corpus show a substantial improvement over competitive neural models in terms of BLEU score as well as metrics of coherence and diversity.
Link: https://arxiv.org/abs/1809.06873
====================================================
FRAGE: Frequency-Agnostic Word Representation (Chengyue Gong - 18 September, 2018)
Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models
Link: https://arxiv.org/abs/1809.06858
====================================================
On the Learning Dynamics of Deep Neural Networks (Remi Tachet des Combes - 18 September, 2018)
While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks
Link: https://arxiv.org/abs/1809.06848
====================================================
Albumentations: fast and flexible image augmentations (Alexander Buslaev - 18 September, 2018)
In computer vision domain, image augmentations have become a common implicit regularization technique to combat overfitting in deep convolutional neural networks and are ubiquitously used to improve performance
Link: https://arxiv.org/abs/1809.06839
====================================================
Adversarial Reinforcement Learning for Observer Design in Autonomous Systems under Cyber Attacks (Abhishek Gupta - 15 September, 2018)
We use neural network as function approximator in our studies with the understanding that any other suitable function approximating class can be used within our framework.
Link: https://arxiv.org/abs/1809.06784
====================================================
Visual Diagnostics for Deep Reinforcement Learning Policy Development (Jieliang Luo - 26 September, 2018)
Modern vision-based reinforcement learning techniques often use convolutional neural networks (CNN) as universal function approximators to choose which action to take for a given visual input
Link: https://arxiv.org/abs/1809.06781
====================================================
Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation (Murhaf Fares - 18 September, 2018)
Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.
Link: https://arxiv.org/abs/1809.06748
====================================================
Is rotation forest the best classifier for problems with continuous features? (A. Bagnall - 18 September, 2018)
We evaluate classifiers from three families of algorithms: support vector machines; tree-based ensembles; and neural networks
Link: https://arxiv.org/abs/1809.06705
====================================================
Symbolic Tensor Neural Networks for Digital Media - from Tensor Processing via BNF Graph Rules to CREAMS Applications (Wladyslaw Skarbek - 18 September, 2018)
The BNF induction begins from a collection of neural unit symbols with extra (up to five) decoration fields (including tensor depth and sharing fields). Moreover, the dual BNF rules are specified in order to generate the Dual Symbolic Tensor Neural Network (DSTNN)
Link: https://arxiv.org/abs/1809.06582
====================================================
Runtime Monitoring Neuron Activation Patterns (Chih-Hong Cheng - 19 September, 2018)
For using neural networks in safety critical domains, it is important to know if a decision made by a neural network is supported by prior similarities in training
Link: https://arxiv.org/abs/1809.06573
====================================================
Image Super-Resolution via Deterministic-Stochastic Synthesis and Local Statistical Rectification (Weifeng Ge - 18 September, 2018)
Afterwards, these two images are fused together using another deep neural network that also performs local statistical rectification, which tries to make the local statistics of the fused image match the same local statistics of the groundtruth image. Quantitative results and a user study indicate that the proposed method outperforms existing state-of-the-art algorithms with a clear margin.
Link: https://arxiv.org/abs/1809.06557
====================================================
HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural Networks against Adversarial Malware Samples (Deqiang Li - 17 September, 2018)
In this paper, we present a framework for enhancing the robustness of Deep Neural Networks (DNNs) against adversarial malware samples, dubbed Hashing Transformation Deep Neural Networks} (HashTran-DNN)
Link: https://arxiv.org/abs/1809.06498
====================================================
Towards Deep and Representation Learning for Talent Search at LinkedIn (Rohan Ramanath - 17 September, 2018)
Our key contributions include: (i) Learning semantic representations of sparse entities within the talent search domain, such as recruiter ids, candidate ids, and skill entity ids, for which we utilize neural network models that take advantage of LinkedIn Economic Graph, and (ii) Deep models for learning recruiter engagement and candidate response in talent search applications
Link: https://arxiv.org/abs/1809.06473
====================================================
Self Configuration in Machine Learning (Eugene Wong - 17 September, 2018)
In this paper we first present a class of algorithms for training multi-level neural networks with a quadratic cost function one layer at a time starting from the input layer. With the ability to self-configure and set parameters, we now have more than a fast training algorithm, but the ability to build automatically a fully trained deep neural network starting with nothing more than data.
Link: https://arxiv.org/abs/1809.06463
====================================================
Robustness Guarantees for Bayesian Inference with Gaussian Processes (Luca Cardelli - 17 September, 2018)
We evaluate our techniques on two examples, a GP regression problem and a fully-connected deep neural network, where we rely on weak convergence to GPs to study adversarial examples on the MNIST dataset.
Link: https://arxiv.org/abs/1809.06452
====================================================
Robust Spoken Language Understanding via Paraphrasing (Avik Ray - 17 September, 2018)
State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application
Link: https://arxiv.org/abs/1809.06444
====================================================
DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning (Kashyap Popat - 17 September, 2018)
It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user
Link: https://arxiv.org/abs/1809.06416
====================================================
DÃ©jÃ  Vu: an empirical evaluation of the memorization properties of ConvNets (Alexandre Sablayrolles - 17 September, 2018)
Convolutional neural networks memorize part of their training data, which is why strategies such as data augmentation and drop-out are employed to mitigate overfitting
Link: https://arxiv.org/abs/1809.06396
====================================================
Learning Effective RGB-D Representations for Scene Recognition (Xinhang Song - 17 September, 2018)
Our video recognition architecture combines convolutional and recurrent neural networks (RNNs) that are trained in three steps with increasingly complex data to learn effective features (i.e. Our approach obtains state-of-the-art performances on RGB-D image (NYUD2 and SUN RGB-D) and video (ISIA RGB-D) scene recognition.
Link: https://arxiv.org/abs/1809.06269
====================================================
Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network (Shuang Li - 17 September, 2018)
In this paper, we present TeachNet, a novel neural network architecture for intuitive and markerless vision-based teleoperation of dexterous robotic hands. Imitation experiments and grasp tasks teleoperated by novice users demonstrate that TeachNet is more reliable and faster than the state-of-the-art vision-based teleoperation method.
Link: https://arxiv.org/abs/1809.06268
====================================================
PointNetGPD: Detecting Grasp Configurations from Point Sets (Hongzhuo Liang - 17 September, 2018)
Compared to recent grasp evaluation metrics that are based on handcrafted depth features and a convolutional neural network (CNN), our proposed PointNetGPD is light-weighted and can directly process the 3D point cloud that locates within the gripper for grasp evaluation. Experiments on object grasping and clutter removal show that our proposed model generalizes well to novel objects and outperforms state-of-the-art methods.
Link: https://arxiv.org/abs/1809.06267
====================================================
Linear and Deformable Image Registration with 3D Convolutional Neural Networks (Stergios Christodoulidis - 13 September, 2018)
Inspired by the recent advances in deep learning, we propose in this paper, a novel convolutional neural network architecture that couples linear and deformable registration within a unified architecture endowed with near real-time performance. We evaluate the performance of our network on the challenging problem of MRI lung registration, and demonstrate superior performance with respect to state of the art elastic registration methods
Link: https://arxiv.org/abs/1809.06226
====================================================
Ensemble learning with 3D convolutional neural networks for connectome-based prediction (Meenakshi Khosla - 11 September, 2018)
We further present an implementation of our ensemble learning strategy with a novel 3D Convolutional Neural Network (CNN) approach
Link: https://arxiv.org/abs/1809.06219
====================================================
A Dataset and Preliminary Results for Umpire Pose Detection Using SVM Classification of Deep Features (Aravind Ravi - 11 September, 2018)
Pre-trained convolutional neural networks such as Inception V3 and VGG19 net-works are selected as primary candidates for feature extraction
Link: https://arxiv.org/abs/1809.06217
====================================================
ManifoldNet: A Deep Network Framework for Manifold-valued Data (Rudrasis Chakraborty - 20 September, 2018)
Thus, there is need to generalize the deep neural networks to cope with input data that reside on curved manifolds where vector space operations are not naturally admissible. In this paper, we present a novel theoretical framework to generalize the widely popular convolutional neural networks (CNNs) to high dimensional manifold-valued data inputs
Link: https://arxiv.org/abs/1809.06211
====================================================
Quantum Statistics-Inspired Neural Attention (Aristotelis Charalampous - 17 September, 2018)
This effectiveness largely stems from the capacity of these models to infer salient temporal dynamics over long horizons; these are encoded into the obtained neural attention (NA) distributions
Link: https://arxiv.org/abs/1809.06205
====================================================
Player Experience Extraction from Gameplay Video (Zijin Luo - 6 September, 2018)
In this paper we present two approaches to derive game logs from game video via convolutional neural networks and transfer learning
Link: https://arxiv.org/abs/1809.06201
====================================================
Intermediate Deep Feature Compression: the Next Battlefield of Intelligent Sensing (Zhuo Chen - 17 September, 2018)
This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis
Link: https://arxiv.org/abs/1809.06196
====================================================
The Fast and the Flexible: training neural networks to learn to follow instructions from small data (Rezka Leonandya - 17 September, 2018)
In contrast, here we seek to establish whether this knowledge can be acquired automatically by a neural network system through a two phase training procedure: A (slow) offline learning stage where the network learns about the general structure of the task and a (fast) online adaptation phase where the network learns the language of a new given speaker
Link: https://arxiv.org/abs/1809.06194
====================================================
Multi Modal Convolutional Neural Networks for Brain Tumor Segmentation (Mehmet AygÃ¼n - 20 September, 2018)
In this work, we propose a multi-modal Convolutional Neural Network (CNN) approach for brain tumor segmentation
Link: https://arxiv.org/abs/1809.06191
====================================================
Study and Observation of the Variations of Accuracies for Handwritten Digits Recognition with Various Hidden Layers and Epochs using Neural Network Algorithm (Md. Abu Bakr Siddique - 22 September, 2018)
The primary objective of this paper is to analyze the influence of the hidden layers of a neural network over the overall performance of the network. To demonstrate this influence, we applied neural network with different layers on the MNIST dataset
Link: https://arxiv.org/abs/1809.06188
====================================================
Study and Observation of the Variations of Accuracies for Handwritten Digits Recognition with Various Hidden Layers and Epochs using Convolutional Neural Network (Rezoana Bente Arif - 22 September, 2018)
In deep learning, Convolutional Neural Network (CNN) is extensively used in the pattern and sequence recognition, video analysis, natural language processing, spam detection, topic categorization, regression analysis, speech recognition, image classification, object detection, segmentation, face recognition, robotics, and control. To demonstrate this influence, we applied neural network with different layers on the Modified National Institute of Standards and Technology (MNIST) dataset
Link: https://arxiv.org/abs/1809.06187
====================================================
Graph Neural Networks for IceCube Signal Classification (Nicholas Choma - 17 September, 2018)
In this work, we leverage graph neural networks to improve signal detection in the IceCube neutrino observatory. We demonstrate the effectiveness of our GNN architecture on a task classifying IceCube events, where it outperforms both a traditional physics-based method as well as classical 3D convolution neural networks.
Link: https://arxiv.org/abs/1809.06166
====================================================
A Deep Learning Framework for Unsupervised Affine and Deformable Image Registration (Bob D. de Vos - 17 September, 2018)
Recent studies have shown that deep learning methods, notably convolutional neural networks (ConvNets), can be used for image registration
Link: https://arxiv.org/abs/1809.06130
====================================================
DeepDrum: An Adaptive Conditional Neural Network (Dimos Makris - 17 September, 2018)
In this paper we present DeepDrum, an adaptive Neural Network capable of generating drum rhythms under constraints imposed by Feed-Forward (Conditional) Layers which contain musical parameters along with given instrumentation information (e.g
Link: https://arxiv.org/abs/1809.06127
====================================================
Muscle Excitation Estimation in Biomechanical Simulation Using NAF Reinforcement Learning (Amir H. Abdi - 17 September, 2018)
This can open paths for neural activity interpretation of this phenomenon.
Link: https://arxiv.org/abs/1809.06121
====================================================
Transparency and Explanation in Deep Reinforcement Learning Neural Networks (Rahul Iyer - 17 September, 2018)
In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque
Link: https://arxiv.org/abs/1809.06061
====================================================
Autonomous Exploration, Reconstruction, and Surveillance of 3D Environments Aided by Deep Learning (Louis Ly - 17 September, 2018)
Using a level set representation of data and information, we train a convolutional neural network to determine vantage points that maximize visibility
Link: https://arxiv.org/abs/1809.06025
====================================================
The Impact of On-chip Communication on Memory Technologies for Neuromorphic Systems (Saber Moradi - 1 October, 2018)
Emergent nanoscale non-volatile memory technologies with high integration density offer a promising solution to overcome the scalability limitations of CMOS-based neural networks architectures, by efficiently exhibiting the key principle of neural computation
Link: https://arxiv.org/abs/1809.06016
====================================================
Uncertainty Propagation in Deep Neural Networks Using Extended Kalman Filtering (Jessica S. Titensky - 16 September, 2018)
Extended Kalman Filtering (EKF) can be used to propagate and quantify input uncertainty through a Deep Neural Network (DNN) assuming mild hypotheses on the input distribution
Link: https://arxiv.org/abs/1809.06009
====================================================
Evaluating Merging Strategies for Sampling-based Uncertainty Techniques in Object Detection (Dimity Miller - 27 September, 2018)
There has been a recent emergence of sampling-based techniques for estimating epistemic uncertainty in deep neural networks
Link: https://arxiv.org/abs/1809.06006
====================================================
UAV Pose Estimation using Cross-view Geolocalization with Satellite Imagery (Akshay Shetty - 16 September, 2018)
Our method consists of two Siamese neural networks that extract relevant features despite large differences in viewpoints
Link: https://arxiv.org/abs/1809.05979
====================================================
Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization (Yizhe Zhang - 24 September, 2018)
Responses generated by neural conversational models tend to lack informativeness and diversity
Link: https://arxiv.org/abs/1809.05972
====================================================
Maximum-Entropy Fine-Grained Classification (Abhimanyu Dubey - 20 September, 2018)
Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve state-of-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task
Link: https://arxiv.org/abs/1809.05934
====================================================
MeshCNN: A Network with an Edge (Rana Hanocka - 16 September, 2018)
This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using MeshCNN, a convolutional neural network designed specifically for triangular meshes
Link: https://arxiv.org/abs/1809.05910
====================================================
Classifying Process Instances Using Recurrent Neural Networks (Markku Hinkka - 16 September, 2018)
Recurrent neural networks and its subclasses, such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), have been demonstrated to be able to learn relevant temporal features for subsequent classification tasks. In this paper we apply recurrent neural networks to classifying process instances
Link: https://arxiv.org/abs/1809.05896
====================================================
Multiple Abnormality Detection for Automatic Medical Image Diagnosis Using Bifurcated Convolutional Neural Network (Mohsen Hajabdollahi - 16 September, 2018)
To address this problem, we propose a bifurcated structure for convolutional neural networks performing both classification and segmentation of multiple abnormalities simultaneously
Link: https://arxiv.org/abs/1809.05831
====================================================
Aesthetic-based Clothing Recommendation (Wenhui Yu - 16 September, 2018)
To achieve this, we first present the aesthetic features extracted by a pre-trained neural network, which is a brain-inspired deep structure trained for the aesthetic assessment task. We conduct extensive experiments on real-world datasets, which demonstrate that our approach can capture the aesthetic preference of users and significantly outperform several state-of-the-art recommendation methods.
Link: https://arxiv.org/abs/1809.05822
====================================================
Deep Learning with Experience Ranking Convolutional Neural Network for Robot Manipulator (Hai Nguyen - 16 September, 2018)
Supervised learning, more specifically Convolutional Neural Networks (CNN), has surpassed human ability in some visual recognition tasks such as detection of traffic signs, faces and handwritten numbers. On the other hand, even state-of-the-art reinforcement learning (RL) methods have difficulties in environments with sparse and binary rewards
Link: https://arxiv.org/abs/1809.05819
====================================================
LVIS: Learning from Value Function Intervals for Contact-Aware Robot Controllers (Robin Deits - 15 September, 2018)
These interval samples are used to weakly supervise the training of a neural net which approximates the true cost-to-go
Link: https://arxiv.org/abs/1809.05802
====================================================
Direct Training for Spiking Neural Networks: Faster, Larger, Better (Yujie Wu - 15 September, 2018)
Yet now, SNNs have not shown competitive performance compared with artificial neural networks (ANNs), due to the lack of effective learning algorithms and efficient programming frameworks. We address this issue from two aspects: (1) We propose a neuron normalization technique to adjust the neural selectivity and develop a direct learning algorithm for large-scale SNNs
Link: https://arxiv.org/abs/1809.05793
====================================================
Finding the way from Ã¤ to a: Sub-character morphological inflection for the SIGMORPHON 2018 Shared Task (Fynn SchrÃ¶der - 15 September, 2018)
We propose a neural architecture based on the concepts of UZH (Makarov et al., 2017), adding new ideas and techniques to their key concept and evaluating different combinations of parameters
Link: https://arxiv.org/abs/1809.05742
====================================================
Neural Networks and Quantifier Conservativity: Does Data Distribution Affect Learnability? (Vishwali Mhasawade - 15 September, 2018)
However, recent work indicates that this bias towards conservativity is not observed during the training stage of artificial neural networks
Link: https://arxiv.org/abs/1809.05733
====================================================
Multi-Scale Deep Compressive Sensing Network (Thuong Nguyen Canh - 18 September, 2018)
In this paper, we propose a multi-scale DCS convolutional neural network (MS-DCSNet) in which we convert image signal using multiple scale-based wavelet transform, then capture it through convolution block by block across scales
Link: https://arxiv.org/abs/1809.05717
====================================================
Abstractive Dialogue Summarization with Sentence-Gated Modeling Optimized by Dialogue Acts (Chih-Wen Goo - 29 September, 2018)
This paper proposes to explicitly leverage dialogue acts in a neural summarization model, where a sentence-gated mechanism is designed for modeling the relationship between dialogue acts and the summary. The experiments show that our proposed model significantly improves the abstractive summarization performance compared to the state-of-the-art baselines on AMI meeting corpus, demonstrating the usefulness of the interactive signal provided by dialogue acts.
Link: https://arxiv.org/abs/1809.05715
====================================================
Learning Robust Manipulation Skills with Guided Policy Search via Generative Motor Reflexes (Philipp Ennen - 15 September, 2018)
Therein, the control policies are represented as high-dimensional neural networks which derive robot actions based on states. However, due to the small number of real-world trajectory samples in Guided Policy Search, the resulting neural networks are only robust in the neighbourhood of the trajectory distribution explored by real-world interactions
Link: https://arxiv.org/abs/1809.05714
====================================================
Neural Networks as Artificial Specifications (I. S. W. B. Prasetya - 15 September, 2018)
In theory, a neural network can be trained to act as an artificial specification for a program by showing it samples of the programs executions
Link: https://arxiv.org/abs/1809.05701
====================================================
Attention as a Perspective for Learning Tempo-invariant Audio Queries (Matthias Dorfer - 15 September, 2018)
Current models for audio--sheet music retrieval via multimodal embedding space learning use convolutional neural networks with a fixed-size window for the input audio
Link: https://arxiv.org/abs/1809.05689
====================================================
Graph Convolutional Networks for Text Classification (Liang Yao - 15 September, 2018)
Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.
Link: https://arxiv.org/abs/1809.05679
====================================================
Towards Better Interpretability in Deep Q-Networks (Raghuram Mandyam Annasamy - 14 September, 2018)
With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.
Link: https://arxiv.org/abs/1809.05630
====================================================
Multi-Task Learning for Email Search Ranking with Auxiliary Query Clustering (Jiaming Shen - 14 September, 2018)
Then, we study three query-dependent ranking models, including two neural models that leverage query type information as additional features, and one novel multi-task neural model that views query type as the label for the auxiliary query cluster prediction task. Our experiments on tens of millions of real-world email search queries demonstrate that the proposed multi-task model can significantly outperform the baseline neural ranking models, which either do not incorporate query type information or just simply feed query type as an additional feature.
Link: https://arxiv.org/abs/1809.05618
====================================================
Brain decoding from functional MRI using long short-term memory recurrent neural networks (Hongming Li - 14 September, 2018)
In this study, we develop a deep learning based framework for brain decoding by leveraging recent advances in sequence modeling using long short-term memory (LSTM) recurrent neural networks (RNNs)
Link: https://arxiv.org/abs/1809.05561
====================================================
Identification of temporal transition of functional states using recurrent neural networks from functional MRI (Hongming Li - 14 September, 2018)
In this study, we develop a deep learning based framework for adaptively detecting temporally dynamic functional state transitions in a data-driven way without any explicit modeling assumptions, by leveraging recent advances in recurrent neural networks (RNNs) for sequence modeling
Link: https://arxiv.org/abs/1809.05560
====================================================
Deep Compressive Autoencoder for Action Potential Compression in Large-Scale Neural Recording (Tong Wu - 16 September, 2018)
One major hurdle to design high-bandwidth, high-precision, large-scale neural interfaces lies in the formidable data streams that are generated by the recorder chip and need to be online transferred to a remote computer
Link: https://arxiv.org/abs/1809.05522
====================================================
Hardware-Aware Machine Learning: Modeling and Optimization (Diana Marculescu - 14 September, 2018)
What is the latency or energy cost for an inference made by a Deep Neural Network (DNN)? Is it possible to predict this latency or energy consumption before a model is trained? If yes, how can machine learners take advantage of these models to design the hardware-optimal DNN for deployment? From lengthening battery life of mobile devices to reducing the runtime requirements of DL models executing in the cloud, the answers to these questions have drawn significant attention.
Link: https://arxiv.org/abs/1809.05476
====================================================
Real Time System for Facial Analysis (Janne Tommola - 14 September, 2018)
All components are based on convolutional neural networks, whose accuracy we study on commonly used training and evaluation sets
Link: https://arxiv.org/abs/1809.05474
====================================================
Socially Aware Kalman Neural Networks for Trajectory Prediction (Ce Ju - 14 September, 2018)
We purpose a data-driven approach socially aware Kalman neural networks (SAKNN) where the interaction layer and the Kalman layer are embedded in the architecture, resulting in a class of architectures with huge potential to directly learn from high variance sensor input and robustly generate low variance outcomes. The evaluation of our approach on NGSIM dataset demonstrates that SAKNN performs state-of-the-art on prediction effectiveness in a relatively long-term horizon and significantly improves the signal-to-noise ratio of the predicted signal.
Link: https://arxiv.org/abs/1809.05408
====================================================
SCORES: Shape Composition with Recursive Substructure Priors (Chenyang Zhu - 14 September, 2018)
We introduce SCORES, a recursive neural network for shape composition. It is trained on structured shapes from ShapeNet, and is applied iteratively to reduce the plausibility loss.We showresults of shape composition from multiple sources over different categories of man-made shapes and compare with state-of-the-art alternatives, demonstrating that our network can significantly expand the range of composable shapes for assembly-based modeling.
Link: https://arxiv.org/abs/1809.05398
====================================================
Style Augmentation: Data Augmentation via Style Randomization (Philip T. Jackson - 14 September, 2018)
We introduce style augmentation, a new form of data augmentation based on random style transfer, for improving the robustness of convolutional neural networks (CNN) over both classification and regression based tasks
Link: https://arxiv.org/abs/1809.05375
====================================================
Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs (Juraj Juraska - 14 September, 2018)
We then automatically label the training data to allow us to conduct two kinds of experiments with a neural generator
Link: https://arxiv.org/abs/1809.05288
====================================================
Deep CNN Frame Interpolation with Lessons Learned from Natural Language Processing (Kian Ghodoussi - 16 September, 2018)
The general explanation within the deep learning community of the robustness of convolutional neural networks (CNNs) within image recognition rests upon the idea that CNNs are able to extract localized features. From there, we demonstrate the effectiveness of our approach by presenting novel deep CNN frame interpolation architecture that is comparable to the state of the art interpolation models with a fraction of the complexity.
Link: https://arxiv.org/abs/1809.05286
====================================================
Macquarie University at BioASQ 6b: Deep learning and deep reinforcement learning for query-based multi-document summarisation (Diego MollÃ¡ - 14 September, 2018)
The global policy was implemented as a neural network that used $tf.idf$ features encoding the candidate sentence, question, and context.
Link: https://arxiv.org/abs/1809.05283
====================================================
Neural Network Topologies for Sparse Training (Ryan A. Robinett - 13 September, 2018)
The sizes of deep neural networks (DNNs) are rapidly outgrowing the capacity of hardware to store and train them. The resulting neural network is known as a sparse neural network
Link: https://arxiv.org/abs/1809.05242
====================================================
VoxelMorph: A Learning Framework for Deformable Medical Image Registration (Guha Balakrishnan - 13 September, 2018)
We define registration as a parametric function, implemented as a convolutional neural network (CNN). We demonstrate that the unsupervised model's accuracy is comparable to state-of-the-art methods, while operating orders of magnitude faster
Link: https://arxiv.org/abs/1809.05231
====================================================
Automatic Catchphrase Extraction from Legal Case Documents via Scoring using Deep Neural Networks (Vu Tran - 13 September, 2018)
We utilize deep neural networks for constructing scoring model of our extraction system
Link: https://arxiv.org/abs/1809.05219
====================================================
Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation (Brian Thompson - 13 September, 2018)
To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the encoder, decoder, and each embedding space) and consider each component's contribution to, and capacity for, domain adaptation
Link: https://arxiv.org/abs/1809.05218
====================================================
CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning (Jiachen Yang - 13 September, 2018)
These two stages are bridged by modular augmentation of neural network policy and value functions
Link: https://arxiv.org/abs/1809.05188
====================================================
A Deep Learning and Gamification Approach to Energy Conservation at Nanyang Technological University (Ioannis C. Konstantakopoulos - 25 September, 2018)
To improve forecasting performance, we extend the benchmark utility learning scheme by leveraging Deep Learning end-to-end training with Deep bi-directional Recurrent Neural Networks
Link: https://arxiv.org/abs/1809.05142
====================================================
IL-Net: Using Expert Knowledge to Guide the Design of Furcated Neural Networks (Khushmeen Sakloth - 13 September, 2018)
In this work, we develop a novel furcated neural network architecture that utilizes domain knowledge as high-level design principles of the network. Compared to existing state-of-the-art approaches, we show that furcated networks can improve model accuracy by approximately 20-35%, without using additional labeled data
Link: https://arxiv.org/abs/1809.05127
====================================================
Learning to Group and Label Fine-Grained Shape Components (Xiaogang Wang - 13 September, 2018)
A multiscale 3D convolutional neural network is trained to extract context-aware features for the hypotheses
Link: https://arxiv.org/abs/1809.05050
====================================================
Sequential Coordination of Deep Models for Learning Visual Arithmetic (Eric Crawford - 13 September, 2018)
The lower tier consists of a heterogeneous collection of information processing modules, which can include pre-trained deep neural networks for locating and extracting characters from the image, as well as modules performing symbolic transformations on the representations extracted by perception
Link: https://arxiv.org/abs/1809.04988
====================================================
High-Accuracy Inference in Neuromorphic Circuits using Hardware-Aware Training (Borna Obradovic - 13 September, 2018)
Neuromorphic Multiply-And-Accumulate (MAC) circuits utilizing synaptic weight elements based on SRAM or novel Non-Volatile Memories (NVMs) provide a promising approach for highly efficient hardware representations of neural networks. The proposed algorithm is applicable to a wide range of hardware models, and uses only standard neural network training methods
Link: https://arxiv.org/abs/1809.04982
====================================================
Unsupervised Machine Commenting with Neural Variational Topic Model (Shuming Ma - 13 September, 2018)
The topic representation is obtained from a neural variational topic model, which is trained in an unsupervised manner. The model also profits from paired corpora and achieves state-of-the-art performance under semi-supervised scenarios.
Link: https://arxiv.org/abs/1809.04960
====================================================
LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts (Shuming Ma - 13 September, 2018)
Then, we introduce two neural models to generate live comments based on the visual and textual contexts, which achieve better performance than previous neural baselines such as the sequence-to-sequence model
Link: https://arxiv.org/abs/1809.04938
====================================================
Deep Network Uncertainty Maps for Indoor Navigation (Jens Lundell - 13 September, 2018)
Deep Neural Networks have recently been proposed to overcome this limitation by learning to estimate object occupancy
Link: https://arxiv.org/abs/1809.04891
====================================================
Adversarial Examples: Opportunities and Challenges (Jiliang Zhang - 13 September, 2018)
With the advent of the era of artificial intelligence(AI), deep neural networks (DNNs) have shown huge superiority over human in image recognition, speech processing, autonomous vehicles and medical diagnosis. First, we introduce the concept, cause, characteristic and evaluation metrics of AEs, then give a survey on the state-of-the-art AE generation methods with the discussion of advantages and disadvantages
Link: https://arxiv.org/abs/1809.04790
====================================================
SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories (Sweta Karlekar - 13 September, 2018)
Furthermore, we present analysis using LIME, first-derivative saliency heatmaps, activation clustering, and embedding visualization to interpret neural model predictions and demonstrate how this extracts features that can help automatically fill out incident reports, identify unsafe areas, avoid unsafe practices, and 'pin the creeps'.
Link: https://arxiv.org/abs/1809.04739
====================================================
DispSegNet: Leveraging Semantics for End-to-End Learning of Disparity Estimation from Stereo Imagery (Junming Zhang - 12 September, 2018)
Recent work has shown that convolutional neural networks (CNNs) can be applied successfully in disparity estimation, but these methods still suffer from errors in regions of low-texture, occlusions and reflections. Experiments on KITTI and Cityscapes datasets show that our model can achieve state-of-the-art results and that leveraging embedding learned from semantic segmentation improves the performance of disparity estimation.
Link: https://arxiv.org/abs/1809.04734
====================================================
Adapting Semantic Segmentation Models for Changes in Illumination and Camera Perspective (Wei Zhou - 12 September, 2018)
Semantic segmentation using deep neural networks has been widely explored to generate high-level contextual information for autonomous vehicles
Link: https://arxiv.org/abs/1809.04730
====================================================
Linear Algebra and Duality of Neural Networks (Galin Georgiev - 17 September, 2018)
Bases, mappings, projections and metrics, natural for Neural network training, are introduced
Link: https://arxiv.org/abs/1809.04711
====================================================
Geometric Image Synthesis (Hassan Abu Alhaija - 12 September, 2018)
On the other hand, recent developments in deep neural networks allow for trainable models that can produce natural-looking images with little or no knowledge about the scene structure. Our geometrically-consistent image synthesis method is a deep neural network, called Geometry to Image Synthesis (GIS) framework, which retains the advantages of a trainable method, e.g., differentiability and adaptiveness, but, at the same time, makes a step towards the generalizability, control and quality output of modern graphics rendering engines
Link: https://arxiv.org/abs/1809.04696
====================================================
Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation (Akiko Eriguchi - 12 September, 2018)
In parallel, the recent progress in Machine Translation (MT) has enabled one to train multilingual Neural MT (NMT) systems that can translate between multiple languages and are also capable of performing zero-shot translation
Link: https://arxiv.org/abs/1809.04686
====================================================
Automatic Program Synthesis of Long Programs with a Learned Garbage Collector (Amit Zohar - 12 September, 2018)
The neural network optimizes multiple tasks concurrently: the next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory. Using our method we are able to create programs that are more than twice as long as existing state-of-the-art solutions, while improving the success rate for comparable lengths, and cutting the run-time by two orders of magnitude
Link: https://arxiv.org/abs/1809.04682
====================================================
Closed-Book Training to Improve Summarization Encoder Memory (Yichen Jiang - 12 September, 2018)
A good neural sequence-to-sequence summarization model should have a strong encoder that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the encoder's memory
Link: https://arxiv.org/abs/1809.04585
====================================================
FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks (Michaela Blott - 12 September, 2018)
Given a neural network description, the tool optimizes for given platforms, design targets and a specific precision. Finally, we evaluate a selection of reduced precision neural networks ranging from CIFAR-10 classifiers to YOLO-based object detection on a range of platforms including PYNQ and AWS\,F1, demonstrating new unprecedented measured throughput at 50TOp/s on AWS-F1 and 5TOp/s on embedded devices.
Link: https://arxiv.org/abs/1809.04570
====================================================
Unsupervised Controllable Text Formalization (Parag Jain - 10 September, 2018)
The crux of the framework is a deep neural encoder-decoder that is reinforced with text-transformation knowledge through auxiliary modules (called scorers)
Link: https://arxiv.org/abs/1809.04556
====================================================
Human Driving Skill Modeling Using Neural Networks for Haptic Assistance in Realistic Virtual Environments (Hojin Lee - 12 September, 2018)
To this end, we obtain a model utilizing artificial neural networks to extract a desired movement of a steering wheel and an accelerator pedal based on the experts' prediction. We validate the performance of our framework in two respective user experiments recruiting expert/novice drivers to show the feasibility and applicability of facilitating neural networks for performance-based haptic driving skill transfer.
Link: https://arxiv.org/abs/1809.04549
====================================================
Genetic algorithms with DNN-based trainable crossover as an example of partial specialization of general search (Alexey Potapov - 18 July, 2018)
We perform a feasibility study of this idea implementing such an operator in the form of a deep feedforward neural network. GAs with trainable crossover operators are compared with the result of complete specialization, which is also represented as a deep neural network
Link: https://arxiv.org/abs/1809.04520
====================================================
Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution (Zhisheng Zhong - 19 September, 2018)
Convolutional neural networks (CNNs) have recently achieved great success in single-image super-resolution (SISR). Extensive quantitative and qualitative experiments on benchmark datasets show that our method achieves superior performance over the state-of-the-art methods.
Link: https://arxiv.org/abs/1809.04508
====================================================
Investigating the generalizability of EEG-based Cognitive Load Estimation Across Visualizations (Viral Parekh - 12 September, 2018)
CL is estimated via two recent approaches: (a) Deep convolutional neural network, and (b) Proximal support vector machines
Link: https://arxiv.org/abs/1809.04507
====================================================
Learning structure-from-motionfrom motion (ClÃ©ment Pinard - 12 September, 2018)
This work is based on a questioning of the quality metrics used by deep neural networks performing depth prediction from a single image, and then of the usability of recently published works on unsupervised learning of depth from videos
Link: https://arxiv.org/abs/1809.04471
====================================================
Multi range Real-time depth inference from a monocular stabilized footage using a Fully Convolutional Neural Network (ClÃ©ment Pinard - 12 September, 2018)
Using a neural network architecture for depth map inference from monocular stabilized videos with application to UAV videos in rigid scenes, we propose a multi-range architecture for unconstrained UAV flight, leveraging flight data from sensors to make accurate depth maps for uncluttered outdoor environment
Link: https://arxiv.org/abs/1809.04467
====================================================
An empirical learning-based validation procedure for simulation workflow (Zhuqing Liu - 10 September, 2018)
In order to make full use of the historical data and implement more efficient validation, four learning algorithms, including back propagation neural network (BPNN), extreme learning machine (ELM), evolving new-neuron (eNFN) and fast incremental gaussian mixture model (FIGMN), are introduced for constructing the empirical relation between the workflow credibility and its features. The experimental results also provide some useful overview of the state-of-the-art learning algorithms on the credibility evaluation of simulation models.
Link: https://arxiv.org/abs/1809.04441
====================================================
Convolutional Neural Networks for Fast Approximation of Graph Edit Distance (Yunsheng Bai - 10 September, 2018)
A Convolutional Neural Network (CNN) based approach is proposed to tackle the set matching problem. We test our algorithm on three real graph datasets, and our model achieves significant performance enhancement against state-of-the-art approximate GED computation algorithms.
Link: https://arxiv.org/abs/1809.04440
====================================================
Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification (Long Chen - 12 September, 2018)
In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.
Link: https://arxiv.org/abs/1809.04427
====================================================
Re-purposing Compact Neuronal Circuit Policies to Govern Reinforcement Learning Tasks (Ramin M. Hasani - 11 September, 2018)
For reconfiguration of the \emph{purpose} of the neural circuit, we adopt a search-based RL algorithm. We show that our neuronal circuit policies perform as good as deep neural network policies with the advantage of realizing interpretable dynamics at the cell-level
Link: https://arxiv.org/abs/1809.04423
====================================================
NNCP: A citation count prediction methodology based on deep neural network learning techniques (Ali Abrishami - 12 September, 2018)
In order to train a citations prediction model, we employed artificial neural networks which is a powerful machine learning tool with recently growing applications in many domains including image and text processing. The empirical experiments show that our proposed method out-performs state-of-the-art methods with respect to the prediction accuracy in both yearly and total prediction of the number of citations.
Link: https://arxiv.org/abs/1809.04365
====================================================
Training Deep Neural Networks with Different Datasets In-the-wild: The Emotion Recognition Paradigm (Dimitrios Kollias - 12 September, 2018)
A novel procedure is presented in this paper, for training a deep convolutional and recurrent neural network, taking into account both the available training data set and some information extracted from similar networks trained with other relevant data sets
Link: https://arxiv.org/abs/1809.04359
====================================================
Neural Melody Composition from Lyrics (Hangbo Bao - 12 September, 2018)
It consists of two neural encoders to encode the current lyrics and the context melody respectively, and a hierarchical decoder to jointly produce musical notes and the corresponding alignment
Link: https://arxiv.org/abs/1809.04318
====================================================
Chinese Poetry Generation with a Salient-Clue Mechanism (Xiaoyuan Yi - 12 September, 2018)
In recent years, significant progress has been made in this area benefiting from the development of neural networks
Link: https://arxiv.org/abs/1809.04313
====================================================
Chinese Poetry Generation with a Working Memory Model (Xiaoyuan Yi - 12 September, 2018)
Different from previous methods, our model explicitly maintains topics and informative limited history in a neural memory. Both automatic and human evaluation results show that our model outperforms current state-of-the-art methods.
Link: https://arxiv.org/abs/1809.04306
====================================================
Retrieval-Enhanced Adversarial Training for Neural Response Generation (Qingfu Zhu - 12 September, 2018)
In this paper, we propose a Retrieval-Enhanced Adversarial Training (REAT) method for neural response generation
Link: https://arxiv.org/abs/1809.04276
====================================================
Convolutional Neural Network Approach for EEG-based Emotion Recognition using Brain Connectivity and its Spatial Information (Seong-Eun Moon - 11 September, 2018)
In this paper, we propose a novel deep learning approach using convolutional neural networks (CNNs) for EEG-based emotion recognition
Link: https://arxiv.org/abs/1809.04208
====================================================
Temporal Pattern Attention for Multivariate Time Series Forecasting (Shun-Yao Shih - 11 September, 2018)
To obtain accurate prediction, it is crucial to model long-term dependency in time series data, which can be achieved to some good extent by recurrent neural network (RNN) with attention mechanism. We applied the proposed model on several real-world tasks and achieved the state-of-the-art performance in all of them with only one exception
Link: https://arxiv.org/abs/1809.04206
====================================================
What can linguistics and deep learning contribute to each other? (Tal Linzen - 14 September, 2018)
Linguists can contribute to research on neural networks for language technologies by clearly delineating the linguistic capabilities that can be expected of such systems, and by constructing controlled experimental paradigms that can determine whether those desiderata have been met. In the other direction, neural networks can benefit the scientific study of language by providing infrastructure for modeling human sentence processing and for evaluating the necessity of particular innate constraints on language acquisition.
Link: https://arxiv.org/abs/1809.04179
====================================================
Time Series Analysis of Clickstream Logs from Online Courses (Yohan Jo - 11 September, 2018)
In this project, we have exploited the temporal dynamics of student behaviors both to do behavior modeling via graphical modeling approaches and to do performance prediction via recurrent neural network approaches in order to first identify student behaviors and then use them to predict their final outcome in the course
Link: https://arxiv.org/abs/1809.04177
====================================================
Leabra7: a Python package for modeling recurrent, biologically-realistic neural networks (C. Daniel Greenidge - 19 September, 2018)
Emergent is a software package that uses the AdEx neural dynamics model and LEABRA learning algorithm to simulate and train arbitrary recurrent neural network architectures in a biologically-realistic manner
Link: https://arxiv.org/abs/1809.04166
====================================================
Heated-Up Softmax Embedding (Xu Zhang - 11 September, 2018)
We study the features extracted from the second last layer of a deep neural network based classifier trained with the cross entropy loss on top of the softmax layer. Leveraging these insights, we propose a "heating-up" strategy to train a classifier with increasing temperatures, leading the corresponding embeddings to achieve state-of-the-art performance on a variety of metric learning benchmarks.
Link: https://arxiv.org/abs/1809.04157
====================================================
End-to-end Image Captioning Exploits Multimodal Distributional Similarity (Pranava Madhyastha - 11 September, 2018)
We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn `distributional similarity' in a multimodal feature space by mapping a test image to similar training images in this space and generating a caption from the same space
Link: https://arxiv.org/abs/1809.04144
====================================================
JigsawNet: Shredded Image Reassembly using Convolutional Neural Network and Loop-based Composition (Canyu Le - 11 September, 2018)
We build a deep convolutional neural network to detect the compatibility of a pairwise stitching, and use it to prune computed pairwise matches
Link: https://arxiv.org/abs/1809.04137
====================================================
Taking a machine's perspective: Human deciphering of adversarial images (Zhenglong Zhou - 11 September, 2018)
How similar is the human mind to the sophisticated machine-learning systems that mirror its performance? Models of object categorization based on convolutional neural networks (CNNs) have achieved human-level benchmarks in assigning known labels to novel images
Link: https://arxiv.org/abs/1809.04120
====================================================
Detecting egregious responses in neural sequence-to-sequence models (Tianxing He - 3 October, 2018)
In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq) model to generate egregious outputs (aggressive, malicious, attacking, etc.)
Link: https://arxiv.org/abs/1809.04113
====================================================
On the Structural Sensitivity of Deep Convolutional Networks to the Directions of Fourier Basis Functions (Yusuke Tsuzuku - 11 September, 2018)
We derived the property by specializing a hypothesis of the cause of the sensitivity, known as the linearity of neural networks, to convolutional networks and empirically validated it
Link: https://arxiv.org/abs/1809.04098
====================================================
DNN Dataflow Choice Is Overrated (Xuan Yang - 10 September, 2018)
Compared with Eyeriss system, it achieves up to 4.2X energy improvement for Convolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long Short-Term Memories (LSTMs) and multi-layer perceptrons (MLPs) respectively.
Link: https://arxiv.org/abs/1809.04070
====================================================
Neural-Augmented Static Analysis of Android Communication (Jinman Zhao - 11 September, 2018)
We describe a neural-network architecture that encodes abstractions of communicating objects in two applications and estimates the probability with which a link indeed exists. Further, we conduct thorough interpretability studies to understand the internals of the learned neural networks.
Link: https://arxiv.org/abs/1809.04059
====================================================
Can LSTM Learn to Capture Agreement? The Case of Basque (Shauli Ravfogel - 21 September, 2018)
Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks
Link: https://arxiv.org/abs/1809.04022
====================================================
Evaluating Semantic Rationality of a Sentence: A Sememe-Word-Matching Neural Network based on HowNet (Shu Liu - 11 September, 2018)
In this paper, we propose a novel model called Sememe-Word-Matching Neural Network (SWM-NN) to tackle semantic rationality evaluation by taking advantage of sememe knowledge base HowNet
Link: https://arxiv.org/abs/1809.03999
====================================================
Efficient Road Lane Marking Detection with Deep Learning (Ping-Rong Chen - 11 September, 2018)
In this paper, we propose a Lane Marking Detector (LMD) using a deep convolutional neural network to extract robust lane marking features
Link: https://arxiv.org/abs/1809.03994
====================================================
Assessing Composition in Sentence Vector Representations (Allyson Ettinger - 11 September, 2018)
An important component of achieving language understanding is mastering the composition of sentence meaning, but an immediate challenge to solving this problem is the opacity of sentence vector representations produced by current neural sentence composition models
Link: https://arxiv.org/abs/1809.03992
====================================================
SAI, a Sensible Artificial Intelligence that plays Go (Francesco Morandin - 11 September, 2018)
The winrate as a function of the komi is modeled with a two-parameters sigmoid function, so that the neural network must predict just one more variable to assess the winrate for all komi values
Link: https://arxiv.org/abs/1809.03928
====================================================
Response Characterization for Auditing Cell Dynamics in Long Short-term Memory Networks (Ramin M. Hasani - 11 September, 2018)
In this paper, we introduce a novel method to interpret recurrent neural networks (RNNs), particularly long short-term memory networks (LSTMs) at the cellular level
Link: https://arxiv.org/abs/1809.03864
====================================================
EXS: Explainable Search Using Local Model Agnostic Interpretability (Jaspreet Singh - 11 September, 2018)
Consequently, it is hard for to identify artifacts in training, data specific biases and intents from a complex trained model like neural rankers even if trained purely on text features. We show how such a system can effectively help a user understand the results of neural rankers and highlight areas of improvement.
Link: https://arxiv.org/abs/1809.03857
====================================================
Visualizing Convolutional Neural Networks to Improve Decision Support for Skin Lesion Classification (Pieter Van Molle - 11 September, 2018)
However, as neural networks are black box function approximators, it is difficult, if not impossible, for a medical expert to reason about their output
Link: https://arxiv.org/abs/1809.03851
====================================================
Long-Term Occupancy Grid Prediction Using Recurrent Neural Networks (Marcel Schreiber - 11 September, 2018)
We tackle the long-term prediction of scene evolution in a complex downtown scenario for automated driving based on Lidar grid fusion and recurrent neural networks (RNNs)
Link: https://arxiv.org/abs/1809.03782
====================================================
3D Human Body Reconstruction from a Single Image via Volumetric Regression (Aaron S. Jackson - 11 September, 2018)
This paper proposes the use of an end-to-end Convolutional Neural Network for direct reconstruction of the 3D geometry of humans via volumetric regression
Link: https://arxiv.org/abs/1809.03770
====================================================
Non-blind Image Restoration Based on Convolutional Neural Network (Kazutaka Uchida - 11 September, 2018)
Blind image restoration processors based on convolutional neural network (CNN) are intensively researched because of their high performance
Link: https://arxiv.org/abs/1809.03757
====================================================
Factorized Q-Learning for Large-Scale Multi-Agent Systems (Yong Chen - 24 September, 2018)
Furthermore, we utilize a composite deep neural network architecture for computing the factorized Q-function, share the model parameters among all the agents within the same group, and estimate the agents' optimal joint actions through a coordinate descent type algorithm
Link: https://arxiv.org/abs/1809.03738
====================================================
Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding (Nan Rosemary Ke - 11 September, 2018)
The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps
Link: https://arxiv.org/abs/1809.03702
====================================================
Visual Attention Model for Cross-sectional Stock Return Prediction and End-to-End Multimodal Market Representation Learning (Ran Zhao - 11 September, 2018)
We apply a convolutional neural network over this market image to build market features in a hierarchical way. We use a recurrent neural network, with an attention mechanism over the market feature maps, to model temporal dynamics in the market
Link: https://arxiv.org/abs/1809.03684
====================================================
CNN-Based Signal Detection for Banded Linear Systems (Congmin Fan - 11 September, 2018)
Motivated by recent advances in deep learning, we propose to design a high-accuracy low-complexity signal detector for banded linear systems based on convolutional neural networks (CNNs). Through extensive numerical experiments, we demonstrate that the proposed CNN-based detector outperforms conventional deep neural networks and existing model-based detectors in both accuracy and computational time
Link: https://arxiv.org/abs/1809.03682
====================================================
Comparing Computing Platforms for Deep Learning on a Humanoid Robot (Alexander Biddulph - 10 September, 2018)
The experiments addressed a number of benchmarking tasks including pedestrian detection using deep neural networks
Link: https://arxiv.org/abs/1809.03668
====================================================
Neural Animation and Reenactment of Human Actor Videos (Lingjie Liu - 10 September, 2018)
Technically, this is achieved by training a neural network that translates simple synthetic images of a human character into realistic imagery. Our results outperform the state-of-the-art in learning-based human image synthesis
Link: https://arxiv.org/abs/1809.03658
====================================================
Evaluation of Preference of Multimedia Content using Deep Neural Networks for Electroencephalography (Seong-Eun Moon - 11 September, 2018)
In this paper, we propose a novel method using deep neural networks toward improved modeling of EEG and thereby improved recognition accuracy
Link: https://arxiv.org/abs/1809.03650
====================================================
Unsupervised Cross-lingual Transfer of Word Embedding Spaces (Ruochen Xu - 10 September, 2018)
We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.
Link: https://arxiv.org/abs/1809.03633
====================================================
Detecting Gang-Involved Escalation on Social Media Using Context (Serina Chang - 10 September, 2018)
Incorporating context in our Convolutional Neural Network (CNN) leads to a significant improvement.
Link: https://arxiv.org/abs/1809.03632
====================================================
URBAN-i: From urban scenes to mapping slums, transport modes, and pedestrians in cities using deep learning and computer vision (Mohamed R. Ibrahim - 10 September, 2018)
In this paper, we introduce a new method for multipurpose realistic-dynamic urban modelling relying on deep learning and computer vision, using deep Convolutional Neural Networks (CNN), to sense and detect informality and slums in urban scenes from aerial and street view images in addition to detection of pedestrian and transport modes
Link: https://arxiv.org/abs/1809.03609
====================================================
Learning Named Entity Tagger using Domain-Specific Dictionary (Jingbo Shang - 10 September, 2018)
After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.
Link: https://arxiv.org/abs/1809.03599
====================================================
Improving Question Answering by Commonsense-Based Pre-Training (Wanjun Zhong - 5 October, 2018)
Results show that incorporating commonsense-based function improves the state-of-the-art on two question answering tasks that require commonsense reasoning. Further analysis shows that our system discovers and leverages useful evidences from an external commonsense knowledge base, which is missing in existing neural network models and help derive the correct answer.
Link: https://arxiv.org/abs/1809.03568
====================================================
Deep Learning Towards Mobile Applications (Ji Wang - 10 September, 2018)
However, there exist many challenges to realize deep learning in mobile applications, including the contradiction between the miniature nature of mobile devices and the resource requirement of deep neural networks, the privacy and security concerns about individuals' data, and so on
Link: https://arxiv.org/abs/1809.03559
====================================================
Energy Disaggregation via Deep Temporal Dictionary Learning (Mahdi Khodayar - 10 September, 2018)
Motivated by such shortcomings, we propose a novel optimization program where the dictionary and its sparse coefficients are optimized simultaneously with a deep neural model extracting powerful nonlinear features from the energy signals. Real experiments on the publicly available Reference Energy Disaggregation Dataset (REDD) show significant improvement compared to the state-of-the-art methodologies in terms of the disaggregation accuracy and F-score metrics.
Link: https://arxiv.org/abs/1809.03534
====================================================
Towards a Fatality-Aware Benchmark of Probabilistic Reaction Prediction in Highly Interactive Driving Scenarios (Wei Zhan - 10 September, 2018)
There is no existing unified framework to homogenize the problem formulation, representation simplification, and evaluation metric for various prediction methods, such as probabilistic graphical models (PGM), neural networks (NN) and inverse reinforcement learning (IRL)
Link: https://arxiv.org/abs/1809.03478
====================================================
SpRRAM: A Predefined Sparsity Based Memristive Neuromorphic Circuit for Low Power Application (Arash Fayyazi - 10 September, 2018)
In this paper, we propose an efficient predefined structured sparsity-based ex-situ training framework for a hybrid CMOS-memristive neuromorphic hardware for deep neural network to significantly lower the power consumption and computational complexity and improve scalability
Link: https://arxiv.org/abs/1809.03476
====================================================
Deep Single-View 3D Object Reconstruction with Visual Hull Embedding (Hanqing Wang - 10 September, 2018)
With the aid of deep convolutional neural networks (CNNs), 3D object reconstruction has witnessed a significant progress in recent years
Link: https://arxiv.org/abs/1809.03451
====================================================
Inverse-Consistent Deep Networks for Unsupervised Deformable Image Registration (Jun Zhang - 10 September, 2018)
Previous deep-learning studies usually employ supervised neural networks to directly learn the spatial transformation from one image to another, requiring task-specific ground-truth registration for model training. We evaluate our method on T1-weighted brain magnetic resonance imaging (MRI) scans for tissue segmentation and anatomical landmark detection, with results demonstrating the superior performance of our ICNet over several state-of-the-art approaches for deformable image registration
Link: https://arxiv.org/abs/1809.03443
====================================================
Not Just Privacy: Improving Performance of Private Deep Learning in Mobile Cloud (Ji Wang - 18 September, 2018)
The increasing demand for on-device deep learning services calls for a highly efficient manner to deploy deep neural networks (DNNs) on mobile devices with limited capacity
Link: https://arxiv.org/abs/1809.03428
====================================================
Filling Missing Paths: Modeling Co-occurrences of Word Pairs and Dependency Paths for Recognizing Lexical Semantic Relations (Koki Washio - 10 September, 2018)
In this paper, we propose novel methods with a neural model of $P(path|w_1, w_2)$ to solve this problem. Our experimental results demonstrate that our methods improve on previous neural approaches based on dependency paths and successfully solve the focused problem.
Link: https://arxiv.org/abs/1809.03411
====================================================
Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space (Koki Washio - 10 September, 2018)
In this paper, we propose a novel model of this pattern-based approach, neural latent relational analysis (NLRA). In addition, when combined with a vector offset model, NLRA achieves a performance comparable to that of the state-of-the-art model that exploits additional semantic relational data.
Link: https://arxiv.org/abs/1809.03401
====================================================
Probabilistic Binary Neural Networks (Jorn W. T. Peters - 10 September, 2018)
In this work, we present a probabilistic training method for Neural Network with both binary weights and activations, called BLRNet. By embracing stochasticity during training, we circumvent the need to approximate the gradient of non-differentiable functions such as sign(), while still obtaining a fully Binary Neural Network at test time
Link: https://arxiv.org/abs/1809.03368
====================================================
Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks (AdriÃ  Recasens - 10 September, 2018)
We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task
Link: https://arxiv.org/abs/1809.03355
====================================================
Towards Efficient Convolutional Neural Network for Domain-Specific Applications on FPGA (Ruizhe Zhao - 4 September, 2018)
FPGA becomes a popular technology for implementing Convolutional Neural Network (CNN) in recent years
Link: https://arxiv.org/abs/1809.03318
====================================================
A Global Alignment Kernel based Approach for Group-level Happiness Intensity Estimation (Xiaohua Huang - 3 September, 2018)
We first exploit Riesz-based volume local binary pattern (RVLBP) and deep convolutional neural network (CNN) based features for characterizing facial images. Our experimental results demonstrate that the proposed approach achieves promising performance for group happiness intensity analysis, when compared with the recent state-of-the-art methods.
Link: https://arxiv.org/abs/1809.03313
====================================================
MANTIS: Model-Augmented Neural neTwork with Incoherent k-space Sampling for efficient MR T2 mapping (Fang Liu - 2 September, 2018)
The purpose of this work was to develop and evaluate a novel deep learning-based reconstruction framework called Model-Augmented Neural neTwork with Incoherent k-space Sampling (MANTIS) for efficient MR parameter mapping
Link: https://arxiv.org/abs/1809.03308
====================================================
Multilingual Extractive Reading Comprehension by Runtime Machine Translation (Akari Asai - 10 September, 2018)
In this paper, we introduce the first extractive RC systems for non-English languages without using language-specific RC training data, but instead by using an English RC model and an attention-based Neural Machine Translation (NMT) model. Experimental results in two non-English languages, namely Japanese and French, show that our method significantly outperforms a back-translation baseline of a state-of-the-art product-level machine translation system
Link: https://arxiv.org/abs/1809.03275
====================================================
Privacy-Preserving Deep Learning for any Activation Function (Le Trieu Phong - 10 September, 2018)
We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks
Link: https://arxiv.org/abs/1809.03272
====================================================
Finding Better Topologies for Deep Convolutional Neural Networks by Evolution (Honglei Zhang - 10 September, 2018)
Due to the nonlinearity of artificial neural networks, designing topologies for deep convolutional neural networks (CNN) is a challenging task and often only heuristic approach, such as trial and error, can be applied
Link: https://arxiv.org/abs/1809.03242
====================================================
Multi-Context Deep Network for Angle-Closure Glaucoma Screening in Anterior Segment OCT (Huazhu Fu - 10 September, 2018)
Based on clinical priors, we formulate this learning with a presented Multi-Context Deep Network (MCDN) architecture, in which parallel Convolutional Neural Networks are applied to particular image regions and at corresponding scales known to be informative for clinically diagnosing angle-closure glaucoma
Link: https://arxiv.org/abs/1809.03239
====================================================
Adaptive Behavior Generation for Autonomous Driving using Deep Reinforcement Learning with Compact Semantic States (Peter Wolf - 10 September, 2018)
The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description
Link: https://arxiv.org/abs/1809.03214
====================================================
Learning to Generate Structured Queries from Natural Language with Indirect Supervision (Ziwei Bai - 10 September, 2018)
An end-to-end neural model integrating with reinforcement learning is proposed to learn SQL generation policy within the answer-driven learning paradigm
Link: https://arxiv.org/abs/1809.03195
====================================================
Improving Response Selection in Multi-turn Dialogue Systems (Debanjan Chaudhuri - 19 September, 2018)
This work proposes a novel neural network architecture for response selection in an end-to-end multi-turn conversational dialogue setting. Experimental results show that our model outperforms all other state-of-the-art methods for response selection in multi-turn conversations.
Link: https://arxiv.org/abs/1809.03194
====================================================
Recent Advances in Object Detection in the Age of Deep Convolutional Neural Networks (Shivang Agarwal - 10 September, 2018)
This strong interest can be explained not only by the importance this task has for many applications but also by the phenomenal advances in this area since the arrival of deep convolutional neural networks (DCNN). This survey also reviews the public datasets and associated state-of-the-art algorithms.
Link: https://arxiv.org/abs/1809.03193
====================================================
Fast and Efficient Information Transmission with Burst Spikes in Deep Spiking Neural Networks (Seongsik Park - 10 September, 2018)
Another type of neural coding, called phase coding, also determines the amount of information being transmitted according to a global reference oscillator, and therefore, is inefficient in hidden layers where dynamics of neurons can change. Furthermore, we introduce a novel hybrid neural coding scheme that uses different neural coding schemes for different types of layers
Link: https://arxiv.org/abs/1809.03142
====================================================
Deep MR Image Super-Resolution Using Structural Priors (Venkateswararao Cherukuri - 10 September, 2018)
Our contributions are then incorporating these priors in an analytically tractable fashion in the learning of a convolutional neural network (CNN) that accomplishes the super-resolution task
Link: https://arxiv.org/abs/1809.03140
====================================================
Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers (Zhen He - 10 September, 2018)
To achieve both label-free and end-to-end learning of MOT, we propose a Tracking-by-Animation framework, where a differentiable neural model first tracks objects from input frames and then animates these objects into reconstructed frames
Link: https://arxiv.org/abs/1809.03137
====================================================
Memristive LSTM network hardware architecture for time-series predictive modeling problem (Kazybek Adam - 9 September, 2018)
With the rapid development of artificial neural networks, long short-term memory (LSTM) recurrent neural network (RNN) configuration is found to be capable in dealing with time-series forecasting problems where data points are time-dependent and possess seasonality trends
Link: https://arxiv.org/abs/1809.03119
====================================================
Stochastic Gradient Descent Learns State Equations with Nonlinear Activations (Samet Oymak - 9 September, 2018)
This relation is the backbone of recurrent neural networks (e.g
Link: https://arxiv.org/abs/1809.03019
====================================================
Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring? (Lena Reed - 9 September, 2018)
While neural generation methods integrate sentence planning and surface realization in one end- to-end learning framework, previous work has not shown that neural generators can: (1) perform common sentence planning and discourse structuring operations; (2) make decisions as to whether to realize content in a single sentence or over multiple sentences; (3) generalize sentence planning and discourse relation operations beyond what was seen in training. We systematically create large training corpora that exhibit particular sentence planning operations and then test neural models to see what they learn
Link: https://arxiv.org/abs/1809.03015
====================================================
Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability (Kai Y. Xiao - 26 September, 2018)
We explore the concept of co-design in the context of neural network verification. Specifically, we aim to train deep neural networks that not only are robust to adversarial perturbations but also whose robustness can be verified more easily
Link: https://arxiv.org/abs/1809.03008
====================================================
Speeding Up Neural Machine Translation Decoding by Cube Pruning (Wen Zhang - 9 September, 2018)
Although neural machine translation has achieved promising results, it suffers from slow translation speed. We apply cube pruning, a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation
Link: https://arxiv.org/abs/1809.02992
====================================================
LS-Net: Learning to Solve Nonlinear Least Squares for Monocular Stereo (Ronald Clark - 9 September, 2018)
In this paper, we propose LS-Net, a neural nonlinear least squares optimization algorithm which learns to effectively optimize these cost functions even in the presence of adversities
Link: https://arxiv.org/abs/1809.02966
====================================================
Automated Strabismus Detection based on Deep neural networks for Telemedicine Applications (Zhun Fan - 29 September, 2018)
Then a new algorithm based on deep neural networks is proposed to achieve automated strabismus detection on the founded tele strabismus dataset. In the second stage, a deep convolutional neural networks is built and trained in order to classify the segmented eye regions as strabismus or normal
Link: https://arxiv.org/abs/1809.02940
====================================================
Mammalian Brain Inspired Localization Algorithms with von Mises Distributions (Tsang-Kai Chang - 8 September, 2018)
Biological agents still outperform the artificial counterparts in navigating the first-visited environments, even with the advance of deep neural networks nowadays
Link: https://arxiv.org/abs/1809.02910
====================================================
Interpreting Neural Networks With Nearest Neighbors (Eric Wallace - 8 September, 2018)
However, the confidence of neural networks is not a robust measure of model uncertainty. We address this by changing the test-time behavior of neural networks using Deep k-Nearest Neighbors
Link: https://arxiv.org/abs/1809.02847
====================================================
Neural Guided Constraint Logic Programming for Program Synthesis (Lisa Zhang - 13 September, 2018)
We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.
Link: https://arxiv.org/abs/1809.02840
====================================================
Context-Free Transductions with Neural Stacks (Yiding Hao - 8 September, 2018)
This paper analyzes the behavior of stack-augmented recurrent neural network (RNN) models
Link: https://arxiv.org/abs/1809.02836
====================================================
Faithful Multimodal Explanation for Visual Question Answering (Jialin Wu - 8 September, 2018)
Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA)
Link: https://arxiv.org/abs/1809.02805
====================================================
CNNs for Surveillance Footage Scene Classification (Utkarsh Contractor - 8 September, 2018)
We additionally use network visualization techniques to gain insight into what the neural network sees, and the basis of the classification decision
Link: https://arxiv.org/abs/1809.02766
====================================================
RealPoint3D: Point Cloud Generation from a Single Image with Complex Background (Yan Xia - 7 September, 2018)
3D point cloud generation by the deep neural network from a single image has been attracting more and more researchers' attention. Experimental results show that the proposed framework achieves state-of-the-art accuracy compared to other volumetric-based and point set generation methods
Link: https://arxiv.org/abs/1809.02743
====================================================
Operations Guided Neural Networks for High Fidelity Data-To-Text Generation (Feng Nie - 7 September, 2018)
Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. In this paper, we attempt to improve the fidelity of neural data-to-text generation by utilizing pre-executed symbolic operations
Link: https://arxiv.org/abs/1809.02735
====================================================
DensSiam: End-to-End Densely-Siamese Network with Self-Attention Model for Object Tracking (Mohamed H. Abdelpakey - 7 September, 2018)
Convolutional Siamese neural networks have been recently used to track objects using deep features
Link: https://arxiv.org/abs/1809.02714
====================================================
Neural Machine Translation of Logographic Languages Using Sub-character Level Information (Longtu Zhang - 7 September, 2018)
Recent neural machine translation (NMT) systems have been greatly improved by encoder-decoder models with attention mechanisms and sub-word units
Link: https://arxiv.org/abs/1809.02694
====================================================
Coherence-Aware Neural Topic Modeling (Ran Ding - 7 September, 2018)
In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process
Link: https://arxiv.org/abs/1809.02687
====================================================
Reservoir Computing based Neural Image Filters (Samiran Ganguly - 7 September, 2018)
In this work, we explore the use of reservoir computing, a dynamical neural network model inspired from biological systems, in creating dynamic image filtering systems that extracts signal from noise using inverse modeling
Link: https://arxiv.org/abs/1809.02651
====================================================
Accelerating Deep Neural Networks with Spatial Bottleneck Modules (Junran Peng - 7 September, 2018)
This paper presents an efficient module named spatial bottleneck for accelerating the convolutional layers in deep neural networks
Link: https://arxiv.org/abs/1809.02601
====================================================
HyperGCN: Hypergraph Convolutional Networks for Semi-Supervised Classification (Naganand Yadati - 7 September, 2018)
In particular, we propose HyperGCN, an SSL method which uses a layer-wise propagation rule for convolutional neural networks operating directly on hypergraphs
Link: https://arxiv.org/abs/1809.02589
====================================================
MixUp as Locally Linear Out-Of-Manifold Regularization (Hongyu Guo - 7 September, 2018)
MixUp, a data augmentation approach through mixing random samples, has been shown to be able to significantly improve the predictive accuracy of the current art of deep neural networks
Link: https://arxiv.org/abs/1809.02499
====================================================
On the Importance of Visual Context for Data Augmentation in Scene Understanding (Nikita Dvornik - 6 September, 2018)
Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not
Link: https://arxiv.org/abs/1809.02492
====================================================
Convolutional Neural Network: Text Classification Model for Open Domain Question Answering System (Muhammad Zain Amin - 7 September, 2018)
Neural network model is trained on top of word embedding. We further propose a method to integrate Convolutional Neural Network Classifier to an open domain question answering system
Link: https://arxiv.org/abs/1809.02479
====================================================
Metamorphic Relation Based Adversarial Attacks on Differentiable Neural Computer (Alvin Chan - 7 September, 2018)
Differentiable neural computer (DNC) is a novel computing machine with DNN as its central controller operating on an external memory module for data processing. The unique architecture of DNC contributes to its state-of-the-art performance in tasks which requires the ability to represent variables and data structure as well as to store data over long timescales
Link: https://arxiv.org/abs/1809.02444
====================================================
HC-Net: Memory-based Incremental Dual-Network System for Continual learning (Jangho Kim - 7 September, 2018)
Training a neural network for a classification task typically assumes that the data to train are given from the beginning. This usually leads to the catastrophic forgetting problem which is inevitable for the traditional training methodology of neural networks
Link: https://arxiv.org/abs/1809.02441
====================================================
Improving Neural Question Generation using Answer Separation (Yanghoon Kim - 7 September, 2018)
Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Consequently, our model significantly outperforms previous state-of-the-art NQG models.
Link: https://arxiv.org/abs/1809.02393
====================================================
Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models (Takashi Wada - 7 September, 2018)
The proposed model, which we call multilingual neural language models, takes sentences of multiple languages as an input
Link: https://arxiv.org/abs/1809.02306
====================================================
Data Augmentation for Spoken Language Understanding via Joint Variational Generation (Kang Min Yoo - 7 September, 2018)
Recent works in neural text generative models, particularly latent variable models such as variational autoencoder (VAE), have shown promising results in regards to generating plausible and natural sentences
Link: https://arxiv.org/abs/1809.02305
====================================================
Dynamic Compositionality in Recursive Neural Networks with Structure-aware Tag Representations (Taeuk Kim - 6 September, 2018)
We show that models built upon the proposed architecture obtain superior performance on several sentence-level tasks such as sentiment analysis and natural language inference when compared against previous tree-structured models and other sophisticated neural models. In particular, our models achieve new state-of-the-art results on Stanford Sentiment Treebank, Movie Review, and Text Retrieval Conference datasets.
Link: https://arxiv.org/abs/1809.02286
====================================================
Deep Neural Net with Attention for Multi-channel Multi-touch Attribution (Ning li - 6 September, 2018)
We present Deep Neural Net With Attention multi-touch attribution model (DNAMTA) model in a supervised learning fashion of predicting if a series of events leads to conversion, and it leads us to have a deep understanding of the dynamic interaction effects between media channels
Link: https://arxiv.org/abs/1809.02230
====================================================
Logical Rule Induction and Theory Learning Using Neural Theorem Proving (Andres Campero - 12 September, 2018)
Our approach is based on a novel neural forward-chaining differentiable rule induction network
Link: https://arxiv.org/abs/1809.02193
====================================================
DRAG: Deep Reinforcement Learning Based Base Station Activation in Heterogeneous Networks (Junhong Ye - 6 September, 2018)
To avoid prohibitively high computational and storage costs of conventional tabular-based approaches, we propose to use deep neural networks to approximate the policy and value functions in the AC approach
Link: https://arxiv.org/abs/1809.02159
====================================================
Deep neural network marketplace recommenders in online experiments (Simen Eide - 6 September, 2018)
This paper focuses on the challenge of measuring recommender performance and summarizes the online experiment results with several promising types of deep neural network recommenders - hybrid item representation models combining features from user engagement and content, sequence-based models, and multi-armed bandit models that optimize user engagement by re-ranking proposals from multiple submodels
Link: https://arxiv.org/abs/1809.02130
====================================================
A Memory-Network Based Solution for Multivariate Time-Series Forecasting (Yen-Yu Chang - 6 September, 2018)
To address such concerns, various deep learning models, mainly Recurrent Neural Network (RNN) based methods, are proposed. Furthermore, lack-of-explainability remains one serious drawback for deep neural network models
Link: https://arxiv.org/abs/1809.02105
====================================================
