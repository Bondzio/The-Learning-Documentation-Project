If there are any errors
please Abort, and run `arxiv_required` for required package installation, and start again
Please wait while we phrase the requested information from global arxiv[arxiv.org] servers 
------------>
---------------------------->
------------------------------------------------------>
 
An Analysis of Approaches Taken in the ACM RecSys Challenge 2018 for Automatic Music Playlist Continuation (Hamed Zamani - 2 October, 2018)
The ACM Recommender Systems Challenge 2018 focused on the task of automatic music playlist continuation, which is a form of the more general task of sequential recommendation. Given a playlist of arbitrary length with some additional meta-data, the task was to recommend up to 500 tracks that fit the target characteristics of the original playlist. In total, 113 teams submitted 1,228 runs to the main track; 33 teams submitted 239 runs to the creative track. The highest performing team in the main track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average number of recommended songs clicks of 1.784. In the creative track, an R-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was obtained by the best team
Link: https://arxiv.org/abs/1810.01520
====================================================
Deep Learning of Human Perception in Audio Event Classification (Yi Yu - 3 September, 2018)
In the experiments, we record brain activities (EEG signals) of several subjects while they are listening to music events of 8 audio categories selected from Google AudioSet, using a 16-channel EEG headset with active electrodes
Link: https://arxiv.org/abs/1809.00502
====================================================
Online classification of imagined speech using functional near-infrared spectroscopy signals (Alborz Rezazadeh Sereshkeh - 2 September, 2018)
Most brain-computer interfaces (BCIs) based on functional near-infrared spectroscopy (fNIRS) require that users perform mental tasks such as motor imagery, mental arithmetic, or music imagery to convey a message or to answer simple yes or no questions. In this paper, a 3-class intuitive BCI is presented which enables users to directly answer yes or no questions by covertly rehearsing the word 'yes' or 'no' for 15 s. Twelve participants each completed one offline block and six online blocks over the course of 2 sessions. By the final online block, 9 out of 12 participants were performing above chance (p<0.001), with a 3-class accuracy of 83.8+9.4%. Even when considering all participants, the average online 3-class accuracy over the last 3 blocks was 64.1+20.6%, with only 3 participants scoring below chance (p<0.001)
Link: https://arxiv.org/abs/1809.00395
====================================================
A Machine Learning Driven IoT Solution for Noise Classification in Smart Cities (Yasser Alsouda - 1 September, 2018)
We evaluate our approach experimentally with a dataset of about 3000 sound samples grouped in eight sound classes (such as, car horn, jackhammer, or street music). We achieve a noise classification accuracy in the range 85% -- 100%. Training and testing of our k-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than a second for a dataset with features of more than 3000 sound samples.
Link: https://arxiv.org/abs/1809.00238
====================================================
Large-Scale Cover Song Detection in Digital Music Libraries Using Metadata, Lyrics and Audio Features (Albin Andrew Correya - 30 August, 2018)
In this work, we investigate whether textual music information (such as metadata and lyrics) can be used along with audio for large-scale cover identification problem in a wide digital music library. By only leveraging standard tf-idf based text similarity measures on song titles and lyrics, we achieved 35.5% of absolute increase in mean average precision compared to the current scalable audio content-based state of the art methods on MSD
Link: https://arxiv.org/abs/1808.10351
====================================================
Extended playing techniques: The next milestone in musical instrument recognition (Vincent Lostanlen - 29 August, 2018)
Yet, although the automatic recognition of a musical instrument from the recording of a single "ordinary" note is considered a solved problem, automatic identification of instrumental playing technique (IPT) remains largely underdeveloped. We benchmark machine listening systems for query-by-example browsing among 143 extended IPTs for 16 instruments, amounting to 469 triplets of instrument, mute, and technique. Evaluating on the Studio On Line (SOL) dataset, we obtain a precision at rank 5 of 99.7% for instrument recognition (baseline at 89.0%) and of 61.0% for IPT recognition (baseline at 44.5%)
Link: https://arxiv.org/abs/1808.09730
====================================================
Improving Networked Music Performance Systems Using Application-Network Collaboration (Emmanouil Lakiotakis - 6 September, 2018)
Networked Music Performance (NMP) systems involve musicians located in different places who perform music while staying synchronized via the Internet. The maximum end-to-end delay in NMP applications is called Ensemble Performance Threshold (EPT) and should be less than 25 milliseconds
Link: https://arxiv.org/abs/1808.09405
====================================================
Application-Network Collaboration Using SDN for Ultra-Low Delay Teleorchestras (Emmanouil Lakiotakis - 28 August, 2018)
Networked Music Performance (NMP) constitutes a class of ultra-low delay sensitive applications, allowing geographically separate musicians to perform seamlessly as a tele-orchestra. For this application type, the QoS indicator is the mouth-to-ear delay, which should be kept under 25 milliseconds. Emulation results show that the proposed scheme achieves an improvement of up to 59% in mouth-to-ear delay over the existing passive solutions.
Link: https://arxiv.org/abs/1808.09399
====================================================
Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations (Guangxiang Zhao - 22 August, 2018)
The biggest challenge lies in the complicated relations of music styles. Especially, the micro F1 is improved from 53.9 to 64.5, and the one-error is reduced from 30.5 to 22.6
Link: https://arxiv.org/abs/1808.07604
====================================================
Predicting Musical Sophistication from Music Listening Behaviors: A Preliminary Study (Bruce Ferwerda - 22 August, 2018)
An analysis of data from a study with 61 participants shows that listening behaviors can successfully be used to infer users' musical sophistication.
Link: https://arxiv.org/abs/1808.07314
====================================================
Automatic Playlist Continuation through a Composition of Collaborative Filters (Irene Teinemaa - 13 August, 2018)
The RecSys Challenge 2018 focused on automatic playlist continuation, i.e., the task was to recommend additional music tracks for playlists based on the playlist's title and/or a subset of the tracks that it already contains. The solution obtained the 12th place out of 112 participating teams in the final leaderboard. Team Latte participated in the main track of the challenge of the RecSys Challenge 2018.
Link: https://arxiv.org/abs/1808.04288
====================================================
Device-directed Utterance Detection (Sri Harish Mallidi - 7 August, 2018)
Consider the example interaction: $"Computer,~play~music", "Computer,~reduce~the~volume"$. Experimental results show that ASR decoder, acoustic embeddings, and 1-best embeddings yield an equal-error-rate (EER) of $9.3~\%$, $10.9~\%$ and $20.1~\%$, respectively. Combination of the features resulted in a $44~\%$ relative improvement and a final EER of $5.2~\%$.
Link: https://arxiv.org/abs/1808.02504
====================================================
How to analyze ambivalence when studying technology acceptance: The case of VJ software automation (Anna Spagnolli - 30 July, 2018)
In a preliminary phase of the study seven stakeholders (VJs, DJs, promoters and venue owners) were interviewed face-to-face, and 102 more via an online survey, about their view of the live music performance sector and of the software they use. The ambivalent points surfaced during this phase were further unpacked in a subsequent, more focused set of interviews with 25 DJs and VJs
Link: https://arxiv.org/abs/1807.11322
====================================================
The Supernumerary Robotic 3rd Thumb for Skilled Music Tasks (James Cunningham - 1 August, 2018)
A proof of concept validation experiment has been conducted to show the effectiveness of the robotic finger in playing musical pieces on a grand piano, showing that naive users were able to use it for 11 finger play within a few hours.
Link: https://arxiv.org/abs/1807.08274
====================================================
A Line in the Sand: Recommendation or Ad-hoc Retrieval? (Surya Kallumadi - 20 July, 2018)
Our proposed solution using ad-hoc retrieval models achieved a competitive performance on the music recommendation task at RecSys 2018 challenge---finishing at rank 7 out of 112 participating teams and at rank 5 out of 31 teams for the main and the creative tracks, respectively.
Link: https://arxiv.org/abs/1807.08061
====================================================
What kind of content are you prone to tweet? Multi-topic Preference Model for Tweeters (Lorena Recalde - 18 July, 2018)
For example, in the context of Twitter, aligned with his/her preferences a user may tweet and retweet more about technology than sports and do not share any music-related content. In the present work, we propose a method based on the Mixed Gaussian Model to extract the multidimensional preference representation for 399 Ecuadorian tweeters concerning twenty-two different topics (or dimensions) which became known by manually categorizing 68.186 tweets
Link: https://arxiv.org/abs/1807.07162
====================================================
A Survey Investigating Usage of Virtual Personal Assistants (Mateusz Dubiel - 12 July, 2018)
According to recent studies, currently the usage of VPAs is constrained to basic tasks such as checking facts, playing music, and obtaining weather updates.In this paper, we present results of a survey (N = 118) that analyses usage of VPAs by frequent and infrequent users
Link: https://arxiv.org/abs/1807.04606
====================================================
Direction of Arrival Estimation for Nanoscale Sensor Networks (Shree M. Prasad - 12 July, 2018)
In this paper, using the well-known MUltiple SIgnal Classification (MUSIC) algorithm, we study DOA estimation for NWSNs for different energy levels, distances, pulse shapes, and frequencies. Our analyses reveal that the best DOA estimation is achieved with the first order Gaussian pulses, which emit their peak energy at 6 THz. Based on Monte Carlo simulations, we demonstrate that MUSIC algorithm is capable of estimating DOA with root mean square error less than one degree from a distance of around 6 meter for pulse energy as little as 1 atto Joule.
Link: https://arxiv.org/abs/1807.04435
====================================================
Denoising Auto-encoder with Recurrent Skip Connections and Residual Regression for Music Source Separation (Jen-Yu Liu - 5 July, 2018)
Convolutional neural networks with skip connections have shown good performance in music source separation. The ARC model with residual regression achieves 5.74 siganl-to-distoration ratio (SDR) in vocals with MUSDB in SiSEC 2018. We also evaluate the ARC model alone on the older dataset DSD100 (used in SiSEC 2016) and it achieves 5.91 SDR in vocals.
Link: https://arxiv.org/abs/1807.01898
====================================================
Exploratory Analysis of a Large Flamenco Corpus using an Ensemble of Convolutional Neural Networks as a Structural Annotation Backend (Nadine Kroher - 29 June, 2018)
The reported findings show that it is feasible to perform a large scale analysis of flamenco music with state-of-the-art classification technology and produce automatically extracted descriptors that are both musicologically valid and useful, in the sense that they can enhance conventional metadata schemes and assist bridging the semantic gap between audio recordings and high-level musicological concepts.
Link: https://arxiv.org/abs/1807.00069
====================================================
Evaluating language models of tonal harmony (David R. W. Sears - 22 June, 2018)
However, few studies employing such models have evaluated the most state-of-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock.
Link: https://arxiv.org/abs/1806.08724
====================================================
A Predictive Model for Music Based on Learned Interval Representations (Stefan Lattner - 22 June, 2018)
We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably
Link: https://arxiv.org/abs/1806.08686
====================================================
Emotion-Recognition Using Smart Watch Sensor Data: Mixed-Design Study (Juan C. Quiroz - 22 June, 2018)
The experimental design is a mixed-design study; within-subjects (emotions; happy, sad, neutral) and between-subjects (stimulus type: audio-visual "movie clips", audio "music clips"). They also had to answer a short questionnaire (20 items; PANAS) before and after experiencing each emotion. For the task of emotion recognition using classifiers, our results show that the personal models outperformed personal baselines, and achieved median accuracies higher than 78% for all conditions of the design study for the binary classification of happiness vs sadness
Link: https://arxiv.org/abs/1806.08518
====================================================
Towards an efficient deep learning model for musical onset detection (Rong Gong - 19 June, 2018)
In this paper, we propose an efficient and reproducible deep learning model for musical onset detection (MOD). However, it has only 28.3% of the total number of trainable parameters compared to the state-of-the-art
Link: https://arxiv.org/abs/1806.06773
====================================================
A data-driven approach to mid-level perceptual musical feature modeling (Anna Aljanaki - 13 June, 2018)
We collect and release a dataset\footnote{https://osf.io/5aupt/} of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation
Link: https://arxiv.org/abs/1806.04903
====================================================
Dual-Primal Graph Convolutional Networks (Federico Monti - 3 June, 2018)
We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.
Link: https://arxiv.org/abs/1806.00770
====================================================
A Sequential Embedding Approach for Item Recommendation with Heterogeneous Attributes (Kuan Liu - 28 May, 2018)
These approaches also fail to combine recurrent neural networks which have recently shown effectiveness in item recommendations in applications such as video and music browsing. HA-RNN extends recurrent neural networks with 1) a hierarchical attribute combination input layer and 2) an output attribute embedding layer
Link: https://arxiv.org/abs/1805.11008
====================================================
Deep Watershed Detector for Music Object Recognition (Lukas Tuggener - 26 May, 2018)
We present state-of-the-art detection results of common music symbols and show DWD's ability to work with synthetic scores equally well as on handwritten music.
Link: https://arxiv.org/abs/1805.10548
====================================================
A Hybrid Approach to Music Playlist Continuation Based on Playlist-Song Membership (Andreu Vall - 24 May, 2018)
Experimental results on two datasets of curated music playlists show that the proposed playlist continuation model compares to a state-of-the-art collaborative filtering model in the ideal situation of extending playlists profiled at training time and where songs occurred frequently in training playlists
Link: https://arxiv.org/abs/1805.09557
====================================================
Music Source Separation Using Stacked Hourglass Networks (Sungheon Park - 22 June, 2018)
Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks.
Link: https://arxiv.org/abs/1805.08559
====================================================
Do You Like What I Like? Similarity Estimation in Proximity-based Mobile Social Networks (Felix Beierle - 19 May, 2018)
Using real user data, we show that a Counting Bloom Filter with a single hash function and a length of 128 is sufficient to accurately estimate the similarity between two multisets representing the musical tastes of two users
Link: https://arxiv.org/abs/1805.07651
====================================================
Computing Information Quantity as Similarity Measure for Music Classification Task (Ayaka Takamoto - 15 April, 2018)
The number of correct results was compared with that of the CDM for the composer estimation task of five composers of 75 piano musical scores
Link: https://arxiv.org/abs/1804.05486
====================================================
Transcribing Lyrics From Commercial Song Audio: The First Step Towards Singing Content Processing (Che-Ping Tsai - 15 April, 2018)
We collected music-removed version of English songs directly from commercial singing content. The WER achieved (73.90%) was significantly lower than the baseline (96.21%), but still relatively high.
Link: https://arxiv.org/abs/1804.05306
====================================================
Music Genre Classification using Machine Learning Techniques (Hareesh Bahuleyan - 3 April, 2018)
Categorizing music files according to their genre is a challenging task in the area of music information retrieval (MIR). The experiments are conducted on the Audio set data set and we report an AUC value of 0.894 for an ensemble classifier which combines the two proposed approaches.
Link: https://arxiv.org/abs/1804.01149
====================================================
ShIFT: A Semi-haptic Interface for Flute Tutoring (Gus Xia - 18 March, 2018)
Haptic interfaces open another door to the music world for the vast majority of beginners when traditional training methods are not effective. Experiments show that our semi-haptic interface is effective as long as learners are not "tone deaf." Using our prototype device, the learning rate is about 30% faster compared to learning from videos.
Link: https://arxiv.org/abs/1803.06625
====================================================
Investigating the Effect of Music and Lyrics on Spoken-Word Recognition (Odette Scharenborg - 13 March, 2018)
This paper addresses this knowledge gap by investigating 1) whether the masking effect of background music with lyrics is larger than that of music without lyrics, and 2) whether the masking effect is larger for more complex music
Link: https://arxiv.org/abs/1803.05058
====================================================
Entity-Aware Language Model as an Unsupervised Reranker (Mohammad Sadegh Rasooli - 17 June, 2018)
Experiments in the music domain demonstrate that global features, as well as features extracted from an external knowledge-base, can be incorporated into our reranker. Our final model, a simple ensemble of a language model and reranker, achieves a 0.44\% absolute word error rate improvement over an LSTM language model on the blind test data.
Link: https://arxiv.org/abs/1803.04291
====================================================
Convolutional Neural Network Achieves Human-level Accuracy in Music Genre Classification (Mingwen Dong - 26 February, 2018)
Music genre classification is one example of content-based analysis of music signals. Traditionally, human-engineered features were used to automatize this task and 61% accuracy has been achieved in the 10-genre classification. However, it's still below the 70% accuracy that humans could achieve in the same task. After training, this method achieves human-level (70%) accuracy and the filters learned in the CNN resemble the spectrotemporal receptive field (STRF) in the auditory system.
Link: https://arxiv.org/abs/1802.09697
====================================================
Complex ISNMF: a Phase-Aware Model for Monaural Audio Source Separation (Paul Magron - 30 September, 2018)
Experiments conducted on a musical source separation task in a semi-informed setting show that the proposed approach outperforms state-of-the-art phase-aware separation techniques.
Link: https://arxiv.org/abs/1802.03156
====================================================
Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices (Ramyad Hadidi - 21 March, 2018)
To demonstrate Musical Chair, on a network of Raspberry PIs (up to 12) each connected to a camera, we implement a state-of-the-art action recognition model for videos and two recognition models for images
Link: https://arxiv.org/abs/1802.02138
====================================================
MaD TwinNet: Masker-Denoiser Architecture with Twin Networks for Monaural Sound Source Separation (Konstantinos Drossos - 1 February, 2018)
In this work we present a novel deep learning based method that learns long-term temporal patterns and structures of a musical piece. We evaluate our method using the Demixing Secret Dataset and we obtain an increment to signal-to-distortion ratio (SDR) of 0.37 dB and to signal-to-interference ratio (SIR) of 0.23 dB, compared to previous SOTA results.
Link: https://arxiv.org/abs/1802.00300
====================================================
Staff line Removal using Generative Adversarial Networks (Aishik Konwer - 5 June, 2018)
It is a challenging task to simultaneously reduce the noise and also retain the quality of music symbol context in ancient degraded music score images. For evaluation we consider the ICDAR/GREC 2013 staff removal database
Link: https://arxiv.org/abs/1801.07141
====================================================
Automatic Classification of Music Genre using Masked Conditional Neural Networks (Fady Medhat - 16 January, 2018)
MCLNN have achieved competitive performance on the Ballroom music dataset compared to several hand-crafted attempts and outperformed models based on state-of-the-art Convolutional Neural Networks.
Link: https://arxiv.org/abs/1801.05504
====================================================
Towards Deep Modeling of Music Semantics using EEG Regularizers (Francisco Raposo - 15 December, 2017)
We evaluate the learned semantic space in a transfer learning context, by using it as an audio feature extractor in an independent dataset and proxy task: music audio-lyrics cross-modal retrieval. We show that our embedding model outperforms Spotify features and performs comparably to a state-of-the-art embedding model that was trained on 700 times more data
Link: https://arxiv.org/abs/1712.05197
====================================================
The organization of a three-manual keyboard for 53-tone tempered and other tempered systems (Vladimir P. Burskii - 10 December, 2017)
Specifically, a numerical comparison of the different musical temperaments among themselves in the degree of approximation of the Pythagorean scale is provided, and thus it numerically substantiates the thesis that the 53-tone tempered system is the most advanced among possible others. Here were proposed some schemes of the three-manual keyboard for the implementation of 53-tone temperament, which are also implemented at the same time for 12 -, 17 -, 24 -, 29 - and 41-sounding system
Link: https://arxiv.org/abs/1712.03569
====================================================
Representations of Sound in Deep Learning of Audio Features from Music (Sergey Shuvaev - 7 December, 2017)
Our trained network demonstrates accurate performance on such classification tasks when presented with 5 s examples of music obtained by simple transformations of the raw audio waveform. Networks based on logarithmic filter banks and RMT were able to correctly guess the one composer out of 31 possibilities in 68 and 84 percent of cases respectively.
Link: https://arxiv.org/abs/1712.02898
====================================================
Now Playing: Continuous low-power music recognition (Blaise AgÃ¼era y Arcas - 29 November, 2017)
Our presented system, Now Playing, has a daily battery usage of less than 1% on average, respects user privacy by running entirely on-device and can passively recognize a wide range of music.
Link: https://arxiv.org/abs/1711.10958
====================================================
Identification of potential Music Information Retrieval technologies for computer-aided jingju singing training (Rong Gong - 2 November, 2017)
To this intent, we answer the question: how the jingju singing tutors and trainees value the importance of each jingju musical dimension-intonation, rhythm, loudness, tone quality and pronunciation? This is done by (i) classifying the classroom singing practices, tutor's verbal feedbacks into these 5 dimensions, (ii) surveying the trainees
Link: https://arxiv.org/abs/1711.07551
====================================================
Audio-to-score alignment of piano music using RNN-based automatic music transcription (Taegyun Kwon - 13 November, 2017)
We propose a framework for audio-to-score alignment on piano performance that employs automatic music transcription (AMT) using neural networks. One predicts the presence of 88 notes or 12 chroma in frame-level and the other detects note onsets in 12 chroma. The result shows that the alignment framework with the learned features significantly improves the accuracy, achieving less than 10 ms in mean onset error.
Link: https://arxiv.org/abs/1711.04480
====================================================
The ACCompanion v0.1: An Expressive Accompaniment System (Carlos Cancino-ChacÃ³n - 7 November, 2017)
The expressiveness of the system is powered by the Basis-Mixer, a state-of-the-art computational model of expressive music performance
Link: https://arxiv.org/abs/1711.02427
====================================================
Does Phase Matter For Monaural Source Separation? (Mohit Dubey - 2 November, 2017)
In this paper, we seek to address whether preserving phase information in spectral representations of sound provides better results in monaural separation of vocals from a musical track by using a neurally plausible sparse generative model. Furthermore, our proposed method achieves state-of-the-art performance for source separation, as quantified by a mean signal to interference ratio (GSIR) of 19.46.
Link: https://arxiv.org/abs/1711.00913
====================================================
Adversarial Semi-Supervised Audio Source Separation applied to Singing Voice Extraction (Daniel Stoller - 6 April, 2018)
The state of the art in music source separation employs neural networks trained in a supervised fashion on multi-track databases to estimate the sources from a given mixture
Link: https://arxiv.org/abs/1711.00048
====================================================
Onsets and Frames: Dual-Objective Piano Transcription (Curtis Hawthorne - 5 June, 2018)
We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset
Link: https://arxiv.org/abs/1710.11153
====================================================
Sample-level CNN Architectures for Music Auto-tagging Using Raw Waveforms (Taejun Kim - 13 February, 2018)
In this paper, we improve the 1-D CNN architecture for music auto-tagging by adopting building blocks from state-of-the-art image classification models, ResNets and SENets, and adding multi-level feature aggregation to it
Link: https://arxiv.org/abs/1710.10451
====================================================
Representation Learning of Music Using Artist Labels (Jiyoung Park - 18 June, 2018)
The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels
Link: https://arxiv.org/abs/1710.06648
====================================================
Improving Compression Based Dissimilarity Measure for Music Score Analysis (Ayaka Takamoto - 3 October, 2017)
Our application is a music score analysis. We have selected 75 famous pieces (15 pieces for each composer)
Link: https://arxiv.org/abs/1710.01446
====================================================
Networks of Music Groups as Success Predictors (Dmitry Zinoviev - 3 August, 2017)
More than 4,600 non-academic music groups emerged in the USSR and post-Soviet independent nations in 1960--2015, performing in 275 genres
Link: https://arxiv.org/abs/1709.08995
====================================================
Linear Computer-Music through Sequences over Galois Fields (H. M. de Oliveira - 19 September, 2017)
New e-compositions and music score are made available, including a new piece from the famous Lenna picture: the score of the e-music <<Between Lenna's eyes in C major.>> The corresponding stretch of music score are presented. Some particular structures, including clock arithmetic (mod 12), GF(7), GF(8), GF(13) and GF(17) are addressed. As an example, Pascal multilevel block codes recently introduced are handled to generate a new style of electronic music over GF(13).
Link: https://arxiv.org/abs/1709.06663
====================================================
A Categorical Approach for Recognizing Emotional Effects of Music (Mohsen Sahraei Ardakani - 17 September, 2017)
Accuracy for recognizing each label has been calculated; where the results show that epic music can be recognized more accurately (77.4%), comparing to the other types of music.
Link: https://arxiv.org/abs/1709.05684
====================================================
A Recurrent Encoder-Decoder Approach with Skip-filtering Connections for Monaural Singing Voice Separation (Stylianos Ioannis Mimilakis - 24 April, 2018)
The objective of deep learning methods based on encoder-decoder architectures for music source separation is to approximate either ideal time-frequency masks or spectral representations of the target music source(s). Compared to previous methods that approximate time-frequency masks, our method has increased performance of signal to distortion ratio by an average of 3.8 dB.
Link: https://arxiv.org/abs/1709.00611
====================================================
Generating Video Descriptions with Topic Guidance (Shizhe Chen - 4 September, 2017)
First, videos cover a broader range of topics, such as news, music, sports and so on. Our comprehensive experimental results on the current largest video caption dataset MSR-VTT prove the effectiveness of our topic-guided model, which significantly surpasses the winning performance in the 2016 MSR video to language challenge.
Link: https://arxiv.org/abs/1708.09666
====================================================
Ephemeral Context to Support Robust and Diverse Music Recommendations (Pavel Kucherbaev - 9 August, 2017)
While prior work on context-based music recommendation focused on fixed set of contexts (e.g. With our approach, we address the problems which current approaches face: 1) a limited ability to infer context from missing or faulty sensor data; 2) an inability to use contextual information to support novel content discovery.
Link: https://arxiv.org/abs/1708.02765
====================================================
Detecting Noteheads in Handwritten Scores with ConvNets and Bounding Box Regression (Jan HajiÄ Jr. - 5 August, 2017)
We present ongoing work on a simple notehead detector using convolutional neural networks for pixel classification and bounding box regression that achieves a detection f-score of 0.97 on binary score images in the MUSCIMA++ dataset, does not require staff removal, and is applicable to a variety of handwriting styles and levels of musical complexity.
Link: https://arxiv.org/abs/1708.01806
====================================================
Audio Super Resolution using Neural Networks (Volodymyr Kuleshov - 2 August, 2017)
Our method is simple and does not involve specialized audio processing techniques; in our experiments, it outperforms baselines on standard speech and music benchmarks at upscaling ratios of 2x, 4x, and 6x
Link: https://arxiv.org/abs/1708.00853
====================================================
Multi-label Music Genre Classification from Audio, Text, and Images Using Deep Features (Sergio Oramas - 16 July, 2017)
In this work we aim to expand this task by categorizing musical items into multiple and fine-grained labels, using three different data modalities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes
Link: https://arxiv.org/abs/1707.04916
====================================================
Optical Music Recognition with Convolutional Sequence-to-Sequence Models (Eelco van der Wel - 16 July, 2017)
We present a deep learning architecture called a Convolutional Sequence-to-Sequence model to both move towards an end-to-end trainable OMR pipeline, and apply a learning process that trains on full sentences of sheet music instead of individually labeled symbols. With the introduced augmentations a pitch recognition accuracy of 81% and a duration accuracy of 94% is achieved, resulting in a note level accuracy of 80%
Link: https://arxiv.org/abs/1707.04877
====================================================
Graph Based Recommendations: From Data Representation to Feature Extraction and Application (Amit Tiroshi - 5 July, 2017)
The proposed approach is domain-independent (demonstrated on data from movies, music, and business recommender systems), and is evaluated using several state-of-the-art machine learning methods, on different recommendation tasks, and using different evaluation metrics
Link: https://arxiv.org/abs/1707.01250
====================================================
An Augmented Lagrangian Method for Piano Transcription using Equal Loudness Thresholding and LSTM-based Decoding (Sebastian Ewert - 30 July, 2017)
An important variant is instrument-dependent music transcription where methods can use calibration data for the instruments in use. However, despite the additional information, results rarely exceed an f-measure of 80%. Despite their simplicity, our two extensions lead to a drop of about 35% in note error rate compared to the state-of-the-art.
Link: https://arxiv.org/abs/1707.00160
====================================================
Vision-based Detection of Acoustic Timed Events: a Case Study on Clarinet Note Onsets (A. Bazzica - 28 June, 2017)
Knowledge of visual information can aid the understanding of complex auditory scenes, even when only a stereo mixdown is available in the audio domain, \eg identifying which musicians are playing in large musical ensembles. We release an audiovisual dataset with 4.5 hours of clarinetist videos together with cleaned annotations which include about 36,000 onsets and the coordinates for a number of salient points and regions of interest
Link: https://arxiv.org/abs/1706.09556
====================================================
Classical Music Clustering Based on Acoustic Features (Xindi Wang - 27 June, 2017)
In this paper we cluster 330 classical music pieces collected from MusicNet database based on their musical note sequence
Link: https://arxiv.org/abs/1706.08928
====================================================
Proceedings of the First International Workshop on Deep Learning and Music (Dorien Herremans - 27 June, 2017)
Proceedings of the First International Workshop on Deep Learning and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017
Link: https://arxiv.org/abs/1706.08675
====================================================
Multi-Level and Multi-Scale Feature Aggregation Using Sample-level Deep Convolutional Neural Networks for Music Classification (Jongpil Lee - 21 June, 2017)
We show that this approach achieves state-of-the-art results on several music classification datasets.
Link: https://arxiv.org/abs/1706.06810
====================================================
RARD: The Related-Article Recommendation Dataset (Joeran Beel - 20 June, 2017)
While there are many datasets for recommender systems in the domains of movies, books, and music, there are rather few datasets from research-paper recommender systems. The dataset contains information about 57.4 million recommendations that were displayed to the users of Sowiport. The dataset is available at http://data.mr-dlib.org, and published under the Creative Commons Attribution 3.0 Unported (CC-BY) license.
Link: https://arxiv.org/abs/1706.03428
====================================================
Setting Players' Behaviors in World of Warcraft through Semi-Supervised Learning (Marcelo Souza Nery - 8 June, 2017)
Digital games are one of the major and most important fields on the entertainment domain, which also involves cinema and music. Our analysis is carried out using data obtained from the game "World of Warcraft" over 3 years (2006 $-$ 2009)
Link: https://arxiv.org/abs/1706.02780
====================================================
Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition (Miroslav Malik - 7 June, 2017)
The method was evaluated using the 'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for arousal and 0.268 for valence, which is the best result reported on this dataset.
Link: https://arxiv.org/abs/1706.02292
====================================================
Music Playlist Continuation by Learning from Hand-Curated Examples and Song Features: Alleviating the Cold-Start Problem for Rare and Out-of-Set Songs (Andreu Vall - 7 September, 2017)
Automated music playlist generation is a specific form of music recommendation. For example, both approaches achieve a recall@100 of roughly 35% for songs occurring in 5 or more training playists, whereas the proposed model achieves a recall@100 of roughly 15% for songs occurring in 4 or less training playlists, compared to the 3% achieved by collaborative filtering.
Link: https://arxiv.org/abs/1705.08283
====================================================
Musical Instrument Recognition Using Their Distinctive Characteristics in Artificial Neural Networks (Babak Toghiani-Rizi - 14 May, 2017)
In this study an Artificial Neural Network was trained to classify musical instruments, using audio samples transformed to the frequency domain. The study concluded that in comparison with the base experiment, that had an accuracy of 93.5%, using the attack only resulted in 80.2% and the initial 100 Hz in 64.2%.
Link: https://arxiv.org/abs/1705.04971
====================================================
Note Value Recognition for Piano Transcription Using Markov Random Fields (Eita Nakamura - 7 July, 2017)
Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Evaluation results show that our method reduces the average error rate by around 40 percent compared to existing/simple methods
Link: https://arxiv.org/abs/1703.08144
====================================================
Adaptive Multi-Class Audio Classification in Noisy In-Vehicle Environment (Myounggyu Won - 21 March, 2017)
More than 420 minutes of audio data including various genres of music, speech, speech+music, and noise are collected from diverse driving environments. The results demonstrate that the proposed approach improves the average classification accuracy up to 166%, and 64% for speech, and speech+music, respectively, compared with a non-adaptive approach in our experimental settings.
Link: https://arxiv.org/abs/1703.07065
====================================================
Gestalt Phenomenon in Music? A Neurocognitive Physics Study with EEG (Shankha Sanyal - 19 March, 2017)
In this work we have tried to analyze the effect of different frequency bands of music on the various frequency rhythms of human brain obtained from EEG data of 5 participants. These resonant frequency bands were presented to the subjects as auditory stimulus and EEG signals recorded simultaneously in 19 different locations of the brain
Link: https://arxiv.org/abs/1703.06491
====================================================
In Search of a Dataset for Handwritten Optical Music Recognition: Introducing MUSCIMA++ (Jan HajiÄ jr. - 14 March, 2017)
Optical Music Recognition (OMR) has long been without an adequate dataset and ground truth for evaluating OMR systems, which has been a major problem for establishing a state of the art in the field. The MUSCIMA++ dataset version 0.9 consists of 140 pages of handwritten music, with 91255 manually annotated notation symbols and 82261 explicitly marked relationships between symbol pairs
Link: https://arxiv.org/abs/1703.04824
====================================================
Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained Convolutional Neural Networks for Music Auto-tagging (Jongpil Lee - 7 June, 2017)
Our experiments show that using the combination of multi-level and multi-scale features is highly effective in music auto-tagging and the proposed method outperforms previous state-of-the-arts on the MagnaTagATune dataset and the Million Song Dataset
Link: https://arxiv.org/abs/1703.01793
====================================================
Sample-level Deep Convolutional Neural Networks for Music Auto-tagging Using Raw Waveforms (Jongpil Lee - 22 May, 2017)
This approach was applied to musical signals as well but has been not fully explored yet. 2 or 3 samples) beyond typical frame-level input representations. Our experiments show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results comparable to previous state-of-the-art performances for the Magnatagatune dataset and Million Song Dataset
Link: https://arxiv.org/abs/1703.01789
====================================================
Recurrent Poisson Factorization for Temporal Recommendation (Seyed Abbas Hosseini - 4 March, 2017)
Furthermore, we demonstrate RPF's superior performance over many state-of-the-art methods on synthetic dataset, and large scale real-world datasets on music streaming logs, and user-item interactions in M-Commerce platforms.
Link: https://arxiv.org/abs/1703.01442
====================================================
Collision Resolution and Interference Elimination in Multiaccess Communication Networks (Naeem Akl - 28 February, 2017)
This subspace is orthogonal to the noise subspace at a receiver and the signals within the subspace can be extracted using the root-MUSIC method. Without synchronization, the worst case asymptotic performance is still greater than the $50\%$ throughput achieved by collision resolution algorithms and interference management techniques like interference alignment.
Link: https://arxiv.org/abs/1703.00134
====================================================
On the Futility of Learning Complex Frame-Level Language Models for Chord Recognition (Filip Korzeniowski - 31 March, 2017)
Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system
Link: https://arxiv.org/abs/1702.00178
====================================================
An Experimental Analysis of the Entanglement Problem in Neural-Network-based Music Transcription Systems (Rainer Kelz - 31 January, 2017)
Several recent polyphonic music transcription systems have utilized deep neural networks to achieve state of the art results on various benchmark datasets, pushing the envelope on framewise and note-level performance measures
Link: https://arxiv.org/abs/1702.00025
====================================================
Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices (Eita Nakamura - 28 January, 2017)
In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model (HMM) that explicitly describes the multiple-voice structure of polyphonic music. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances
Link: https://arxiv.org/abs/1701.08343
====================================================
Universal Audio Steganalysis Based on Calibration and Reversed Frequency Resolution of Human Auditory System (Hamzeh Ghasemzadeh - 18 September, 2017)
The system detects steghide at capacity of 0.06 bit per symbol (BPS) with sensitivity of 98.6% (music) and 78.5% (speech). These figures are respectively 7.1% and 27.5% higher than state-of-the-art results based on RMFCC.
Link: https://arxiv.org/abs/1701.05614
====================================================
Creating A Multi-track Classical Musical Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications (Bochen Li - 7 August, 2018)
The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks
Link: https://arxiv.org/abs/1612.08727
====================================================
Audio-based Distributional Semantic Models for Music Auto-tagging and Similarity Measurement (Giannis Karamanolakis - 26 December, 2016)
Acoustic-semantic models are shown to outperform the state-of-the-art for this task and produce high quality tags for audio/music clips.
Link: https://arxiv.org/abs/1612.08391
====================================================
Basis-Function Modeling of Loudness Variations in Ensemble Performance (Thassilo Gadermaier - 16 December, 2016)
We test both linear and non-linear variants of the extended model n a data set of audio recordings of symphonic music, in a leave-one-out setting. Even if the accuracy of the predicted loudness varies from one recording to another, in several cases the model explains well over 50% of the variance in loudness.
Link: https://arxiv.org/abs/1612.05432
====================================================
A Fully Convolutional Deep Auditory Model for Musical Chord Recognition (Filip Korzeniowski - 15 December, 2016)
We show that the learned auditory system extracts musically interpretable features, and that the proposed chord recognition system achieves results on par or better than state-of-the-art algorithms.
Link: https://arxiv.org/abs/1612.05082
====================================================
Live Score Following on Sheet Music Images (Matthias Dorfer - 15 December, 2016)
Instead of relying on some symbolic representation, we are using a multi-modal convolutional neural network to match the incoming audio stream directly to sheet music images. Nonetheless, the audience will have the opportunity to test our implementation themselves via 3 simple piano pieces.
Link: https://arxiv.org/abs/1612.05076
====================================================
Adaptive DCTNet for Audio Signal Classification (Yin Xian - 29 April, 2017)
Experimental results show that the A-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art performance in bird song classification rate, and improve artist identification accuracy in music data
Link: https://arxiv.org/abs/1612.04028
====================================================
An algorithm to assign musical prime commas to every prime number and construct a universal and compact free Just Intonation musical notation (David Ryan - 28 March, 2017)
Musical frequencies in Just Intonation are comprised of rational numbers. The larger component uses only integer powers of the first two primes, 2 and 3. The smaller component decomposes into a series of microtonal adjustments, one for each prime number 5 and above present in the original frequency. Results for DR are presented for primes below 1400
Link: https://arxiv.org/abs/1612.01860
====================================================
FMA: A Dataset For Music Analysis (MichaÃ«l Defferrard - 5 September, 2017)
We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres
Link: https://arxiv.org/abs/1612.01840
====================================================
A Non Linear Approach towards Automated Emotion Analysis in Hindustani Music (Shankha Sanyal - 1 December, 2016)
With the help of this technique, 3 min alap portion of six conventional ragas of Hindustani classical music namely, Darbari Kanada, Yaman, Mian ki Malhar, Durga, Jay Jayanti and Hamswadhani played in three different musical instruments were analyzed
Link: https://arxiv.org/abs/1612.00172
====================================================
Decision-Based Transcription of Jazz Guitar Solos Using a Harmonic Bident Analysis Filter Bank and Spectral Distribution Weighting (Stanislaw Gorlow - 20 November, 2016)
It resorts to a very intuitive representation of tonal music signals: the pitchgram. Essentially, the proposed transcriber is a decision tree, thus a classifier, with a depth of 3. We achieve an improvement of 34% w.r.t. the reference system and 19% w.r.t. Another measure of accuracy, the error score, attests that the number of erroneous pitch detections is reduced by more than 50% w.r.t. the reference system and by 45% w.r.t
Link: https://arxiv.org/abs/1611.06505
====================================================
VR 'SPACE OPERA': Mimetic Spectralism in an Immersive Starlight Audification System (Benedict Carey - 9 November, 2016)
In this scene music is generated, where the harmonic material is determined based on observations of light variation from pulsating stars, that would have theoretically been overhead on the 1st of October 8000 BC at 23:00 and animal calls based on the reliefs in the temple. Based on the observations of the stars V465 Per, HD 217860, 16 Lac, BG CVn and KIC 6382916, frequency collections were derived and applied to the generation of musical sound and notation sequences within a custom VR environment using a novel method incorporating spectralist techniques
Link: https://arxiv.org/abs/1611.03081
====================================================
Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control (Natasha Jaques - 16 October, 2017)
The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation
Link: https://arxiv.org/abs/1611.02796
====================================================
Maximum entropy models for generation of expressive music (Simon Moulieras - 12 October, 2016)
In this paper, we show how Maximum Entropy (MaxEnt) models can be used to generate musical expression in order to mimic a human performance. As a training corpus, we had a professional pianist play about 150 melodies of jazz, pop, and latin jazz
Link: https://arxiv.org/abs/1610.03606
====================================================
Applying Topological Persistence in Convolutional Neural Network for Music Audio Signals (Jen-Yu Liu - 26 August, 2016)
Our evaluation on automatic music tagging, a multi-label classification task, shows that the resulting persistent convolutional neural network (PCNN) model can perform significantly better than state-of-the-art models in prediction accuracy
Link: https://arxiv.org/abs/1608.07373
====================================================
StegIbiza: New Method for Information Hiding in Club Music (Krzysztof Szczypiorski - 9 August, 2016)
The evaluation of the system was performed for several music samples (with and without StegIbiza enabled) on a selected group of testers who had a music background. Finally, for the worst case scenario, none of them could identify any differences in the audio with a 1% margin of changed tempo.
Link: https://arxiv.org/abs/1608.02988
====================================================
Model-based STFT phase recovery for audio source separation (Paul Magron - 27 February, 2018)
Experiments conducted on realistic music pieces show that, given accurate magnitude estimates, this procedure outperforms the state-of-the-art consistent Wiener filter.
Link: https://arxiv.org/abs/1608.01953
====================================================
Understanding Communication Patterns in MOOCs: Combining Data Mining and qualitative methods (Rebecca Eynon - 25 July, 2016)
Within a few years, the phenomenon of crowd-based learning has gained enormous popularity with millions of learners across the globe participating in courses ranging from Popular Music to Astrophysics. They have captured the imaginations of many, attracting significant media attention - with The New York Times naming 2012 "The Year of the MOOC." For those engaged in learning analytics and educational data mining, MOOCs have provided an exciting opportunity to develop innovative methodologies that harness big data in education.
Link: https://arxiv.org/abs/1607.07495
====================================================
Explaining Deep Convolutional Neural Networks on Music Classification (Keunwoo Choi - 8 July, 2016)
Deep convolutional neural networks (CNNs) have been actively adopted in the field of music information retrieval, e.g. We introduce auralisation of a CNN to understand its underlying mechanism, which is based on a deconvolution procedure introduced in [2]
Link: https://arxiv.org/abs/1607.02444
====================================================
Acoustic scene classification using convolutional neural network and multiple-width frequency-delta data augmentation (Yoonchang Han - 8 July, 2016)
In particular, convolutional neural networks (ConvNets) exploit spatially local correlations across input data to improve the performance of audio processing tasks, such as speech recognition, musical chord recognition, and onset detection. We describe calculation results using the DCASE 2016 challenge dataset, which shows that ConvNet outperforms both of the baseline system with hand-crafted features and a deep neural network approach by around 7%. The performance was further improved (by 5.7%) using the MWFD augmentation together with folded mean aggregation. The system exhibited a classification accuracy of 0.831 when classifying 15 acoustic scenes.
Link: https://arxiv.org/abs/1607.02383
====================================================
Gender and Interest Targeting for Sponsored Post Advertising at Tumblr (Mihajlo Grbovic - 23 June, 2016)
Advertisers can tell their story through images, animation, text, music, video, and more, and promote that content by sponsoring it to appear as an advertisement in the streams of Tumblr users. The model was trained on a large-scale data set consisting of 6.8 billion user posts, with very limited amount of categorized keywords, and was shown to have superior performance over the bag-of-words model. We successfully deployed gender and interest targeting capability in Yahoo production systems, delivering inference for users that cover more than 90% of daily activities at Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20% lift in user engagement with sponsored posts as compared to untargeted campaigns.
Link: https://arxiv.org/abs/1606.07189
====================================================
The "Horse'' Inside: Seeking Causes Behind the Behaviours of Music Content Analysis Systems (Bob L. Sturm - 9 June, 2016)
In this paper, we take a state-of-the-art music content analysis system and investigate what causes it to achieve exceptionally high performance in a benchmark music audio dataset. Our results demonstrate how the initial manifestation of music intelligence in this state-of-the-art can be deceptive
Link: https://arxiv.org/abs/1606.03044
====================================================
Modelling Symbolic Music: Beyond the Piano Roll (Christian Walder - 4 June, 2016)
We further improve our model by augmenting our training data set with transpositions of the original pieces through all musical keys, thereby convincingly advancing the state of the art on these benchmark problems
Link: https://arxiv.org/abs/1606.01368
====================================================
Multi-Label Zero-Shot Learning via Concept Embedding (Ubai Sandouk - 1 June, 2016)
Experimental results of multi-label ZSL on images and music tracks suggest that our approach outperforms a state-of-the-art multi-label ZSL model and can deal with a scenario involving out-of-vocabulary labels without re-training the semantics learning model.
Link: https://arxiv.org/abs/1606.00282
====================================================
Deep convolutional neural networks for predominant instrument recognition in polyphonic music (Yoonchang Han - 26 December, 2016)
We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.
Link: https://arxiv.org/abs/1605.09507
====================================================
Robust Downbeat Tracking Using an Ensemble of Convolutional Networks (S. Durand - 26 May, 2016)
In this paper, we present a novel state of the art system for automatic downbeat tracking from music signals. We then perform an evaluation of our system on a large base of 9 datasets, compare its performance to 4 other published algorithms and obtain a significant increase of 16.8 percent points compared to the second best system, for altogether a moderate cost in test and training
Link: https://arxiv.org/abs/1605.08396
====================================================
Audio Features Affected by Music Expressiveness (Alberto Introini - 17 May, 2016)
Within a Music Information Retrieval perspective, the goal of the study presented here is to investigate the impact on sound features of the musician's affective intention, namely when trying to intentionally convey emotional contents via expressiveness. A preliminary experiment has been performed involving $10$ tuba players
Link: https://arxiv.org/abs/1605.05369
====================================================
Chill-Pass: Using Neuro-Physiological Responses to Chill Music to Defeat Coercion Attacks (Max Wolotsky - 3 May, 2016)
We have experimentally validated our proposed Chill music based CRAS using human subjects and measuring their neuro-physiological responses on our prototype system. Based on the 100 samples collected from the subjects, we were able to successfully authenticate the subjects with an accuracy of over 90\%
Link: https://arxiv.org/abs/1605.01072
====================================================
Music transcription modelling and composition using deep learning (Bob L. Sturm - 29 April, 2016)
We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition
Link: https://arxiv.org/abs/1604.08723
====================================================
Robust Joint Alignment of Multiple Versions of a Piece of Music (Siying Wang - 28 April, 2016)
To establish a link between different versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state-of-the-art method, with significantly less outliers (standard deviation 53% lower).
Link: https://arxiv.org/abs/1604.08516
====================================================
Multifractal Detrended Cross Correlation Analysis A Tool for the Assessment of Raga in Bollywood Music (Shankha Sanyal - 8 April, 2016)
We have taken 3 minute clips of these two ragas from the renderings of two eminent maestros of Hindustani classical music. 3 min clips of ten (10) widely popular songs of Bollywood films were selected for analysis
Link: https://arxiv.org/abs/1604.02243
====================================================
Science Concierge: A fast content-based recommendation system for scientific publications (Titipat Achakulvisut - 11 May, 2016)
Algorithms can help with this task as they help for music, movie, and product recommendations. We tested the library on 15K posters from the Society of Neuroscience Conference 2015
Link: https://arxiv.org/abs/1604.01070
====================================================
Recurrent Neural Networks for Polyphonic Sound Event Detection in Real Life Recordings (Giambattista Parascandolo - 4 April, 2016)
music, car, speech) from 10 different everyday contexts. Overall, our system reports an average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a relative improvement over previous state-of-the-art approach of 6.8% and 15.1% respectively.
Link: https://arxiv.org/abs/1604.00861
====================================================
Singing Voice Separation and Vocal F0 Estimation based on Mutual Combination of Robust Principal Component Analysis and Subharmonic Summation (Yukara Ikemiya - 1 April, 2016)
The proposed method also outperformed all the other methods of singing voice separation submitted to an international music analysis competition called MIREX 2014.
Link: https://arxiv.org/abs/1604.00192
====================================================
Augur: Mining Human Behaviors from Fiction to Power Interactive Systems (Ethan Fast - 25 February, 2016)
We demonstrate Augur-powered, activity-based systems such as a phone that silences itself when the odds of you answering it are low, and a dynamic music player that adjusts to your present activity. A field deployment of an Augur-powered wearable camera resulted in 96% recall and 71% precision on its unsupervised predictions of common daily activities. A second evaluation where human judges rated the system's predictions over a broad set of input images found that 94% were rated sensible.
Link: https://arxiv.org/abs/1602.06977
====================================================
A Full-Bandwidth Audio Codec With Low Complexity And Very Low Delay (Jean-Marc Valin - 17 February, 2016)
We propose an audio codec that addresses the low-delay requirements of some applications such as network music performance. However, at 96 kbit/s and with only 4 ms algorithmic delay, the proposed codec out-performs the ULD codec operating at the same rate. The total complexity of the codec is small, at only 17 WMOPS for real-time operation at 48 kHz.
Link: https://arxiv.org/abs/1602.05311
====================================================
Categorization of Stringed Instruments with Multifractal Detrended Fluctuation Analysis (Archi Banerjee - 28 January, 2016)
Some previous attempts to categorize several musical instruments using various linear analysis methods required a number of parameters to be determined. For this, 30 second sound signals of 26 different string instruments from all over the world were analyzed with the help of non linear multifractal analysis (MFDFA) technique
Link: https://arxiv.org/abs/1601.07709
====================================================
Categorization of Tablas by Wavelet Analysis (Anirban Patranabis - 3 January, 2016)
Tabla, a percussion instrument, mainly used to accompany vocalists, instrumentalists and dancers in every style of music from classical to light in India, mainly used for keeping rhythm. Kumar in 1920 on spectrum modeling of tabla strokes
Link: https://arxiv.org/abs/1601.02489
====================================================
Toward a Robust Diversity-Based Model to Detect Changes of Context (Sylvain Castagnos - 8 January, 2016)
In order to do so, we use a music corpus of 100 users and more than 210,000 consultations (number of songs played in the global history)
Link: https://arxiv.org/abs/1601.01917
====================================================
Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips (Tomohiko Nakamura - 24 December, 2015)
We confirmed real-time operation of the algorithms with music scores of practical length (around 10000 notes) on a modern laptop and their tracking ability to the input performance within 0.7 s on average after repeats/skips in clarinet performance data
Link: https://arxiv.org/abs/1512.07748
====================================================
Predicting the top and bottom ranks of billboard songs using Machine Learning (Vivek Datla - 3 December, 2015)
The music industry is a $130 billion industry. Results indicate that we can classify whether a song belongs to top and bottom of the billboard charts with a precision of 0.76.
Link: https://arxiv.org/abs/1512.01283
====================================================
Low-cost Sensor Glove with Force Feedback for Learning from Demonstrations using Probabilistic Trajectory Representations (Elmar Rueckert - 12 October, 2015)
Sensor gloves are popular input devices for a large variety of applications including health monitoring, control of music instruments, learning sign language, dexterous computer interfaces, and tele-operating robot hands. 250 EUROs) sensor gloves with force feedback can be build, provide an open source software interface for Matlab and present first results in learning object manipulation skills through imitation learning on the humanoid robot iCub.
Link: https://arxiv.org/abs/1510.03253
====================================================
Towards non-threaded Concurrent Constraint Programming for implementing multimedia interaction systems (Mauricio Toro - 11 October, 2015)
In the other hand, the NTCC interpreter was tested with a music improvisation system based on NTCC (CCFOMI), developed by the AVISPA research group and IRCAM. Additionally, we present GECOL 2, a wrapper for the Generic Constraints Development Environment (GECODE) to Common LISP, de- veloped to port the interpreters to Common LISP in the future
Link: https://arxiv.org/abs/1510.03057
====================================================
Processing of acoustical signals via a wavelet-based analysis (Evangelos Matsinos - 30 September, 2015)
A scheme is set forth to determine the optimal values of the parameters of this type of wavelet on the basis of the goodness of the reproduction of a $30$-s audio file containing harmonic signals corresponding to six successive $A$ notes of the chromatic musical scale, from $A_2$ to $A_7$
Link: https://arxiv.org/abs/1509.09113
====================================================
Melodic Contour and Mid-Level Global Features Applied to the Analysis of Flamenco Cantes (Francisco GÃ³mez - 16 September, 2015)
First, we use a state-of-the-art automatic transcription method to account for general melodic similarity from music recordings
Link: https://arxiv.org/abs/1509.04956
====================================================
Background-tracking Acoustic Features for Genre Identification of Broadcast Shows (Oscar Saz - 16 September, 2015)
With this setup, the resulting features can track changes in the audio background like appearance and disappearance of music, applause or laughter, independently of the speakers in the foreground of the audio. In this paper, the performance of these features in a genre identification task in a set of 332 BBC shows is explored. The proposed background--tracking features outperform short--term Perceptual Linear Prediction features in this task using Gaussian Mixture Model classifiers (62% vs 72% accuracy). The use of more complex classifiers, Hidden Markov Models and Support Vector Machines, increases the performance of the system with the novel background--tracking features to 79% and 81% in accuracy respectively.
Link: https://arxiv.org/abs/1509.04934
====================================================
Market Formation as Transitive Closure: the Evolving Pattern of Trade in Music (Jesse Shore - 28 August, 2015)
Where do new markets come from? I construct a network model in which national markets are nodes and flows of recorded music between them are links and conduct a longitudinal analysis of the global pattern of trade in the period 1976 to 2010
Link: https://arxiv.org/abs/1508.07272
====================================================
Learning Contextualized Semantics from Co-occurring Terms via a Siamese Architecture (Ubai Sandouk - 17 June, 2015)
Using various settings in semantic priming, we have carried out a thorough evaluation by comparing our approach to a number of state-of-the-art methods on six annotation corpora in different domains, i.e., MagTag5K, CAL500 and Million Song Dataset in the music domain as well as Corel5K, LabelMe and SUNDatabase in the image domain
Link: https://arxiv.org/abs/1506.05514
====================================================
Neural Adaptive Sequential Monte Carlo (Shixiang Gu - 16 November, 2015)
Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling
Link: https://arxiv.org/abs/1506.03338
====================================================
Learning Contextualized Music Semantics from Tags via a Siamese Network (Ubai Sandouk - 7 June, 2016)
We conduct experiments on three public music tag collections -namely, CAL500, MagTag5K and Million Song Dataset- and compare our approach to a number of state-of-the-art semantics learning approaches
Link: https://arxiv.org/abs/1504.07968
====================================================
The YLI-MED Corpus: Characteristics, Procedures, and Plans (Julia Bernd - 13 March, 2015)
The videos in YLI-MED are categorized as depicting one of ten target events, or no target event, and are annotated for additional attributes like language spoken and whether the video has a musical score. Version 1.0 of YLI-MED includes 1823 "positive" videos that depict the target events and 48,138 "negative" videos, as well as 177 supplementary videos that are similar to event videos but are not positive examples
Link: https://arxiv.org/abs/1503.04250
====================================================
LSTM: A Search Space Odyssey (Klaus Greff - 4 October, 2017)
In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. In total, we summarize the results of 5400 experimental runs ($\approx 15$ years of CPU time), which makes our study the largest of its kind on LSTM networks
Link: https://arxiv.org/abs/1503.04069
====================================================
Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC (Sungjin Ahn - 9 March, 2015)
We also show that our method reduces the prediction error as fast as distributed stochastic gradient descent, achieving a 4.1% improvement in RMSE for the Netflix dataset and an 1.8% for the Yahoo music dataset.
Link: https://arxiv.org/abs/1503.01596
====================================================
Plagiarism Detection in Polyphonic Music using Monaural Signal Separation (Soham De - 27 February, 2015)
Our experiments on a database of about 3000 musical track pairs show that the new feature space characterization produces significant improvements over standard baselines.
Link: https://arxiv.org/abs/1503.00022
====================================================
Vivace: a collaborative live coding language and platform (Vilson Vieira - 30 October, 2017)
Many recent endeavors have focused in live coding both because of aesthetics and as a way to alleviate performance drawbacks when the musical instrument is a computer. The approach is compelling by 1) allowing many performers to code simultaneously, 2) the synthesis of audio and video, 3) a very simple syntax, 4) being a multiplatform software
Link: https://arxiv.org/abs/1502.01312
====================================================
Unsupervised Incremental Learning and Prediction of Music Signals (Ricard Marxer - 23 October, 2015)
A system is presented that segments, clusters and predicts musical audio in an unsupervised manner, adjusting the number of (timbre) clusters instantaneously to the audio input. The flow of the system is as follows: 1) segmentation by onset detection, 2) timbre representation of each segment by Mel frequency cepstrum coefficients, 3) discretization by incremental clustering, yielding a tree of different sound classes (e.g. instruments) that can grow or shrink on the fly driven by the instantaneous sound events, resulting in a discrete symbol sequence, 4) extraction of statistical regularities of the symbol sequence, using hierarchical N-grams and the newly introduced conceptual Boltzmann machine, and 5) prediction of the next sound event in the sequence. Clustering in isolation yields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing voice and drums. Onset detection jointly with clustering achieve an ARI of 81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% / 39.2%.
Link: https://arxiv.org/abs/1502.00524
====================================================
Efficient Algorithms for the Order Preserving Pattern Matching Problem (Simone Faro - 16 January, 2015)
This interesting problem finds applications in a lot of fields as time series analysis, like share prices on stock markets, weather data analysis or to musical melody matching. From our experimental results it turns out that our proposed solutions are up to 2 times faster than the previous solutions reducing the number of false positives up to 99%
Link: https://arxiv.org/abs/1501.04001
====================================================
Consistence beats causality in recommender systems (Xuzhen Zhu - 15 January, 2015)
We further propose a consistence-based algorithm that outperforms the state-of-the-art recommendation algorithms in disparate real data sets, including \textit{Netflix}, \textit{MovieLens}, \textit{Amazon} and \textit{Rate Your Music}.
Link: https://arxiv.org/abs/1501.03577
====================================================
Learning Temporal Dependencies in Data Using a DBN-BLSTM (Kratarth Goel - 23 December, 2014)
We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.
Link: https://arxiv.org/abs/1412.6093
====================================================
Music Data Analysis: A State-of-the-art Survey (Shubhanshu Gupta - 18 November, 2014)
This paper attempts to offer current state-of-the-art in music related analysis
Link: https://arxiv.org/abs/1411.5014
====================================================
Precision-Energy-Throughput Scaling Of Generic Matrix Multiplication and Convolution Kernels Via Linear Projections (Mohammad Ashraful Anam - 11 November, 2014)
Results derived from a voltage- and frequency-scaled ARM Cortex A15 processor running face recognition and music matching algorithms demonstrate that the proposed approach allows for 280%~440% increase of processing throughput and 75%~80% decrease of energy consumption against optimized GEMM and CONV kernels without any impact in the obtained recognition or matching accuracy
Link: https://arxiv.org/abs/1411.2860
====================================================
Real-Time Human-Computer Interaction Based on Face and Hand Gesture Recognition (Reza Azad - 7 August, 2014)
We used the face recognition scheme for viewer verification and the hand gesture recognition in mechanism of computer media player, for instance, volume down/up, next music and etc. Additional the recommended hand gesture recognition method is applied on static American Sign Language (ASL) database and the correctness rate achieved nearby 99.40%
Link: https://arxiv.org/abs/1408.1549
====================================================
Sonic interaction with a virtual orchestra of factory machinery (Laurent Simon - 6 July, 2014)
The application was exhibited during the 2013 Science and Music day and designed to be used in a large immersive system with head tracking, shutter glasses and a 10.2 loudspeaker configuration.
Link: https://arxiv.org/abs/1407.2221
====================================================
On the Application of Generic Summarization Algorithms to Music (Francisco Raposo - 18 June, 2014)
In this paper, we review and apply these algorithms to music. To evaluate this summarization's performance, we adopt an extrinsic approach: we compare a Fado Genre Classifier's performance using truncated contiguous clips against the summaries extracted with those algorithms on 2 different datasets
Link: https://arxiv.org/abs/1406.4877
====================================================
Automatic Fado Music Classification (Pedro GirÃ£o Antunes - 17 June, 2014)
This study aims to develop a tool for automatic detection of Fado music based on the audio signal. Tests were run both in a 10-fold cross-validation setup (97.6% accuracy), and in a traditional train/test setup (95.8% accuracy)
Link: https://arxiv.org/abs/1406.4447
====================================================
Music and Vocal Separation Using Multi-Band Modulation Based Features (Sunil Kumar Kopparapu - 10 June, 2014)
We first identify the distribution of these non-linear features for music only and voice only segments in the audio signal in different Mel spaced frequency bands and show that they have the ability to discriminate. Experimental results show that the discrimination ability is evident in certain low and mid frequency bands (200 - 1500 Hz).
Link: https://arxiv.org/abs/1406.2464
====================================================
Design and Optimization of a Speech Recognition Front-End for Distant-Talking Control of a Music Playback Device (Ramin Pichevar - 5 May, 2014)
The user controls the device through voice, where the speech-to-music ratio can be as low as -30 dB during music playback
Link: https://arxiv.org/abs/1405.1379
====================================================
An Efficient Feature Selection in Classification of Audio Files (Jayita Mitra - 24 March, 2014)
Experimental results indicate that by using GR the application can produce a satisfactory result for music genre classification. After dimensionality reduction best three features have been selected out of various features of audio file and in this technique we will get more than 90% successful classification result.
Link: https://arxiv.org/abs/1404.1491
====================================================
Keep Your Friends Close and Your Facebook Friends Closer: A Multiplex Network Approach to the Analysis of Offline and Online Social Ties (Desislava Hristova - 31 March, 2014)
In this work, we apply a new model that allows us to distinguish between social ties of varying strength, and to observe evidence of homophily with regards to politics, music, health, residential sector & year in college, within the online and offline social network of 74 college students. We are able to identify 75% of close friendships, 90% of weaker ties, and 90% of Facebook friendships as compared to reported ground truth
Link: https://arxiv.org/abs/1403.8034
====================================================
Using perceptually defined music features in music information retrieval (Anders Friberg - 31 March, 2014)
Instead of using concepts from music theory such as tones, pitches, and chords, a set of nine features describing overall properties of the music was selected. The results indicate that (1) at least some of the perceptual features are reliable estimates; (2) emotion ratings could be predicted by a small combination of perceptual features with an explained variance up to 90%; (3) the perceptual features could only to a limited extent be modeled using existing audio features
Link: https://arxiv.org/abs/1403.7923
====================================================
Sequential Complexity as a Descriptor for Musical Similarity (Peter Foster - 28 September, 2014)
We base our evaluation on a dataset of 15500 track excerpts of Western popular music, for which we obtain 7800 web-sourced pairwise similarity ratings. To assess the agreement among similarity ratings, we perform an evaluation under controlled conditions, obtaining a rank correlation of 0.33 between intersected sets of ratings. Combined with bag-of-features descriptors, we obtain performance gains of 31.1% and 10.9% for similarity rating prediction and song year prediction
Link: https://arxiv.org/abs/1402.6926
====================================================
Reverberant Audio Source Separation via Sparse and Low-Rank Modeling (Simon Arberet - 11 December, 2013)
Evaluation on reverberant music mixtures shows that the resulting algorithm improves state-of-the-art methods by more than 2 dB of signal-to-distortion ratio.
Link: https://arxiv.org/abs/1312.2795
====================================================
Idea of a new Personality-Type based Recommendation Engine (Animesh Pandey - 8 November, 2013)
Here an idea is presented where a user can get recommendations for books, web media content, music and movies on the basis of the users' MBTI type. This has a more than 100 features that show the preference of a personality type
Link: https://arxiv.org/abs/1311.2103
====================================================
An Intuitive Design Approach For Implementing Real Time Audio Effects (Mayukh Mukhopadhyay - 4 November, 2013)
Audio effect implementation on random musical signal is a basic application of digital signal processors. In this paper, the compatibility features of MATLAB R2008a with Code Composer Studio 3.3 has been exploited to develop Simulink models which when emulated on TMS320C6713 DSK generate real time audio effects
Link: https://arxiv.org/abs/1311.0842
====================================================
Musical recommendations and personalization in a social network (Dmitry Bugaychenko - 28 October, 2013)
This paper presents a set of algorithms used for music recommendations and personalization in a general purpose social network www.ok.ru, the second largest social network in the CIS visited by more then 40 millions users per day
Link: https://arxiv.org/abs/1310.7428
====================================================
Meme and Variations: A Computer Model of Cultural Evolution (Liane Gabora - 28 September, 2013)
The name is a pun on the classical music form 'theme and variations', because it is based on the premise that novel ideas are variations of old ones; they result from tweaking or combining existing ideas in new ways (Holland et al. 1981)
Link: https://arxiv.org/abs/1309.7524
====================================================
On Optimal and Fair Service Allocation in Mobile Cloud Computing (M. Reza Rahimi - 20 August, 2013)
We propose an efficient heuristic algorithm called MuSIC that is able to perform well (73% of optimal, 30% better than simple strategies), and scale well to a large number of users while ensuring high mobile application QoS. We observe about 25% lower delays and power (under fixed price constraints) and about 35% decrease in price (considering fixed delay) in comparison to only using the public cloud
Link: https://arxiv.org/abs/1308.4391
====================================================
Harmony Perception by Periodicity Detection (Frieder Stolzenburg - 7 October, 2016)
This is shown in this article by consistently applying recent results from psychophysics and neuroacoustics, namely that the just noticeable difference between pitches for humans is about 1% for the musically important low frequency range and that periodicities of complex chords can be detected in the human brain
Link: https://arxiv.org/abs/1306.6458
====================================================
The GTZAN dataset: Its contents, its faults, their effects on evaluation, and its future use (Bob L. Sturm - 7 June, 2013)
The GTZAN dataset appears in at least 100 published works, and is the most-used public dataset for evaluation in machine listening research for music genre recognition (MGR)
Link: https://arxiv.org/abs/1306.1461
====================================================
Do Social Explanations Work? Studying and Modeling the Effects of Social Explanations in Recommender Systems (Amit Sharma - 11 April, 2013)
We present a study of the effects of these social explanations in a music recommendation context. We start with an experiment with 237 users, in which we show explanations with varying levels of social information and analyze their effect on users' decisions
Link: https://arxiv.org/abs/1304.3405
====================================================
Global SPACING Constraint (Technical Report) (Nina Narodytska - 25 March, 2013)
Then, we experimentally evaluate performance of the proposed algorithms on a music composition problem and demonstrate that our filtering algorithms outperform the state-of-the-art approach for solving this problem.
Link: https://arxiv.org/abs/1303.6107
====================================================
Heuristic Ternary Error-Correcting Output Codes Via Weight Optimization and Layered Clustering-Based Approach (Xiao-Lei Zhang - 22 April, 2014)
Results on 14 UCI datasets and a music genre classification problem demonstrate the effectiveness of WOLC-ECOC.
Link: https://arxiv.org/abs/1303.2132
====================================================
Target Estimation in Colocated MIMO Radar via Matrix Completion (Shunqiao Sun - 25 March, 2013)
Under certain conditions, matrix completion techniques can be applied to recover the full receive data matrix, which can then be used in conjunction with array processing techniques, e.g., MUSIC, to obtain target information. Numerical results indicate that good target recovery can be achieved with occupancy of the receive data matrix as low as 50%.
Link: https://arxiv.org/abs/1302.4118
====================================================
Measuring the Significance of the Geographic Flow of Music (Conrad Lee - 23 January, 2013)
We find that information on the past musical preferences in other cities allows a linear model to improve its predictions by approx. 5% over a simple baseline
Link: https://arxiv.org/abs/1301.5586
====================================================
Audio Classical Composer Identification by Deep Neural Network (Zhen Hu - 15 March, 2016)
The famous annual competition, Music Information Retrieval Evaluation eXchange (MIREX), also takes it as one of the four training&testing tasks. We got an accuracy of 76.26% in our data set which is better than some pure models and shallow models
Link: https://arxiv.org/abs/1301.3195
====================================================
High-dimensional sequence transduction (Nicolas Boulanger-Lewandowski - 9 December, 2012)
The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.
Link: https://arxiv.org/abs/1212.1936
====================================================
A Literature Survey of Cooperative Caching in Content Distribution Networks (Jing Zhang - 28 September, 2012)
Content distribution networks (CDNs) which serve to deliver web objects (e.g., documents, applications, music and video, etc.) have seen tremendous growth since its emergence. A recent research paper [15] in the field of large-scale caching for CDN was chosen to be the anchor paper which serves as a guide to the topic
Link: https://arxiv.org/abs/1210.0071
====================================================
Learning Robust Low-Rank Representations (Pablo Sprechmann - 27 September, 2012)
We show the strength of the inclusion of learning to the RPCA approach on a music source separation application, where the encoders outperform the exact RPCA algorithms, which are already reported to produce state-of-the-art results on a benchmark database
Link: https://arxiv.org/abs/1209.6393
====================================================
Throughput Scaling Of Convolution For Error-Tolerant Multimedia Applications (Mohammad Ashraful Anam - 14 January, 2012)
Indicative experimental results with a digital music matching system and an MPEG-7 audio descriptor system demonstrate that the proposed approach offers up to 175% increase in processing throughput against optimized (full-precision) convolution with virtually no effect in the accuracy of the results
Link: https://arxiv.org/abs/1201.3018
====================================================
Fractal String Generation and Its Application in Music Composition (Avishek Ghosh - 27 September, 2011)
Music is a string of some of the notes out of 12 notes (Sa, Komal_re, Re, Komal_ga, Ga, Ma, Kari_ma, Pa, Komal_dha, Dha, Komal_ni, Ni) and their harmonics
Link: https://arxiv.org/abs/1109.6270
====================================================
Pattern Matching under Polynomial Transformation (Ayelet Butman - 28 October, 2011)
Normalised pattern matching plays a key role in fields as diverse as image processing and musical information processing where application specific transformations are often applied to the input. We show that, for any epsilon>0, there cannot exist an O(n m^(1-epsilon)) time algorithm for additive and linear transformations conditional on the hardness of the classic 3SUM problem
Link: https://arxiv.org/abs/1109.1494
====================================================
Characterization and exploitation of community structure in cover song networks (Joan SerrÃ  - 12 September, 2011)
Starting from the output of a state-of-the-art system, songs are embedded in a complex weighted network whose links represent similarity (related musical content)
Link: https://arxiv.org/abs/1108.6003
====================================================
Learning content similarity for music recommendation (Brian McFee - 11 May, 2011)
Current state-of-the-art systems employ collaborative filter methods to represent musical items, effectively comparing items in terms of their constituent users
Link: https://arxiv.org/abs/1105.2344
====================================================
A basic gesture and motion format for virtual reality multisensory applications (Annie Luciani - 25 May, 2010)
The question of encoding movements such as those produced by human gestures may become central in the coming years, given the growing importance of movement data exchanges between heterogeneous systems and applications (musical applications, 3D motion control, virtual reality interaction, etc.). For the past 20 years, various formats have been proposed for encoding movement, especially gestures
Link: https://arxiv.org/abs/1005.4564
====================================================
G3 : GENESIS software envrionment update (Nicolas CastagnÃ© - 24 November, 2009)
GENESIS3 is the new version of the GENESIS software environment for musical creation by means of mass-interaction physics network modeling. It was designed, and developed from scratch, in hindsight of more than 10 years working on and using the previous version
Link: https://arxiv.org/abs/0911.4642
====================================================
On the Interesting World of Fractals and Their Applications to Music (Pabitra Pal Choudhury - 6 October, 2009)
Further, interestingly enough, these very fractals could be a frame of lyrics for the musicians, as we know that the fractal dimension of music is around 1.65 and varies between a high of 1.68 and a low of 1.60
Link: https://arxiv.org/abs/0909.2517
====================================================
Novel Algorithm for Sparse Solutions to Linear Inverse Problems with Multiple Measurements (Lianlin Li - 20 May, 2009)
Moreover several efforts have been made to improve the NESTA-based MMV algorithm, in particular, (1) the NESTA-based MMV algorithm for partially known support to greatly improve the convergence rate, (2) the detection of partial (or all) locations of unknown jointly sparse signals by using so-called MUSIC algorithm; (3) the iterative NESTA-based algorithm by combing hard thresholding technique to decrease the numbers of measurements. [1], where the measurement matrix denoted by A satisfies the so-called restricted isometry property (RIP)
Link: https://arxiv.org/abs/0905.3245
====================================================
Mining User Profiles to Support Structure and Explanation in Open Social Networking (Avare Stewart - 23 December, 2008)
However, by integrating the social activities of music bloggers and listeners, we are able to overcome this limitation: improving the quality of the blogroll neighborhoods, in terms of similarity, by 85 percent when using tracks and by 120 percent when integrating tags from another site.
Link: https://arxiv.org/abs/0812.4461
====================================================
A Grateful Dead Analysis: The Relationship Between Concert and Listening Behavior (Marko A. Rodriguez - 15 July, 2008)
The band played music together from 1965 to 1995 and is well known for concert performances containing extended improvisations and long and unique set lists. This article presents a comparative analysis between 1,590 of the Grateful Dead's concert set lists from 1972 to 1995 and 2,616,990 last.fm Grateful Dead listening events from August 2005 to October 2007. While there is a strong correlation between how songs were played in concert and how they are listened to by last.fm members, the outlying songs in this trend identify interesting aspects of the band and their fans 10 years after the band's dissolution.
Link: https://arxiv.org/abs/0807.2466
====================================================
Cross Entropy Approximation of Structured Covariance Matrices (Cheng-Yuan Liou - 30 August, 2006)
For an array beamforming problem with P incident narrowband point sources, N > P sensors, and colored noise, both approaches yield eigenvector fitting methods similar to that of the MUSIC algorithm[1]. Furthermore, the corresponding cross-entropies are related to the MDL model order selection criterion[2].
Link: https://arxiv.org/abs/cs/0608121
====================================================
Current Trends and Future Research Directions for Interactive Music (Mauricio Toro - 4 October, 2018)
In this review, it is explained and compared different software and formalisms used in music interaction: sequencers, computer-assisted improvisation, meta- instruments, score-following, asynchronous dataflow languages, synchronous dataflow languages, process calculi, temporal constraints and interactive scores
Link: https://arxiv.org/abs/1810.04276
====================================================
Rethinking Recurrent Latent Variable Model for Music Composition (Eunjeong Stella Koh - 7 October, 2018)
We compare the performance of our proposed model with other types of Neural Networks using the criteria of Information Rate that is implemented by Variable Markov Oracle, a method that allows statistical characterization of musical information dynamics and detection of motifs in a song. Our results suggest that the proposed model has a better statistical resemblance to the musical structure of the training data, which improves the creation of new sequences of music in the style of the originals.
Link: https://arxiv.org/abs/1810.03226
====================================================
MeetupNet Dublin: Discovering Communities in Dublin's Meetup Network (Arjun Pakrashi - 6 October, 2018)
A meetup group typically focuses on one specific topic of interest, such as sports, music, language, or technology
Link: https://arxiv.org/abs/1810.03046
====================================================
Disambiguating Music Artists at Scale with Audio Metric Learning (Jimena Royo-Letelier - 3 October, 2018)
We also propose a new negative sampling method for metric learning that takes advantage of side information such as music genre during the learning phase and shows promising results for the artist clustering task.
Link: https://arxiv.org/abs/1810.01807
====================================================
Diversifying Music Recommendations (Houssam Nassif - 2 October, 2018)
We compare submodular and Jaccard methods to diversify Amazon Music recommendations
Link: https://arxiv.org/abs/1810.01482
====================================================
A Lightweight Music Texture Transfer System (Xutan Peng - 27 September, 2018)
However, present methods for music feature transfer using neural networks are far from practical application. In this paper, we initiate a novel system for transferring the texture of music, and release it as an open source project
Link: https://arxiv.org/abs/1810.01248
====================================================
Predicate learning in neural systems: Discovering latent generative structures (Andrea E. Martin - 2 October, 2018)
Humans learn complex latent structures from their environments (e.g., natural language, mathematics, music, social hierarchies)
Link: https://arxiv.org/abs/1810.01127
====================================================
Eigentriads and Eigenprogressions on the Tonnetz (Vincent Lostanlen - 1 October, 2018)
Mozart) from polyphonic music pieces in MIDI format.
Link: https://arxiv.org/abs/1810.00790
====================================================
Detecting Changes in User Preferences using Hidden Markov Models for Sequential Recommendation Tasks (Farzad Eskandanian - 29 September, 2018)
These models are evaluated in terms of accuracy of change point detection and also the effectiveness of recommendations using a real music streaming dataset.
Link: https://arxiv.org/abs/1810.00272
====================================================
Modulated Variational auto-Encoders for many-to-many musical timbre transfer (Adrien Bitton - 29 September, 2018)
In this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer. We define timbre transfer as applying parts of the auditory properties of a musical instrument onto another
Link: https://arxiv.org/abs/1810.00222
====================================================
The Case for MUSIC: A Programmable IoT Framework for Mobile Urban Sensing Applications (Shiva R. Iyer - 28 September, 2018)
The MUSIC platform is designed for urban-centric sensing applications such as location sensing on mobile phones for road traffic monitoring, air quality sensing and urban quality monitoring using remote cameras. We briefly present three different urban sensing applications built on top of the MUSIC stack.
Link: https://arxiv.org/abs/1809.11120
====================================================
Bayesian inference for PCA and MUSIC algorithms with unknown number of sources (Viet Hung Tran - 26 September, 2018)
We then use Bayesian method to, for the first time, compute the MAP estimate for the number of sources in PCA and MUSIC algorithms. In simulations of overlapping multi-tone sources for linear sensor array, our exact MAP estimate is far superior to the asymptotic Akaike information criterion (AIC), which is a popular method for estimating the number of components in PCA and MUSIC algorithms.
Link: https://arxiv.org/abs/1809.10168
====================================================
Enabling Ultra-Low Delay Teleorchestras using Software Defined Networking (Emmanouil Lakiotakis - 29 August, 2018)
An example of this application class are the Networked Music Performance (NMP) systems that describe a live music performance by geographically separate musicians over the Internet
Link: https://arxiv.org/abs/1809.07864
====================================================
MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer (Gino Brunner - 20 September, 2018)
The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.
Link: https://arxiv.org/abs/1809.07600
====================================================
Symbolic Music Genre Transfer with CycleGAN (Gino Brunner - 20 September, 2018)
In order to improve the fidelity of the transformed music, we add additional discriminators that cause the generators to keep the structure of the original music mostly intact, while still achieving strong genre transfer. To the best of our knowledge, this paper represents the first application of GANs to symbolic music domain transfer.
Link: https://arxiv.org/abs/1809.07575
====================================================
Music Mood Detection Based On Audio And Lyrics With Deep Neural Net (RÃ©mi Delbouys - 19 September, 2018)
We consider the task of multimodal music mood prediction based on the audio signal and the lyrics of a track
Link: https://arxiv.org/abs/1809.07276
====================================================
Audio Based Disambiguation Of Music Genre Tags (Romain Hennequin - 19 September, 2018)
In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres
Link: https://arxiv.org/abs/1809.07256
====================================================
Investigating Crowd Creativity in Online Music Communities (Fabio Calefato - 14 September, 2018)
Crowd creativity is typically associated with peer-production communities focusing on artistic products like animations, video games, and music, but less frequently to Open Source Software (OSS), despite the fact that also developers must be creative to come up with new solutions to their technical challenges
Link: https://arxiv.org/abs/1809.06172
====================================================
DeepDrum: An Adaptive Conditional Neural Network (Dimos Makris - 17 September, 2018)
However, the generation of rhythms requires additional information regarding musical structure and accompanying instruments. In this paper we present DeepDrum, an adaptive Neural Network capable of generating drum rhythms under constraints imposed by Feed-Forward (Conditional) Layers which contain musical parameters along with given instrumentation information (e.g
Link: https://arxiv.org/abs/1809.06127
====================================================
Attention as a Perspective for Learning Tempo-invariant Audio Queries (Matthias Dorfer - 15 September, 2018)
Depending on the tempo of a query performance, this window captures more or less musical content, while notehead density in the score is largely tempo-independent. Empirical results on classical piano music indicate that attention is beneficial for retrieval performance, and exhibits intuitively appealing behavior.
Link: https://arxiv.org/abs/1809.05689
====================================================
Mugeetion: Musical Interface Using Facial Gesture and Emotion (Eunjeong Stella Koh - 7 October, 2018)
However, emotions are not tangible objects that can be exploited in the music composition process as they are difficult to capture and quantify in algorithms. We present a novel musical interface, Mugeetion, designed to capture occurring instances of emotional states from users' facial gestures and relay that data to associated musical features
Link: https://arxiv.org/abs/1809.05502
====================================================
Neural Melody Composition from Lyrics (Hangbo Bao - 12 September, 2018)
In this paper, we study a novel task that learns to compose music from natural language. It consists of two neural encoders to encode the current lyrics and the context melody respectively, and a hierarchical decoder to jointly produce musical notes and the corresponding alignment
Link: https://arxiv.org/abs/1809.04318
====================================================
Music Transformer (Cheng-Zhi Anna Huang - 10 October, 2018)
This is impractical for long sequences such as musical compositions since their memory complexity is quadratic in the sequence length. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-competition, and obtain state-of-the-art results on the latter.
Link: https://arxiv.org/abs/1809.04281
====================================================
Automatic, Personalized, and Flexible Playlist Generation using Reinforcement Learning (Shun-Yao Shih - 11 September, 2018)
Songs can be well arranged by professional music curators to form a riveting playlist that creates engaging listening experiences. By exploiting the techniques of deep learning and reinforcement learning, in this paper, we consider music playlist generation as a language modeling problem and solve it by the proposed attention language model with policy gradient
Link: https://arxiv.org/abs/1809.04214
====================================================
Local Music Event Recommendation with Long Tail Artists (Douglas Turnbull - 6 September, 2018)
That is, it can be hard to find social tag and artist similarity information for many of the artists who are playing shows in the local music community. We also introduce the concept of a Music Event Graph as a data structure that makes it easy and efficient to recommend events based on user-selected genre tags and popular artists
Link: https://arxiv.org/abs/1809.02277
====================================================
Music Sequence Prediction with Mixture Hidden Markov Models (Tao Li - 27 September, 2018)
We compare the mixture model with state-of-the-art methods and evaluate the predictions quantitatively and qualitatively on a large-scale real-world dataset in a Kaggle competition. We conclude by envisioning a next-generation music recommendation system that integrates our model with recent advances in deep learning, computer vision, and speech techniques, and has promising potential in both academia and industry.
Link: https://arxiv.org/abs/1809.00842
====================================================
PlayNPort: A Portable Wireless Music Player and Text Reader System (Lakhan Shiva Kamireddy - 2 September, 2018)
Our idea is to develop a portable wireless music player and text reader using a Cortex-M series microcontroller and bare-metal programming techniques. The resulting electronic device is similar to a consumer grade music player available in a car
Link: https://arxiv.org/abs/1809.00406
====================================================
Multitask Learning for Fundamental Frequency Estimation in Music (Rachel M. Bittner - 2 September, 2018)
Fundamental frequency (f0) estimation from polyphonic music includes the tasks of multiple-f0, melody, vocal, and bass line estimation
Link: https://arxiv.org/abs/1809.00381
====================================================
Content-based feature exploration for transparent music recommendation using self-attentive genre classification (Seungjin Lee - 3 September, 2018)
Interpretation of retrieved results is an important issue in music recommender systems, particularly from a user perspective. The experimental results show that the proposed methods provide the characteristics that are interpretable in terms of both lyrical and musical contents
Link: https://arxiv.org/abs/1808.10600
====================================================
Superhighway: Bypass Data Sparsity in Cross-Domain CF (Kwei-Herng Lai - 28 August, 2018)
The experiments conducted on a real-world cross-region music dataset and a cross-platform movie dataset show that the proposed superhighway construction significantly improves recommendation performance in both target and source domains.
Link: https://arxiv.org/abs/1808.09784
====================================================
Representation Learning for Image-based Music Recommendation (Chih-Chun Hsia - 29 August, 2018)
We propose a novel representation learning framework for image-based music recommendation that bridges the heterogeneity gap between music and image data; the proposed method is a key component for various contextual recommendation tasks
Link: https://arxiv.org/abs/1808.09198
====================================================
TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial Networks (Xinyue Liu - 22 August, 2018)
Recently, GANs have been extended from generating images to generating sequences (e.g., poems, music and codes)
Link: https://arxiv.org/abs/1808.07582
====================================================
Attainment Ratings for Graph-Query Recommendation (Hal Cooper - 17 August, 2018)
The video game industry is larger than both the film and music industries combined
Link: https://arxiv.org/abs/1808.05988
====================================================
Automatic Chord Recognition with Higher-Order Harmonic Language Modelling (Filip Korzeniowski - 16 August, 2018)
Due to this fact, they are unable to capture musical knowledge about chord progressions
Link: https://arxiv.org/abs/1808.05341
====================================================
Genre-Agnostic Key Classification With Convolutional Neural Networks (Filip Korzeniowski - 15 August, 2018)
These modifications enable the network to learn a genre-independent model that performs better than models trained for specific music styles, which has not been the case in existing work. We then evaluate the model on a number of unseen data sets, and show its superior performance compared to the state of the art
Link: https://arxiv.org/abs/1808.05340
====================================================
Improved Chord Recognition by Combining Duration and Harmonic Language Models (Filip Korzeniowski - 15 August, 2018)
However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs)
Link: https://arxiv.org/abs/1808.05335
====================================================
Statistical Piano Reduction Controlling Performance Difficulty (Eita Nakamura - 15 August, 2018)
Second, to describe musical fidelity, we construct a probabilistic model integrating a prior piano-score model and a model representing how ensemble scores are likely to be edited. We confirm the effect of the iterative procedure; we find that subjective difficulty and musical fidelity monotonically increase with controlled difficulty values; and we show that incorporating sequential dependence of pitches and fingering motion in the piano-score model improves the quality of reduction scores in high-difficulty cases.
Link: https://arxiv.org/abs/1808.05006
====================================================
This Time with Feeling: Learning Expressive Musical Performance (Sageev Oore - 10 August, 2018)
Music generation has generally been focused on either creating scores or interpreting them. We also include feedback from professional composers and musicians about some of these examples.
Link: https://arxiv.org/abs/1808.03715
====================================================
Simulating Raga Notes with a Markov Chain of Order 1-2 (Devashish Gosain - 5 August, 2018)
Semi Natural Algorithmic composition (SNCA) is the technique of using algorithms to create music note sequences in computer with the understanding that how to render them would be decided by the composer. The choice between first and second order Markov model, is best left to the composer who has to decide how to render these music notes sequences
Link: https://arxiv.org/abs/1808.01603
====================================================
AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies (Sourish Chaudhuri - 23 August, 2018)
The labels in the dataset annotate three different speech activity conditions: clean speech, speech co-occurring with music, and speech co-occurring with noise, which enable analysis of model performance in more challenging conditions based on the presence of overlapping noise. We report benchmark performance numbers on AVA-Speech using off-the-shelf, state-of-the-art audio and vision models that serve as a baseline to facilitate future research.
Link: https://arxiv.org/abs/1808.00606
====================================================
Data Augmentation for Robust Keyword Spotting under Playback Interference (Anirudh Raju - 1 August, 2018)
The training set audio is artificially corrupted by mixing in music and TV/movie audio, at different signal to interference ratios
Link: https://arxiv.org/abs/1808.00563
====================================================
Harmonic-Percussive Source Separation with Deep Neural Networks and Phase Recovery (Konstantinos Drossos - 30 July, 2018)
MaD TwinNet is a deep learning architecture that has reached state-of-the-art results in monaural singing voice separation. Experiments conducted on realistic music mixtures show that this novel separation system outperforms the previous state-of-the art kernel additive model approach.
Link: https://arxiv.org/abs/1807.11298
====================================================
Lead Sheet Generation and Arrangement by Conditional Generative Adversarial Network (Hao-Min Liu - 29 July, 2018)
However, the generation of multi-instrument music of arbitrary genres still remains a challenge. Existing research either works on lead sheets or multi-track piano-rolls found in MIDIs, but both musical notations have their limits
Link: https://arxiv.org/abs/1807.11161
====================================================
Visual Display and Retrieval of Music Information (Rafael Valle - 26 July, 2018)
This paper describes computational methods for the visual display and analysis of music information. We provide a concise description of software, music descriptors and data visualization techniques commonly used in music information retrieval
Link: https://arxiv.org/abs/1807.10204
====================================================
A Hybrid of Deep Audio Feature and i-vector for Artist Recognition (Jiyoung Park - 24 July, 2018)
Artist recognition is a task of modeling the artist's musical style. We show that this approach achieves state-of-the-art performance by complementing each other
Link: https://arxiv.org/abs/1807.09208
====================================================
CNN-based Facial Affect Analysis on Mobile Devices (Charlie Hewitt - 23 July, 2018)
To demonstrate the feasibility of deploying these models for real-world applications, we implement a music recommendation interface based on predicted user affect. Although the CNN models were not trained in the context of music recommendation, our case study shows that: (i) the trained models achieve similar prediction performance to the benchmark dataset, and (ii) users tend to positively rate the song recommendations provided by the interface
Link: https://arxiv.org/abs/1807.08775
====================================================
Auto-adaptive Resonance Equalization using Dilated Residual Networks (Maarten Grachten - 23 July, 2018)
In music and audio production, attenuation of spectral resonances is an important step towards a technically correct result
Link: https://arxiv.org/abs/1807.08636
====================================================
Audio-to-Score Alignment using Transposition-invariant Features (Andreas Arzt - 19 July, 2018)
Audio-to-score alignment is an important pre-processing step for in-depth analysis of classical music. Furthermore, they can even outperform widely used features for audio-to-score alignment on `untransposed data', and thus are a viable and more flexible alternative to well-established features for music alignment and matching.
Link: https://arxiv.org/abs/1807.07278
====================================================
RARD II: The 2nd Related-Article Recommendation Dataset (Joeran Beel - 20 July, 2018)
As such, it complements datasets from other domains such as books, movies, and music
Link: https://arxiv.org/abs/1807.06918
====================================================
Deep Content-User Embedding Model for Music Recommendation (Jongpil Lee - 18 July, 2018)
In this work, we propose deep content-user embedding model, a simple and intuitive architecture that combines the user-item interaction and music audio content. We evaluate the model on music recommendation and music auto-tagging tasks
Link: https://arxiv.org/abs/1807.06786
====================================================
Psychological constraints on string-based methods for pattern discovery in polyphonic corpora (David R. W. Sears - 17 July, 2018)
Researchers often divide symbolic music corpora into contiguous sequences of n events (called n-grams) for the purposes of pattern discovery, key finding, classification, and prediction
Link: https://arxiv.org/abs/1807.06700
====================================================
Learning to Listen, Read, and Follow: Score Following as a Reinforcement Learning Game (Matthias Dorfer - 17 July, 2018)
In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images.
Link: https://arxiv.org/abs/1807.06391
====================================================
Machine Learning Approaches to Hybrid Music Recommender Systems (Andreu Vall - 16 July, 2018)
We proposed hybrid music recommender systems based solely on data and robust to the so-called "cold-start problem" for new music items, favoring the discovery of relevant but non-popular music. We thoroughly studied the specific task of music playlist continuation, by analyzing fundamental playlist characteristics, song feature representations, and the relationship between playlists and the songs therein.
Link: https://arxiv.org/abs/1807.05858
====================================================
Deep Learning in the Wild (Thilo Stadelmann - 13 July, 2018)
Specifically, we give insight into deep learning projects on face matching, print media monitoring, industrial quality control, music scanning, strategy game playing, and automated machine learning, thereby providing best practices for deep learning in practice.
Link: https://arxiv.org/abs/1807.04950
====================================================
TequilaGAN: How to easily identify GAN samples (Rafael Valle - 13 July, 2018)
We provide results on MNIST, CIFAR10, music and speech data.
Link: https://arxiv.org/abs/1807.04919
====================================================
The Importance of Song Context and Song Order in Automated Music Playlist Generation (Andreu Vall - 12 July, 2018)
The automated generation of music playlists can be naturally regarded as a sequential task, where a recommender system suggests a stream of songs that constitute a listening session. We investigate the impact of the song context and the song order on next-song recommendations by conducting dedicated off-line experiments on two datasets of hand-curated music playlists
Link: https://arxiv.org/abs/1807.04690
====================================================
Improving DNN-based Music Source Separation using Phase Features (Joachim Muth - 16 July, 2018)
Music source separation with deep neural networks typically relies only on amplitude features
Link: https://arxiv.org/abs/1807.02710
====================================================
Natural Language Processing for Music Knowledge Discovery (Sergio Oramas - 5 July, 2018)
In this work, we present different Natural Language Processing (NLP) approaches to harness the potential of these text collections for automatic music knowledge discovery, covering different phases in a prototypical NLP pipeline, namely corpus compilation, text-mining, information extraction, knowledge graph generation and sentiment analysis. Each of these approaches is presented alongside different use cases (i.e., flamenco, Renaissance and popular music) where large collections of documents are processed, and conclusions stemming from data-driven analyses are presented and discussed.
Link: https://arxiv.org/abs/1807.02200
====================================================
Context Data Categories and Privacy Model for Mobile Data Collection Apps (Felix Beierle - 4 July, 2018)
With TYDR, we track a larger variety of smartphone data than similar existing apps, including metadata on notifications, photos taken, and music played back by the user
Link: https://arxiv.org/abs/1807.01515
====================================================
Weakly Supervised Deep Recurrent Neural Networks for Basic Dance Step Generation (Nelson Yalta - 3 July, 2018)
In addition, we investigate the use of a contrastive cost function for music-motion regulation. This cost function targets motion direction and maps similarities between music frames
Link: https://arxiv.org/abs/1807.01126
====================================================
A Computational Study of the Role of Tonal Tension in Expressive Piano Performance (Carlos Cancino-ChacÃ³n - 3 July, 2018)
In this work we use a computational approach to study the role of three measures of tonal tension proposed by Herremans and Chew (2016) in the prediction of expressive performances of classical piano music. These features capture tonal relationships of the music represented in Chew's spiral array model, a three dimensional representation of pitch classes, chords and keys constructed in such a way that spatial proximity represents close tonal relationships
Link: https://arxiv.org/abs/1807.01080
====================================================
An energy-based generative sequence model for testing sensory theories of Western harmony (Peter M. C. Harrison - 2 July, 2018)
Applied to our three musical genres, the results generally support the relationship between sensory consonance and harmony, but lead us to question the high importance attributed to spectral distance in the psychological literature. We anticipate that our methods will provide a useful platform for future work linking music psychology to music theory.
Link: https://arxiv.org/abs/1807.00790
====================================================
Play Duration based User-Entity Affinity Modeling in Spoken Dialog System (Bo Xiao - 29 June, 2018)
Based on a large-scale database of Alexa music service records, we evaluate the affinity models by computing Spearman correlation between play durations and predicted affinities
Link: https://arxiv.org/abs/1806.11479
====================================================
GenerationMania: Learning to Semantically Choreograph (Zhiyu Lin - 6 September, 2018)
Beatmania is a rhythm action game where players take on the role of a DJ who performs music by pressing specific controller buttons to mix "Keysounds" (audio samples) at the correct time. We present a deep neural network based process for automatically generating Beatmania charts for arbitrary pieces of music
Link: https://arxiv.org/abs/1806.11170
====================================================
Modeling Majorness as a Perceptual Property in Music from Listener Ratings (Anna Aljanaki - 27 June, 2018)
For the tasks of automatic music emotion recognition, genre recognition, music recommendation it is helpful to be able to extract mode from any section of a musical piece as a perceived amount of major or minor mode (majorness) inside that section, perceived as a whole (one or several melodies and any harmony present). We collect annotations from musicians and show that majorness can be understood by musicians in an intuitive way
Link: https://arxiv.org/abs/1806.10570
====================================================
The challenge of realistic music generation: modelling raw audio at scale (Sander Dieleman - 26 June, 2018)
This is problematic because music exhibits structure at many different timescales. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.
Link: https://arxiv.org/abs/1806.10474
====================================================
Conditioning Deep Generative Raw Audio Models for Structured Automatic Music (Rachel Manzelli - 26 June, 2018)
In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music
Link: https://arxiv.org/abs/1806.09905
====================================================
Frame-level Instrument Recognition by Timbre and Pitch (Yun-Ning Hung - 25 June, 2018)
Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction
Link: https://arxiv.org/abs/1806.09587
====================================================
Learning Transposition-Invariant Interval Features from Symbolic Music and Audio (Stefan Lattner - 21 June, 2018)
An unsupervised training method is proposed yielding an organization of intervals in the representation space which is musically plausible. Based on the representations, a transposition-invariant self-similarity matrix is constructed and used to determine repeated sections in symbolic music and in audio, yielding competitive results in the MIREX task "Discovery of Repeated Themes and Sections".
Link: https://arxiv.org/abs/1806.08236
====================================================
Towards multi-instrument drum transcription (Richard Vogl - 3 October, 2018)
Automatic drum transcription, a subtask of the more general automatic music transcription, deals with extracting drum instrument note onsets from an audio source. Yet, for many applications, the ability to transcribe more drum instruments which make up standard drum kits used in western popular music would be desirable
Link: https://arxiv.org/abs/1806.06676
====================================================
Modeling Musical Taste Evolution with Recurrent Neural Networks (Massimo Quadrana - 18 June, 2018)
In this paper, we leverage a massive dataset on internet radio station creation from a large music streaming company in order to develop computational models of listener taste evolution. We apply our model to the problem of next station prediction and show that it not only outperforms several baselines, but excels at long tail music personalization, particularly by learning the long-term dependency structure of listener music preference evolution.
Link: https://arxiv.org/abs/1806.06535
====================================================
The NES Music Database: A multi-instrumental dataset with expressive performance attributes (Chris Donahue - 11 June, 2018)
In this paper, we introduce the Nintendo Entertainment System Music Database (NES-MDB), a large corpus allowing for separate examination of the tasks of composition and performance. For each song, the dataset contains a musical score for four instrument voices as well as expressive attributes for the dynamics and timbre of each voice
Link: https://arxiv.org/abs/1806.04278
====================================================
An Optimization View of MUSIC and Its Extension to Missing Data (Shuang Li - 9 June, 2018)
In particular, we propose an algorithm for spectral estimation that involves searching for the peaks of the dual polynomial corresponding to a certain nuclear norm minimization (NNM) problem, and we show that this algorithm is in fact equivalent to MUSIC itself. Building on this connection, we also extend the classical MUSIC algorithm to the missing data case
Link: https://arxiv.org/abs/1806.03511
====================================================
Revisiting Singing Voice Detection: a Quantitative Review and the Future Outlook (Kyungyun Lee - 4 June, 2018)
Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval
Link: https://arxiv.org/abs/1806.01180
====================================================
Semi-Recurrent CNN-based VAE-GAN for Sequential Data Generation (Mohammad Akbari - 1 June, 2018)
The promising experimental results on piano music generation indicates the potential of the proposed framework in modeling other sequential data such as video.
Link: https://arxiv.org/abs/1806.00509
====================================================
Deep Segment Hash Learning for Music Generation (Kevin Joslyn - 30 May, 2018)
To the best of our knowledge, DSHL is the first end-to-end segment hash learning method for music generation, and the first to use pair-wise training with segments of music. We demonstrate that this method is capable of generating music which is both original and enjoyable, and that DSHL offers a promising new direction for music generation research.
Link: https://arxiv.org/abs/1805.12176
====================================================
Learning to Transcribe by Ear (Rainer Kelz - 29 May, 2018)
Such a task formulation encompasses the notion of a musical agent and an environment containing an instrument as well as the sound source to be transcribed. This process resembles how a human musician might approach the task of transcription, and the satisfaction she achieves by closely mimicking the sound source to transcribe on her instrument
Link: https://arxiv.org/abs/1805.11526
====================================================
Real-valued parametric conditioning of an RNN for interactive sound synthesis (Lonce Wyse - 29 May, 2018)
The result after training is an audio synthesizer that is played like a musical instrument with the desired musical characteristics provided as continuous parametric control
Link: https://arxiv.org/abs/1805.10808
====================================================
A Distributed Version of the Hungarian Method for Multi-Robot Assignment (Smriti Chopra - 22 May, 2018)
As a concrete experimental test-bed, we provide an interactive "multi-robot orchestral" framework in which a team of robots cooperatively plays a piece of music on a so-called orchestral floor.
Link: https://arxiv.org/abs/1805.08712
====================================================
Generative timbre spaces: regularizing variational auto-encoders with perceptual metrics (Philippe Esling - 1 October, 2018)
Timbre spaces have been used in music perception to study the perceptual relationships between instruments based on dissimilarity ratings
Link: https://arxiv.org/abs/1805.08501
====================================================
Joint Detection and Localization of an Unknown Number of Sources Using Algebraic Structure of the Noise Subspace (Matthew W. Morency - 22 May, 2018)
Methods which rely on the orthogonality of the signal and noise subspaces, such as Pisarenko's method, MUSIC, and root-MUSIC are some of the most widely used algorithms to solve these problems. Simulation results show a significant improvement over root-MUSIC in challenging scenarios such as closely located sources, both in terms of detection of the number of sources and their localization over a broad and practical range of SNRs
Link: https://arxiv.org/abs/1805.08421
====================================================
A Universal Music Translation Network (Noam Mor - 23 May, 2018)
The method is unsupervised and does not rely on supervision in the form of matched samples between domains or musical transcriptions. We evaluate our method on NSynth, as well as on a dataset collected from professional musicians, and achieve convincing translations, even when translating from whistling, potentially enabling the creation of instrumental music by untrained humans.
Link: https://arxiv.org/abs/1805.07848
====================================================
Extended pipeline for content-based feature engineering in music genre recognition (Tina Raissi - 12 May, 2018)
We present a feature engineering pipeline for the construction of musical signal characteristics, to be used for the design of a supervised model for musical genre identification
Link: https://arxiv.org/abs/1805.05324
====================================================
Mining and Forecasting Career Trajectories of Music Artists (Shushan Arakelyan - 8 May, 2018)
Many musicians, from up-and-comers to established artists, rely heavily on performing live to promote and disseminate their music. We then demonstrate how this dataset can be used to mine and predict important career milestones for the musicians, such as signing by a major music label, or performing at a certain venue
Link: https://arxiv.org/abs/1805.03324
====================================================
Transfer Learning of Artist Group Factors to Musical Genre Classification (Jaehun Kim - 5 May, 2018)
The automated recognition of music genres from audio information is a challenging problem, as genre labels are subjective and noisy
Link: https://arxiv.org/abs/1805.02043
====================================================
Weakly-supervised Visual Instrument-playing Action Detection in Videos (Jen-Yu Liu - 5 May, 2018)
While audio-based recognition of instruments has been widely studied, the visual aspect of the music instrument playing remains largely unaddressed in the literature. The resulted model only needs to analyze the visual part of a music video to deduce which, when and where instruments are played
Link: https://arxiv.org/abs/1805.02031
====================================================
Randomly weighted CNNs for (music) audio classification (Jordi Pons - 9 May, 2018)
Following this idea, we study how non-trained (randomly weighted) convolutional neural networks perform as feature extractors for (music) audio classification tasks. By following this methodology, we run a comprehensive evaluation of the current deep architectures for audio classification, and provide evidence that the architectures alone are an important piece for resolving (music) audio problems using deep neural networks.
Link: https://arxiv.org/abs/1805.00237
====================================================
Off the Beaten Track: Using Deep Learning to Interpolate Between Music Genres (Tijn Borghuis - 2 May, 2018)
We describe a system based on deep learning that generates drum patterns in the electronic dance music domain. Experimental results reveal that generated patterns can be employed to produce musically sound and creative transitions between different genres, and that the process of generation is of interest to practitioners in the field.
Link: https://arxiv.org/abs/1804.09808
====================================================
Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation (Hao-Wen Dong - 6 October, 2018)
It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices
Link: https://arxiv.org/abs/1804.09399
====================================================
Vocal melody extraction using patch-based CNN (Li Su - 24 April, 2018)
A patch-based convolutional neural network (CNN) model presented in this paper for vocal melody extraction in polyphonic music is inspired from object detection in image processing
Link: https://arxiv.org/abs/1804.09202
====================================================
An Overview of Lead and Accompaniment Separation in Music (Zafar Rafii - 23 April, 2018)
This particular case of source separation yields very specific challenges and opportunities, including the particular complexity of musical structures, but also relevant prior knowledge coming from acoustics, musicology or sound engineering. Finally, we discuss the delicate problem of evaluating the quality of music separation through adequate metrics and present the results of the largest evaluation, to-date, of lead and accompaniment separation systems
Link: https://arxiv.org/abs/1804.08300
====================================================
Tempo-Invariant Processing of Rhythm with Convolutional Neural Networks (Anders Elowsson - 28 April, 2018)
This presents a challenge for many music information retrieval (MIR) systems; ideally, perceptually similar rhythms should be represented and processed similarly, regardless of the specific tempo at which they were performed
Link: https://arxiv.org/abs/1804.08167
====================================================
Taylor's law for Human Linguistic Sequences (Tatsuru Kobayashi - 7 June, 2018)
The exponent was also compared for other language-related data, such as the child-directed speech, music, and programming language code
Link: https://arxiv.org/abs/1804.07893
====================================================
Generating Music using an LSTM Network (Nikhil Kotecha - 18 April, 2018)
Detailed in the paper is a neural network architecture that predicts and generates polyphonic music aligned with musical rules. When analyzed quantitatively and qualitatively, this approach performs well in composing polyphonic music
Link: https://arxiv.org/abs/1804.07300
====================================================
Deep Layered Learning in MIR (Anders Elowsson - 29 April, 2018)
This paper discusses how these can be used as intermediate representations in MIR to facilitate deep processing that generalizes well: each music concept is predicted individually in learning modules that are connected through latent representations in a directed acyclic graph. A background to DLL and modular music processing is provided, and relevant concepts such as pruning, skip-connections, and layered performance supervision are reviewed.
Link: https://arxiv.org/abs/1804.07297
====================================================
The Sound of Pixels (Hang Zhao - 30 July, 2018)
Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation
Link: https://arxiv.org/abs/1804.03160
====================================================
A Large-Scale Study of Language Models for Chord Prediction (Filip Korzeniowski - 5 April, 2018)
This finding constitutes a further step towards the development of chord recognition systems that are more aware of local musical context than what was previously possible.
Link: https://arxiv.org/abs/1804.01849
====================================================
Jointly Detecting and Separating Singing Voice: A Multi-Task Approach (Daniel Stoller - 4 April, 2018)
A main challenge in applying deep learning to music processing is the availability of training data
Link: https://arxiv.org/abs/1804.01650
====================================================
DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects (Lukas Tuggener - 26 May, 2018)
DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. DeepScores thus poses a relevant challenge for computer vision in general, beyond the scope of optical music recognition (OMR) research
Link: https://arxiv.org/abs/1804.00525
====================================================
Lisp, Jazz, Aikido -- Three Expressions of a Single Essence (Didier Verna - 27 March, 2018)
My personal life has been revolving around three major creative activities, of equal importance: programming in Lisp, playing Jazz music, and practicing Aikido
Link: https://arxiv.org/abs/1804.00485
====================================================
Conditional End-to-End Audio Transforms (Albert Haque - 7 June, 2018)
For the case of music, we can specify musical instruments and achieve the same result
Link: https://arxiv.org/abs/1804.00047
====================================================
Automatic Music Accompanist (Anyi Rao - 23 March, 2018)
The computer musician is able to produce musical accompaniment that relates musically to the human performance. It details how to train a score HMM, how to deal with polyphonic input, how this HMM work when following score, how to build up a musical accompanist
Link: https://arxiv.org/abs/1803.09033
====================================================
Investigating Collaboration Within Online Communities: Software Development Vs. Artistic Creation (Giuseppe Iaffaldano - 21 March, 2018)
These communities have also expanded to creative arts domains such as animation, video games, and music
Link: https://arxiv.org/abs/1803.07856
====================================================
Music Style Transfer: A Position Paper (Shuqi Dai - 19 July, 2018)
Thus, we vitally propose a more scientifically-viable definition of music style transfer by breaking it down into precise concepts of timbre style transfer, performance style transfer and composition style transfer, as well as to connect different aspects of music style transfer with existing well-established sub-fields of computer music studies. In addition, we discuss the current limitations of music style modeling and its future directions by drawing spirit from some deep generative models, especially the ones using unsupervised learning and disentanglement techniques.
Link: https://arxiv.org/abs/1803.06841
====================================================
A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music (Adam Roberts - 30 July, 2018)
We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.
Link: https://arxiv.org/abs/1803.05428
====================================================
Learning to Recognize Musical Genre from Audio (MichaÃ«l Defferrard - 13 March, 2018)
We here summarize our experience running a challenge with open data for musical genre recognition
Link: https://arxiv.org/abs/1803.05337
====================================================
Music Genre Classification Using Spectral Analysis and Sparse Representation of the Signals (Mehdi Banitalebi-Dehkordi - 13 March, 2018)
In this paper, we proposed a robust music genre classification method based on a sparse FFT based feature extraction method which extracted with discriminating power of spectral analysis of non-stationary audio signals, and the capability of sparse representation based classifiers. Our experimental results on the GTZAN database demonstrate that the proposed method outperforms the other state of the art SRC approaches
Link: https://arxiv.org/abs/1803.04652
====================================================
Learning the Base Distribution in Implicit Generative Models (Cem Subakan - 13 March, 2018)
We also show that our approach is amenable to learning generative model for sequential data, by learning to generate speech and music.
Link: https://arxiv.org/abs/1803.04357
====================================================
Cross-Domain Recommendation for Cold-Start Users via Neighborhood Based Feature Mapping (Xinghua Wang - 5 March, 2018)
Traditional CF models mainly focus on predicting a user's preference to the items in a single domain such as the movie domain or the music domain. Experimental results on two real data sets extracted from Amazon transaction data demonstrate the superiority of our proposed model against other state-of-the-art methods.
Link: https://arxiv.org/abs/1803.01617
====================================================
Exploring Novel Game Spaces with Fluidic Games (Swen E. Gaudl - 4 March, 2018)
People listen to music, play games or read the news while in transit or bridging gap times
Link: https://arxiv.org/abs/1803.01403
====================================================
Raw Multi-Channel Audio Source Separation using Multi-Resolution Convolutional Auto-Encoders (Emad M. Grais - 1 March, 2018)
In this work, we introduce a novel multi-channel, multi-resolution convolutional auto-encoder neural network that works on raw time-domain signals to determine appropriate multi-resolution features for separating the singing-voice from stereo music
Link: https://arxiv.org/abs/1803.00702
====================================================
Interplay between musical practices and tuning in the marimba de chonta music (Jorge Useche - 27 February, 2018)
We conclude that in this music the emergence of tunings and musical practices cannot be considered as separate issues. Consonance, timbre, and musical practices are entangled.
Link: https://arxiv.org/abs/1802.10162
====================================================
Music Genre Classification using Masked Conditional Neural Networks (Fady Medhat - 18 February, 2018)
We have evaluated the MCLNN performance using the Ballroom and Homburg datasets of music genres. MCLNN has achieved accuracies that are competitive to state-of-the-art handcrafted attempts in addition to models based on Convolutional Neural Networks.
Link: https://arxiv.org/abs/1802.06432
====================================================
BachProp: Learning to Compose Music in Multiple Styles (Florian Colombo - 20 February, 2018)
In this paper, new music scores sampled by BachProp are compared with the original corpora via crowdsourcing. This evaluation indicates that the music scores generated by BachProp are not less preferred than the original music corpus the algorithm was provided with.
Link: https://arxiv.org/abs/1802.05162
====================================================
Policy Gradients for General Contextual Bandits (Feiyang Pan - 22 May, 2018)
We evaluate PGCB on toy datasets as well as a music recommender dataset
Link: https://arxiv.org/abs/1802.04162
====================================================
One Deep Music Representation to Rule Them All? : A comparative analysis of different representation learning strategies (Jaehun Kim - 13 February, 2018)
We conducted this investigation via an extensive empirical study that involves multiple learning tasks, as well as multiple deep learning architectures with varying levels of information sharing between tasks, in order to learn music representations. The results of our experiments yield several insights on how to approach the design of methods for learning widely deployable deep data representations in the music domain.
Link: https://arxiv.org/abs/1802.04051
====================================================
Neural Dynamic Programming for Musical Self Similarity (Christian J. Walder - 28 August, 2018)
We present a neural sequence model designed specifically for symbolic music
Link: https://arxiv.org/abs/1802.03144
====================================================
Deep Predictive Models in Interactive Music (Charles P. Martin - 1 June, 2018)
Musical performance requires prediction to operate instruments, to perform in groups and to improvise. We argue, with reference to a number of digital music instruments (DMIs), including two of our own, that predictive machine learning models can help interactive systems to understand their temporal context and ensemble behaviour
Link: https://arxiv.org/abs/1801.10492
====================================================
Deep Interactive Evolution (Philip Bontrager - 24 January, 2018)
On the other hand, interactive evolution has shown promise in creating various artifacts such as images, music and 3D objects, but traditionally relies on a hand-designed evolvable representation of the target domain
Link: https://arxiv.org/abs/1801.08230
====================================================
Binning based algorithm for Pitch Detection in Hindustani Classical Music (Malvika Singh - 7 January, 2018)
It can be used to identify pitches in Hindustani Classical Music which is based on suitable intonations and swaras. The binning algorithm thus helps to segregate the important pitches in a given musical piece.
Link: https://arxiv.org/abs/1801.02155
====================================================
Explorations in an English Poetry Corpus: A Neurocognitive Poetics Perspective (Arthur M. Jacobs - 6 January, 2018)
Some exemplary QNA studies show author similarities based on latent semantic analysis, significant topics for each author or various text-analytic metrics for George Eliot's poem 'How Lisa Loved the King' and James Joyce's 'Chamber Music', concerning e.g
Link: https://arxiv.org/abs/1801.02054
====================================================
DeepJ: Style-Specific Music Generation (Huanru Henry Mao - 2 January, 2018)
Our innovations include methods to learn musical style and music dynamics. We use our model to demonstrate a simple technique for controlling the style of generated music as a proof of concept
Link: https://arxiv.org/abs/1801.00887
====================================================
Music Genre Classification with Paralleling Recurrent Convolutional Neural Network (Lin Feng - 22 December, 2017)
The paralleling network guarantees the extracting features robust enough to represent music. Moreover, the experiments prove our proposed architecture improve the music genre classification performance and the additional Bi-RNN block is a supplement for CNNs.
Link: https://arxiv.org/abs/1712.08370
====================================================
Towards a Deep Improviser: a prototype deep learning post-tonal free music generator (Roger T. Dean - 21 December, 2017)
Two modest-sized symbolic corpora of post-tonal and post-metric keyboard music have been constructed, one algorithmic, the other improvised. Music has been generated using the approach, and informal judgements place it roughly on a par with algorithmic and composed music in related forms
Link: https://arxiv.org/abs/1712.07799
====================================================
Use of Deep Learning in Modern Recommendation System: A Summary of Recent Works (Ayush Singhal - 20 December, 2017)
With the exponential increase in the amount of digital information over the internet, online shops, online music, video and image libraries, search engines and recommendation system have become the most convenient ways to find relevant information within a short time
Link: https://arxiv.org/abs/1712.07525
====================================================
Direct Positioning with Channel Database Assistance (Laurence Mailaender - 19 December, 2017)
Using wall knowledge, we develop a generalized MUSIC algorithm that treats the wall reflection parameter as a nuisance variable
Link: https://arxiv.org/abs/1712.07148
====================================================
Using Deep learning methods for generation of a personalized list of shuffled songs (Rushin Gindra - 17 December, 2017)
The shuffle mode, where songs are played in a randomized order that is decided upon for all tracks at once, is widely found and known to exist in music player systems. There are only few music enthusiasts who use this mode since it either is too random to suit their mood or it keeps on repeating the same list every time
Link: https://arxiv.org/abs/1712.06076
====================================================
Automatic Music Highlight Extraction using Convolutional Recurrent Attention Networks (Jung-Woo Ha - 15 December, 2017)
Music highlights are valuable contents for music services
Link: https://arxiv.org/abs/1712.05901
====================================================
A Novel Approach for Effective Learning in Low Resourced Scenarios (Sri Harsha Dumpala - 15 December, 2017)
We demonstrate our approach for speech/music discrimination and emotion classification through experiments
Link: https://arxiv.org/abs/1712.05608
====================================================
DLR : Toward a deep learned rhythmic representation for music content analysis (Yeonwoo Jeong - 14 December, 2017)
The proposed approach aims to learn DLR from the raw audio signal and use it for other music informatics tasks
Link: https://arxiv.org/abs/1712.05119
====================================================
Music Generation by Deep Learning - Challenges and Directions (Jean-Pierre Briot - 30 September, 2018)
Furthermore, deep learning architectures alone are autistic automata which generate music autonomously without human user interaction, far from the objective of interactively assisting musicians to compose and refine music. In this paper, we select some limitations of a direct application of deep learning to music generation, analyze why the issues are not fulfilled and how to address them by possible approaches
Link: https://arxiv.org/abs/1712.04371
====================================================
Efficient Approximation Algorithms for String Kernel Based Sequence Classification (Muhammad Farhan - 12 December, 2017)
We give analytical bounds on quality and runtime of our algorithm and report its empirical performance on real world biological and music sequences datasets.
Link: https://arxiv.org/abs/1712.04264
====================================================
Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search (Fabien AndrÃ© - 7 December, 2017)
For instance, multimedia objects (images, music or videos) can be represented by high-dimensional feature vectors
Link: https://arxiv.org/abs/1712.02912
====================================================
Secure Directional Modulation to Enhance Physical Layer Security in IoT Networks (Feng Shu - 24 January, 2018)
Firstly, the directions of arrival (DOAs) of the signals from the desired user and eavesdropper are estimated by the Root Multiple Signal Classificaiton (Root-MUSIC) algorithm and the related signal-to-noise ratios (SNRs) are estimated based on the ratio of the corresponding eigenvalue to the minimum eigenvalue of the covariance matrix of the received signals
Link: https://arxiv.org/abs/1712.02104
====================================================
Low-Complexity and High-Resolution DOA Estimation for Hybrid Analog and Digital Massive MIMO Receive Array (Feng Shu - 6 December, 2017)
Subsequently, a fast root multiple signal classification HDAPA (Root-MUSIC-HDAPA) method is proposed specially for this hybrid structure to implement an approximately analytical solution. Simulation results show that our proposed methods, Root-MUSIC-HDAPA and HDAPA, can achieve the hybrid CRLB with their complexities being significantly lower than those of pure linear searching-based methods, such as APA.
Link: https://arxiv.org/abs/1712.02085
====================================================
Learning to Fuse Music Genres with Generative Adversarial Dual Learning (Zhiqian Chen - 4 December, 2017)
FusionGAN is a novel genre fusion framework for music generation that integrates the strengths of generative adversarial networks and dual learning. Experimental results on public music datasets demonstrated that our approach could effectively merge two genres.
Link: https://arxiv.org/abs/1712.01456
====================================================
Raw Waveform-based Audio Classification Using Sample-level CNN Architectures (Jongpil Lee - 3 December, 2017)
Music, speech, and acoustic scene sound are often handled separately in the audio domain because of their different signal characteristics. We show that the sample-level models reach state-of-the-art performance levels for the three different categories of sound
Link: https://arxiv.org/abs/1712.00866
====================================================
Enabling Embodied Analogies in Intelligent Music Systems (Fabio Paolizzo - 30 November, 2017)
The present methodology is aimed at cross-modal machine learning and uses multidisciplinary tools and methods drawn from a broad range of areas and disciplines, including music, systematic musicology, dance, motion capture, human-computer interaction, computational linguistics and audio signal processing. Main tasks include: (1) adapting wisdom-of-the-crowd approaches to embodiment in music and dance performance to create a dataset of music and music lyrics that covers a variety of emotions, (2) applying audio/language-informed machine learning techniques to that dataset to identify automatically the emotional content of the music and the lyrics, and (3) integrating motion capture data from a Vicon system and dancers performing on that music.
Link: https://arxiv.org/abs/1712.00334
====================================================
Audio Cover Song Identification using Convolutional Neural Network (Sungkyun Chang - 30 November, 2017)
The trained CNN outputs the probability of being in the cover song relation given a cross-similarity matrix generated from any two pieces of music and identifies the cover song by ranking on the probability. Experimental results show that the proposed algorithm achieves performance better than or comparable to the state-of-the-art.
Link: https://arxiv.org/abs/1712.00166
====================================================
Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories (Alexandre Yahi - 30 November, 2017)
From generating realistic images and videos to assisting musical creation, GANs are transforming many fields of arts and sciences
Link: https://arxiv.org/abs/1712.00164
====================================================
Autonomy in the interactive music system VIVO (Fabio Paolizzo - 30 November, 2017)
Interactive Music Systems (IMS) have introduced a new world of music-making modalities. But can we really say that they create music, as in true autonomous creation? Here we discuss Video Interactive VST Orchestra (VIVO), an IMS that considers extra-musical information by adopting a simple salience based model of user-system interaction when simulating intentionality in automatic music generation
Link: https://arxiv.org/abs/1711.11319
====================================================
RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction (Charles P. Martin - 29 November, 2017)
In this paper, we describe the design and implementation of RoboJam's network and how it has been integrated into a touchscreen music app. A preliminary evaluation analyses the system in terms of training, musical generation and user interaction.
Link: https://arxiv.org/abs/1711.10746
====================================================
Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval (Yi Yu - 28 November, 2017)
ii) We further suggest an end-to-end architecture that simultaneously trains convolutional layers and fully-connected layers to better learn temporal structures of music audio. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval.
Link: https://arxiv.org/abs/1711.08976
====================================================
JamBot: Music Theory Aware Chord Based Generation of Polyphonic Music with LSTMs (Gino Brunner - 21 November, 2017)
We show that our approach is sensible from a music theory perspective by evaluating the learned chord embeddings. Surprisingly, our simple model managed to extract the circle of fifths, an important tool in music theory, from the dataset.
Link: https://arxiv.org/abs/1711.07682
====================================================
Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models (Jesse Engel - 21 December, 2017)
Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function
Link: https://arxiv.org/abs/1711.05772
====================================================
Optimal Tuning of Two-Dimensional Keyboards (Aricca Bannerman - 14 November, 2017)
We give a new analysis of a tuning problem in music theory, pertaining specifically to the approximation of harmonics on a two-dimensional keyboard
Link: https://arxiv.org/abs/1711.05260
====================================================
Considering Durations and Replays to Improve Music Recommender Systems (Pierre Hanna - 14 November, 2017)
Implicit like and dislike can be deduced from this information of durations and replays and can be taken into account for music recommendation and for the evaluation of music recommendation engines. Several simple algorithms show that this post-filtering operation leads to an improvement of the quality of the music recommendations.
Link: https://arxiv.org/abs/1711.05237
====================================================
Text Mining Descriptions Of Dreams: aesthetic and clinical efforts (Renato Fabbri - 26 October, 2017)
They were subsequently mined using various techniques for the achievement of poems and summaries, which were then used in clinical sessions by means of music and declamation
Link: https://arxiv.org/abs/1711.04609
====================================================
End-to-end learning for music audio tagging at scale (Jordi Pons - 15 June, 2018)
This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models - using waveforms as input with very small convolutional filters; and models that rely on domain knowledge - log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios.
Link: https://arxiv.org/abs/1711.02520
====================================================
Non-uniform time-scaling of Carnatic music transients (Venkata Subramanian Viraraghavan - 7 November, 2017)
The results indicate that the changing tempo of Carnatic music does not change the duration of transients significantly. We report listening tests on our algorithm to slow down Carnatic music that is consistent with this observation.
Link: https://arxiv.org/abs/1711.02318
====================================================
Strategies for Conceptual Change in Convolutional Neural Networks (Maarten Grachten - 5 November, 2017)
We show, among other things, that across handwritten digits, natural images, and classical music, adaptive strategies are systematically more effective than a baseline method that starts learning from scratch.
Link: https://arxiv.org/abs/1711.01634
====================================================
Melody Generation for Pop Music via Word Representation of Musical Properties (Andrew Shin - 31 October, 2017)
It is also difficult to remain in the permissible spectrum of musical variety, outside of which would be perceived as a plain random play without auditory pleasantness. Observing the conventional structure of pop music poses further challenges
Link: https://arxiv.org/abs/1710.11549
====================================================
Polyphonic Music Generation with Sequence Generative Adversarial Networks (Sang-gil Lee - 2 July, 2018)
The proposed method condenses duration, octaves, and keys of both melodies and chords into a single word vector representation, and recurrent neural networks learn to predict distributions of sequences from the embedded musical word space. The network can create sequences that are musically coherent and shows an improved quantitative and qualitative measures
Link: https://arxiv.org/abs/1710.11418
====================================================
Homophily of Music Listening in Online Social Networks (Zhenkun Zhou - 29 October, 2017)
It is found that female friends are more homogeneous in music listening and positive and energetic songs significantly pull users close. Our methodology and findings would shed lights on realistic applications in online music services.
Link: https://arxiv.org/abs/1710.10642
====================================================
Retirement Transition in the Digital Ecology: Reflecting on Identity Reconstruction and Technology Appropriation (Mao Mao - 15 October, 2017)
We deepened our understanding of retirement transitions with technologies by showing how retirees participating in community music make sense of new rules and norms after retirement. A key finding is that retirees reconstruct identities by connecting with music communities, through which they can develop an understanding of unfamiliar patterns of the retired life, and gain support musically and socially
Link: https://arxiv.org/abs/1710.08867
====================================================
NoReC: The Norwegian Review Corpus (Erik Velldal - 15 October, 2017)
The full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories
Link: https://arxiv.org/abs/1710.05370
====================================================
Current Challenges and Visions in Music Recommender Systems Research (Markus Schedl - 21 March, 2018)
Music recommender systems (MRS) have experienced a boom in recent years, thanks to the emergence and success of online streaming services, which nowadays make available almost all music in the world at the user's fingertip. While today's MRS considerably help users to find interesting music in these huge catalogs, MRS research is still facing substantial challenges
Link: https://arxiv.org/abs/1710.03208
====================================================
Generating Nontrivial Melodies for Music as a Service (Yifei Teng - 6 October, 2017)
The autoencoder can make musically plausible variations on an existing melody, suitable for recurring motifs. The generated music compares favorably with that generated by other academic and commercial software designed for the music-as-a-service industry.
Link: https://arxiv.org/abs/1710.02280
====================================================
Tracking Persons-of-Interest via Unsupervised Representation Adaptation (Shun Zhang - 5 October, 2017)
We extensively evaluate the proposed algorithm on two sets of TV sitcoms and YouTube music videos, analyze the contribution of each component, and demonstrate significant performance improvement over existing techniques.
Link: https://arxiv.org/abs/1710.02139
====================================================
Collaboration Success Factors in an Online Music Community (Fabio Calefato - 18 October, 2017)
However, online communities also involve creative arts domains such as animation, video games, and music. In this paper, we present a study on creative collaboration in a music community where authors write songs together by 'overdubbing,' that is, by mixing a new track with an existing audio recording
Link: https://arxiv.org/abs/1710.00366
====================================================
Measuring the Eccentricity of Items (Chanyoung Park - 28 September, 2017)
We used this metric to analyze two real-world datasets of music and movies and observed the characteristics of items in terms of eccentricity
Link: https://arxiv.org/abs/1709.10060
====================================================
NIME: A Community of Communities (Michael J. Lyons - 7 September, 2017)
Commentary on the article Fourteen Years of NIME: The Value and Meaning of Community in Interactive Music Research by A
Link: https://arxiv.org/abs/1709.09733
====================================================
On the Complex Network Structure of Musical Pieces: Analysis of Some Use Cases from Different Music Genres (Stefano Ferretti - 13 September, 2017)
We analyze some main tracks coming from different music genres, with melodies played using different musical instruments. This approach can have an impact in several multimedia applications such as music didactics, multimedia entertainment, and digital music generation.
Link: https://arxiv.org/abs/1709.09708
====================================================
Emotion-Recognition Using Smart Watch Accelerometer Data: Preliminary Findings (Juan C. Quiroz - 26 September, 2017)
Participants were primed either with audio-visual (movie clips) or audio (classical music) to elicit emotional responses
Link: https://arxiv.org/abs/1709.09148
====================================================
Learning a Predictive Model for Music Using PULSE (Jonas Langhabel - 26 September, 2017)
The proposed method significantly outperforms comparable state-of-the-art models. The models learned by PULSE afford an easy inspection and are musicologically interpreted for the first time.
Link: https://arxiv.org/abs/1709.08842
====================================================
Image operator learning coupled with CNN classification and its application to staff line removal (Frank D. Julca-Aguilar - 19 September, 2017)
The problem of removing staff-lines in music score images is chosen to evaluate the effects of window and convolutional mask sizes on the learned image operator performance
Link: https://arxiv.org/abs/1709.06476
====================================================
Interactive Music Generation with Positional Constraints using Anticipation-RNNs (GaÃ«tan Hadjeres - 19 September, 2017)
However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation. This fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes.
Link: https://arxiv.org/abs/1709.06404
====================================================
Clustering of Musical Pieces through Complex Networks: an Assessment over Guitar Solos (Stefano Ferretti - 14 September, 2017)
This fosters innovative ways to categorize music, paving the way towards novel applications in multimedia domains, such as music didactics, multimedia entertainment and digital music generation. Clustering these networks through their main metrics allows grouping similar musical tracks
Link: https://arxiv.org/abs/1709.05193
====================================================
The wave method of building color palette and its application in computer graphics (I. I. Sabo - 10 September, 2017)
Presents a parallel between harmoniously matched colors and the concept of harmony in music theory (consonance)
Link: https://arxiv.org/abs/1709.04752
====================================================
A Tutorial on Deep Learning for Music Information Retrieval (Keunwoo Choi - 3 May, 2018)
Following their success in Computer Vision and other areas, deep learning techniques have recently become widely adopted in Music Information Retrieval (MIR) research. However, the majority of works aim to adopt and assess methods that have been shown to be effective in other domains, while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight
Link: https://arxiv.org/abs/1709.04396
====================================================
Flexible End-to-End Dialogue System for Knowledge Grounded Conversation (Wenya Zhu - 13 September, 2017)
We collect a human-human conversation data (ConversMusic) with knowledge annotations. The proposed method is evaluated on CoversMusic and a public question answering dataset
Link: https://arxiv.org/abs/1709.04264
====================================================
What were you expecting? Using Expectancy Features to Predict Expressive Performances of Classical Piano Music (Carlos Cancino-ChacÃ³n - 11 September, 2017)
In this paper we present preliminary work examining the relationship between the formation of expectations and the realization of musical performances, paying particular attention to expressive tempo and dynamics. To compute features that reflect what a listener is expecting to hear, we employ a computational model of auditory expectation called the Information Dynamics of Music model (IDyOM)
Link: https://arxiv.org/abs/1709.03629
====================================================
Basic Filters for Convolutional Neural Networks Applied to Music: Training or Design? (Monika Doerfler - 19 September, 2018)
When convolutional neural networks are used to tackle learning problems based on music or, more generally, time series data, raw one-dimensional data are commonly pre-processed to obtain spectrogram or mel-spectrogram coefficients, which are then used as input to the actual neural network. We also conducted extensive experimental work on the task of singing voice detection in music
Link: https://arxiv.org/abs/1709.02291
====================================================
Composition by Conversation (Donya Quick - 7 September, 2017)
Most musical programming languages are developed purely for coding virtual instruments or algorithmic compositions. Although there has been some work in the domain of musical query languages for music information retrieval, there has been little attempt to unify the principles of musical programming and query languages with cognitive and natural language processing models that would facilitate the activity of composition by conversation
Link: https://arxiv.org/abs/1709.02076
====================================================
A Comparison of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging (Keunwoo Choi - 3 June, 2018)
In this paper, we empirically investigate the effect of audio preprocessing on music tagging with deep neural networks
Link: https://arxiv.org/abs/1709.01922
====================================================
Probabilistic Rule Realization and Selection (Haizi Yu - 9 March, 2018)
Taking music compositional rules as the main example throughout the paper, we demonstrate our model's efficiency in not only music realization (composition) but also music interpretation and understanding (analysis).
Link: https://arxiv.org/abs/1709.01674
====================================================
Deep Learning Techniques for Music Generation - A Survey (Jean-Pierre Briot - 5 September, 2017)
At first, we propose a methodology based on four dimensions for our analysis: - objective - What musical content is to be generated? (e.g., melody, accompaniment...); - representation - What are the information formats used for the corpus and for the expected generated output? (e.g., MIDI, piano roll, text...); - architecture - What type of deep neural network is to be used? (e.g., recurrent network, autoencoder, generative adversarial networks...); - strategy - How to model and control the process of generation (e.g., direct feedforward, sampling, unit selection...). This classification is bottom-up, based on the analysis of many existing deep-learning based systems for music generation, which are described in this book
Link: https://arxiv.org/abs/1709.01620
====================================================
Deep rank-based transposition-invariant distances on musical sequences (GaÃ«tan Hadjeres - 3 September, 2017)
It is a hybrid distance which combines learned feature representations of musical sequences with a handcrafted rank distance. This distance depends less on the musical encoding of the data than previous methods and gives perceptually good results
Link: https://arxiv.org/abs/1709.00740
====================================================
Computational Topology Techniques for Characterizing Time-Series Data (Nicole Sanderson - 1 September, 2017)
Its results allow us to distinguish time-series data from different systems - e.g., the same note played on different musical instruments.
Link: https://arxiv.org/abs/1708.09359
====================================================
Distributed Holistic Clustering on Linked Data (Markus Nentwig - 30 August, 2017)
We provide a novel gold standard for multi-source clustering, and evaluate our methods with respect to effectiveness and efficiency for large data sets from the geographic and music domains.
Link: https://arxiv.org/abs/1708.09299
====================================================
Large-Scale User Modeling with Recurrent Neural Networks for Music Discovery on Multiple Time Scales (Cedric De Boom - 22 August, 2017)
Collaborative filtering has the disadvantage that it relies on explicit ratings, which are often unavailable, and generally disregards the temporal nature of music consumption. This way we obtain semantically rich user representations, which capture a user's musical taste over time
Link: https://arxiv.org/abs/1708.06520
====================================================
Image2song: Song Retrieval via Bridging Image Content and Lyric Words (Xuelong Li - 19 August, 2017)
We collect a dataset from the social-sharing multimodal data to study the proposed problem, which consists of (image, music clip, lyric) triplets
Link: https://arxiv.org/abs/1708.05851
====================================================
Learning Musical Relations using Gated Autoencoders (Stefan Lattner - 17 August, 2017)
We find that it is difficult to learn musical transformations with the RBM and that the GAE is much more adequate for this task, since it is able to learn representations of specific transformations that are largely content-invariant. We believe these results show that models such as GAEs may provide the basis for more encompassing music analysis systems, by endowing them with a better understanding of the structures underlying music.
Link: https://arxiv.org/abs/1708.05325
====================================================
Neural Collaborative Filtering (Xiangnan He - 25 August, 2017)
Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods
Link: https://arxiv.org/abs/1708.05031
====================================================
Independent Low-Rank Matrix Analysis Based on Complex Student's $t$-Distribution for Blind Audio Source Separation (Shinichi Mogami - 16 August, 2017)
Experiments are conducted using both music and speech BSS tasks, and the results show the validity of the proposed method.
Link: https://arxiv.org/abs/1708.04795
====================================================
Convolutive Audio Source Separation using Robust ICA and an intelligent evolving permutation ambiguity solution (Dimitrios Mallis - 13 August, 2017)
The application of the MuSIC algorithm, as a preprocessing step to the previous solution, forms a second methodology with promising results.
Link: https://arxiv.org/abs/1708.03989
====================================================
Creating an A Cappella Singing Audio Dataset for Automatic Jingju Singing Evaluation Research (Rong Gong - 13 August, 2017)
This dataset is also an extension our existing CompMusic jingju corpus. Both professional and amateur singers were invited to the dataset recording sessions, and the most common jingju musical elements have been covered
Link: https://arxiv.org/abs/1708.03986
====================================================
Classical Music Composition Using State Space Models (Anna K. Yanchenko - 28 September, 2018)
However, we conclude that the major limitation in using these models to generate music that sounds like it was composed by a human is the lack of melodic progression in the composed pieces. We also examine the performance of the models in the context of music theory.
Link: https://arxiv.org/abs/1708.03822
====================================================
Neural Translation of Musical Style (Iman Malik - 11 August, 2017)
In this paper, we describe a model that can learn to perform sheet music. Our research concludes that the generated performances are indistinguishable from a human performance, thereby passing a test in the spirit of a "musical Turing test".
Link: https://arxiv.org/abs/1708.03535
====================================================
Automatic Raga Recognition in Hindustani Classical Music (Sanchit Alekh - 7 August, 2017)
Raga is the central melodic concept in Hindustani Classical Music
Link: https://arxiv.org/abs/1708.02322
====================================================
Generative Statistical Models with Self-Emergent Grammar of Chord Sequences (Hiroaki Tsushima - 2 March, 2018)
Generative statistical models of chord sequences play crucial roles in music processing
Link: https://arxiv.org/abs/1708.02255
====================================================
Aktuelle Entwicklungen in der Automatischen Musikverfolgung (Andreas Arzt - 7 August, 2017)
In this way at any given time the exact position of the musician(s) in the sheet music is computed. The latter enables direct learning of correspondences between complex audio data and images of the sheet music, avoiding the complicated and time-consuming definition of a mid-level representation.
Link: https://arxiv.org/abs/1708.02100
====================================================
Piece Identification in Classical Piano Music Without Reference Scores (Andreas Arzt - 2 August, 2017)
The main challenge is the amount of noise that is introduced into the identification process by the music transcription algorithm and the automatic (but possibly suboptimal) choice of performances to represent a piece in the reference database. As the results show this approach leads to a robust system that is able to identify piano music with high accuracy -- without any need for data annotation or manual data preparation.
Link: https://arxiv.org/abs/1708.00733
====================================================
Predicting Session Length in Media Streaming (Theodore Vasiloudis - 31 July, 2017)
In this work we present the first analysis of session length in a mobile-focused online service, using a real world data-set from a major music streaming service
Link: https://arxiv.org/abs/1708.00130
====================================================
Multiple Stakeholders in Music Recommender Systems (Himan Abdollahpouri - 31 July, 2017)
Advertisements are generally interspersed throughout the music stream to generate revenue for the business. These stakeholders each have their own objectives and must work in concert to sustain a healthy music recommendation service.
Link: https://arxiv.org/abs/1708.00120
====================================================
Learning Audio - Sheet Music Correspondences for Score Identification and Offline Alignment (Matthias Dorfer - 31 July, 2017)
We show how to employ neural network-based cross-modality embedding spaces for solving the following two sheet music-related tasks: retrieving the correct piece of sheet music from a database when given a music audio as a search query; and aligning an audio recording of a piece with the corresponding images of sheet music. We demonstrate the feasibility of this in experiments on classical piano music by five different composers (Bach, Haydn, Mozart, Beethoven and Chopin), and additionally provide a discussion on why we expect multi-modal neural networks to be a fruitful paradigm for dealing with sheet music and audio at the same time.
Link: https://arxiv.org/abs/1707.09887
====================================================
Recurrent Ladder Networks (Isabeau PrÃ©mont-Schwarz - 18 December, 2017)
The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues
Link: https://arxiv.org/abs/1707.09219
====================================================
A novel CS Beamformer root-MUSIC algorithm and its subspace deviation analysis (Abhishek Aich - 27 September, 2017)
In this paper, a novel CS beamformer root-MUSIC algorithm is presented with a revised optimal measurement matrix bound. The CS beamformer greatly reduces computational complexity without affecting resolution of the algorithm, works on par with root-MUSIC under low snapshot scenario and also, gives an option of non-uniform linear array sensors unlike the case of root-MUSIC algorithm
Link: https://arxiv.org/abs/1707.08115
====================================================
Machine Intelligence, New Interfaces, and the Art of the Soluble (Michael J. Lyons - 25 July, 2017)
Position: (1) Partial solutions to machine intelligence can lead to systems which may be useful creating interesting and expressive musical works. (3) The study of the aesthetics of human augmentation in musical performance is in its infancy.
Link: https://arxiv.org/abs/1707.08011
====================================================
HMM-based Writer Identification in Music Score Documents without Staff-Line Removal (Partha Pratim Roy - 27 July, 2017)
Given a query musical sheet, writer specific confidence for each musical line is returned by each writer specific model using a loglikelihood score. A novel Factor Analysis based feature selection technique is applied in sliding window features to reduce the noise appearing from staff lines which proves efficiency in writer identification performance.In our framework we have also proposed a novel score line detection approach in musical sheet using HMM
Link: https://arxiv.org/abs/1707.06828
====================================================
On the mathematics of beauty: beautiful music (A. M. Khalili - 5 August, 2018)
In this paper, we will study the simplest kind of beauty that can be found in a simple piece of music and can be appreciated universally. The proposed model is tested on a set of beautiful music pieces.
Link: https://arxiv.org/abs/1707.06510
====================================================
From Bach to the Beatles: The simulation of human tonal expectation using ecologically-trained predictive models (Carlos Cancino-ChacÃ³n - 19 July, 2017)
Our experiments indicate that various types of recurrent neural networks produce musical expectations that clearly convey tonal structure. Furthermore, the results imply that although implicit knowledge of tonal structure is a necessary condition for accurate musical expectation, the most accurate predictive models also use other cues beyond the tonal structure of the musical context.
Link: https://arxiv.org/abs/1707.06231
====================================================
Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio (Georgi Dzhambazov - 19 July, 2017)
We carry out an evaluation on a varied collection of multi-instrument datasets from two music traditions (English popular music and Turkish makam) with different types of metrical cycles and singing styles
Link: https://arxiv.org/abs/1707.06163
====================================================
A Comparative Analysis of Social Network Pages by Interests of Their Followers (Elena Mikhalkova - 17 October, 2017)
We take three interest domains that are typical of both English and Russian-speaking communities: football, rock music, vegetarianism
Link: https://arxiv.org/abs/1707.05481
====================================================
Lyrics-Based Music Genre Classification Using a Hierarchical Attention Network (Alexandros Tsaptsinos - 14 July, 2017)
Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres.
Link: https://arxiv.org/abs/1707.04678
====================================================
GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures (GaÃ«tan Hadjeres - 14 July, 2017)
We demonstrate its efficiency on a monophonic music generation task where we manage to generate variations of discrete sequences in an intended and playful way.
Link: https://arxiv.org/abs/1707.04588
====================================================
Modeling Harmony with Skip-Grams (David R. W. Sears - 18 July, 2017)
To address this problem, this study examines the efficacy of skip-grams in music research, an alternative viewpoint method developed in corpus linguistics and natural language processing that includes sub-sequences of n events (or n-grams) in a frequency distribution if their constituent members occur within a certain number of skips.
Link: https://arxiv.org/abs/1707.04457
====================================================
Audio to score matching by combining phonetic and duration information (Rong Gong - 12 July, 2017)
We argue that, due to the existence of a basic melodic contour for each mode in jingju music, only using melodic information (such as pitch contour) will result in an ambiguous matching
Link: https://arxiv.org/abs/1707.03547
====================================================
Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks (Jordi Pons - 12 July, 2017)
In addition, we propose using a score-informed Viterbi algorithm -instead of thresholding the onset function-, because the available musical knowledge we have (the score) can be used to inform the Viterbi algorithm in order to overcome the identified challenges. The proposed method outperforms the state-of-the-art in syllable segmentation for jingju a cappella singing
Link: https://arxiv.org/abs/1707.03544
====================================================
Evaluating Social Networks Using Task-Focused Network Inference (Ivan Brugere - 7 July, 2017)
We apply this general framework to the case study on collective classification of music preferences in a newly available dataset of the Last.fm social network.
Link: https://arxiv.org/abs/1707.02385
====================================================
Creative Robot Dance with Variational Encoder (Agnese Augello - 5 July, 2017)
What we appreciate in dance is the ability of people to sponta- neously improvise new movements and choreographies, sur- rendering to the music rhythm, being inspired by the cur- rent perceptions and sensations and by previous experiences, deeply stored in their memory. In particular, we exploit a deep learning approach that allows a robot to generate in real time new dancing move- ments according to to the listened music.
Link: https://arxiv.org/abs/1707.01489
====================================================
Automatic estimation of harmonic tension by distributed representation of chords (Ali Nikrang - 4 July, 2017)
A veridical computational model of perceived musical tension would be an important ingredient for many music informatics applications. To assess the veridicality of the model, we compare its outputs on a number of well-defined chord classes and cadential contexts to results from pertinent empirical studies in music psychology
Link: https://arxiv.org/abs/1707.00972
====================================================
A Deep Multimodal Approach for Cold-start Music Recommendation (Sergio Oramas - 24 July, 2017)
An increasing amount of digital music is being published daily. Music streaming services often ingest all available music, but this poses a challenge: how to recommend new artists for which prior knowledge is scarce? In this work we aim to address this so-called cold-start problem by combining text and audio information with user feedback data using deep network architectures
Link: https://arxiv.org/abs/1706.09739
====================================================
Talking Drums: Generating drum grooves with neural networks (P. Hutchings - 28 June, 2017)
A sequence to sequence neural network model used in natural language translation was adopted to encode multiple musical styles and an online survey was developed to test different techniques for sampling the output of the softmax function
Link: https://arxiv.org/abs/1706.09558
====================================================
Machine listening intelligence (C. E. Cella - 28 June, 2017)
This manifesto paper will introduce machine listening intelligence, an integrated research framework for acoustic and musical signals modelling, based on signal processing, deep learning and computational musicology.
Link: https://arxiv.org/abs/1706.09557
====================================================
Music Signal Processing Using Vector Product Neural Networks (Z. C. Fan - 28 June, 2017)
We propose a novel neural network model for music signal processing using vector product neurons and dimensionality transformations
Link: https://arxiv.org/abs/1706.09555
====================================================
Transforming Musical Signals through a Genre Classifying Convolutional Neural Network (S. Geng - 28 June, 2017)
In this paper, we propose a method to utilize the stored information from a CNN trained on musical genre classification task. In addition to the potential of such CNNs to produce interesting audio transformation, more information about the network and the original music could be obtained from the analysis of the generated features since these features indicate how the network 'understands' the music.
Link: https://arxiv.org/abs/1706.09553
====================================================
Modeling Musical Context with Word2vec (Dorien Herremans - 28 June, 2017)
A visualization of the reduced vector space using t-distributed stochastic neighbor embedding shows that the resulting embedded vector space captures tonal relationships, even without any explicit information about the musical contents of the slices. The resulting music shows that the selected slice based on similar word2vec context also has a relatively short tonal distance from the original slice.
Link: https://arxiv.org/abs/1706.09088
====================================================
The Minor Fall, the Major Lift: Inferring Emotional Valence of Musical Chords through Lyrics (Artemy Kolchinsky - 4 December, 2017)
We investigate the association between musical chords and lyrics by analyzing a large dataset of user-contributed guitar tablatures. We also examine the usage patterns of chords and lyrics in different musical genres, historical eras, and geographical regions
Link: https://arxiv.org/abs/1706.08609
====================================================
Wideband DOA Estimation through Projection Matrix Interpolation (J. Selva - 26 June, 2017)
The first is a fast procedure to compute one-dimensional search estimators like Multiple Signal Classification (MUSIC), that exploits the close relation between Chebyshev interpolation and the Discrete Cosine Transform (DCT)
Link: https://arxiv.org/abs/1706.08280
====================================================
Toward Faultless Content-Based Playlists Generation for Instrumentals (Yann Bayle - 22 November, 2017)
In this paper, we consider an industrial real-world musical database that is unevenly distributed between Songs and Instrumentals and bigger than databases used in previous studies. We give insight on how to improve further the quality of generated playlists and to extend our methods to other musical tags
Link: https://arxiv.org/abs/1706.07613
====================================================
Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of Deep Neural Network Models with Keras (Keunwoo Choi - 19 June, 2017)
We introduce Kapre, Keras layers for audio and music signal preprocessing. Music research using deep neural networks requires a heavy and tedious preprocessing stage, for which audio processing parameters are often ignored in parameter optimisation
Link: https://arxiv.org/abs/1706.05781
====================================================
Investigating the Potential of Pseudo Quadrature Mirror Filter-Banks in Music Source Separation Tasks (Stylianos Ioannis Mimilakis - 15 June, 2017)
In this work we investigate the potential of an optimized pseudo quadrature mirror filter-bank (PQMF), as a T-F representation for music source separation tasks. Experimental results, suggest that the PQMF maintains the aforementioned desirable properties and can be regarded as an alternative for representing mixtures of musical signals.
Link: https://arxiv.org/abs/1706.04924
====================================================
Learning and Evaluating Musical Features with Deep Autoencoders (Mason Bretan - 15 June, 2017)
In this work we describe and evaluate methods to learn musical embeddings. Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation
Link: https://arxiv.org/abs/1706.04486
====================================================
End-to-End Musical Key Estimation Using a Convolutional Neural Network (Filip Korzeniowski - 9 June, 2017)
for pop music performs subpar on pieces of electronic music. However, using the data-driven approach proposed in this paper, we can train models that deal with multiple musical styles adequately, and without major losses in accuracy.
Link: https://arxiv.org/abs/1706.02921
====================================================
The Effects of Noisy Labels on Deep Convolutional Neural Networks for Music Tagging (Keunwoo Choi - 14 November, 2017)
Deep neural networks (DNN) have been successfully applied to music classification including music tagging. We analyse and (re-)validate a large music tagging dataset to investigate the reliability of training and evaluation
Link: https://arxiv.org/abs/1706.02361
====================================================
Item-Item Music Recommendations With Side Information (ÃzgÃ¼r Demir - 18 December, 2017)
The content itself is broad and covers various musical genres as well as non-musical audio content such as radio plays and podcasts. On a data set from music streaming service SoundCloud, the method here outperforms the widely adopted implicit matrix factorization technique
Link: https://arxiv.org/abs/1706.00218
====================================================
Deep Complex Networks (Chiheb Trabelsi - 25 February, 2018)
We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.
Link: https://arxiv.org/abs/1705.09792
====================================================
Joint Sparse Recovery With Semisupervised MUSIC (Zaidao Wen - 26 May, 2017)
From this viewpoint, we develop a semisupervised MUSIC (SS-MUSIC) in the spirit of machine learning, which declares that the insufficient supervised information in the training samples can be compensated from those unlabeled atoms. Numerical experimental results demonstrate that SS-MUSIC can achieve much better recovery performances than other MUSIC extended algorithms as well as some typical greedy algorithms for JSR in terms of iterations and recovery probability.
Link: https://arxiv.org/abs/1705.09446
====================================================
Taste or Addiction?: Using Play Logs to Infer Song Selection Motivation (Kosetsu Tsukuda - 26 May, 2017)
First, to the best of our knowledge, this is the first study modeling music listening behavior by taking into account the influence of addiction to artists. Third, we carried out qualitative experiments and showed that taking addiction into account enables us to analyze music listening behavior from a new viewpoint in terms of how people listen to music according to the time of day, how an artist's songs are listened to by people, etc
Link: https://arxiv.org/abs/1705.09439
====================================================
StegIbiza: Steganography in Club Music Implemented in Python (Krzysztof Szczypiorski - 22 May, 2017)
Once the message was hidden into a music files, an internet radio was created to evaluate broadcast possibilities. No dedicated music or signal processing equipment was used in this StegIbiza implementation
Link: https://arxiv.org/abs/1705.07788
====================================================
End-to-End Cross-Modality Retrieval with CCA Projections and Pairwise Ranking Loss (Matthias Dorfer - 16 April, 2018)
We show the effectiveness of our approach for cross-modality retrieval on three different scenarios (text-to-image, audio-sheet-music and zero-shot retrieval), surpassing both Deep CCA and a multi-view network using freely learned projections optimized by a pairwise ranking loss, especially when little training data is available (the code for all three methods is released at: https://github.com/CPJKU/cca_layer).
Link: https://arxiv.org/abs/1705.06979
====================================================
Music generation with variational recurrent autoencoder supported by history (Alexey Tikhonov - 14 July, 2017)
A serious problem for automated music generation is to propose the model that could reproduce sophisticated temporal and melodic patterns that would correspond to the style of the training input
Link: https://arxiv.org/abs/1705.05458
====================================================
Understanding MIDI: A Painless Tutorial on Midi Format (H. M. de Oliveira - 15 May, 2017)
The goal is to explain the file structure and how the instructions are used to produce a music signal, both in the case of monophonic signals as for polyphonic signals.
Link: https://arxiv.org/abs/1705.05322
====================================================
Modeling of the Latent Embedding of Music using Deep Neural Network (Zhou Xing - 11 May, 2017)
This approach can be used to discover the latent factor models of the music based upon acoustic hyper-images that are extracted from the raw audio waves of music. These latent embeddings can be used either as features to feed to subsequent models, such as collaborative filtering, or to build similarity metrics between songs, or to classify music based on the labels for training such as genre, mood, sentiment, etc.
Link: https://arxiv.org/abs/1705.05229
====================================================
Riddim: A Rhythm Analysis and Decomposition Tool Based On Independent Subspace Analysis (Iroro Orife - 13 May, 2017)
The goal of this thesis was to implement a tool that, given a digital audio input, can extract and represent rhythm and musical time. This information is then represented in a format that can be used by a variety of algorithms that interpret timing information to infer rhythmic and musical structure
Link: https://arxiv.org/abs/1705.04792
====================================================
Mining Communication Data in a Music Community: A Preliminary Analysis (Fabio Calefato - 25 February, 2018)
In this paper, we report the results of a preliminary study aimed at mining the communication network of a music community for collaborative songwriting, where users collaborate online by first uploading new songs and then by adding new tracks and providing feedback in forms of comments.
Link: https://arxiv.org/abs/1705.04485
====================================================
Deep Cross-Modal Audio-Visual Generation (Lele Chen - 26 April, 2017)
Specifically, we use conditional generative adversarial networks to achieve cross-modal audio-visual generation of musical performances. Being the first to explore this new problem, we compose two new datasets with pairs of images and sounds of musical performances of different instruments
Link: https://arxiv.org/abs/1704.08292
====================================================
Content-Based Video-Music Retrieval Using Soft Intra-Modal Structure Constraint (Sungeun Hong - 1 September, 2017)
We also introduce reasonable quantitative and qualitative experimental protocols to solve the lack of standard protocols for less-mature video-music related tasks. Finally, we construct a large-scale 200K video-music pair benchmark
Link: https://arxiv.org/abs/1704.06761
====================================================
Control Improvisation (Daniel J. Fremont - 20 April, 2017)
Other applications include robotic surveillance, machine improvisation of music, and randomized variants of the supervisory control problem
Link: https://arxiv.org/abs/1704.06319
====================================================
CNN based music emotion classification (Xin Liu - 19 April, 2017)
In this paper, we propose a novel MER method by using deep convolutional neural network (CNN) on the music spectrograms that contains both the original time and frequency domain information. Results show that, for both datasets, the proposed method outperforms state-of-the-art methods.
Link: https://arxiv.org/abs/1704.05665
====================================================
Diagonal RNNs in Symbolic Music Modeling (Y. Cem Subakan - 19 April, 2017)
We show the benefits of using diagonal recurrent matrices with popularly used LSTM and GRU architectures as well as with the vanilla RNN architecture, on four standard symbolic music datasets.
Link: https://arxiv.org/abs/1704.05420
====================================================
Revisiting the problem of audio-based hit song prediction using convolutional neural networks (Li-Chia Yang - 5 April, 2017)
Being able to predict whether a song can be a hit has impor- tant applications in the music industry. Although it is true that the popularity of a song can be greatly affected by exter- nal factors such as social and commercial influences, to which degree audio features computed from musical signals (whom we regard as internal factors) can predict song popularity is an interesting research question on its own
Link: https://arxiv.org/abs/1704.01280
====================================================
Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders (Jesse Engel - 5 April, 2017)
Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets
Link: https://arxiv.org/abs/1704.01279
====================================================
MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation (Li-Chia Yang - 18 July, 2017)
However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e
Link: https://arxiv.org/abs/1703.10847
====================================================
Transfer learning for music classification and regression tasks (Keunwoo Choi - 13 September, 2017)
In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features.
Link: https://arxiv.org/abs/1703.09179
====================================================
Simplifying the Bible and Wikipedia Using Statistical Machine Translation (Yohan Jo - 25 March, 2017)
I started this work with the hope of generating a text synthesizer (like a musical synthesizer) that can imitate certain linguistic styles
Link: https://arxiv.org/abs/1703.08646
====================================================
Visual Analyses of Music History: A User-Centric Approach (Jingxian Zhang - 22 March, 2017)
Music history, referring to the records of users' listening or downloading history in online music services, is the primary source for music service providers to analyze users' preferences on music and thus to provide personalized recommendations to users. In order to engage users into the service and to improve user experience, it would be beneficial to provide visual analyses of one user's music history as well as visualized recommendations to that user
Link: https://arxiv.org/abs/1703.07534
====================================================
Dance Dance Convolution (Chris Donahue - 20 June, 2017)
Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts
Link: https://arxiv.org/abs/1703.06891
====================================================
Timbre Analysis of Music Audio Signals with Convolutional Neural Networks (Jordi Pons - 2 June, 2017)
Several architectures based on the design principles we propose are successfully assessed for different research tasks related to timbre: singing voice phoneme classification, musical instrument recognition and music auto-tagging.
Link: https://arxiv.org/abs/1703.06697
====================================================
Sampling Variations of Lead Sheets (Pierre Roy - 2 March, 2017)
We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity measure defined by Mongeau and Sankoff. We then show how this mechanism can used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem.
Link: https://arxiv.org/abs/1703.00760
====================================================
Constructing Adjacency Arrays from Incidence Arrays (Hayden Jananthan - 24 February, 2017)
Illustrations of the various results possible from different $\oplus$ and $\otimes$ operations are provided using a small collection of popular music metadata.
Link: https://arxiv.org/abs/1702.07832
====================================================
Transitioning Between Audience and Performer: Co-Designing Interactive Music Performances with Children (Alina Striner - 20 February, 2017)
This work presents findings from three co-design sessions with children that investigated how audiences might want to interact with live music performances, including design considerations and opportunities. Findings from these sessions also formed a Spectrum of Audience Interactivity in live musical performances, outlining ways to encourage interactivity in music performances from the child perspective.
Link: https://arxiv.org/abs/1702.06236
====================================================
On the Importance of Temporal Context in Proximity Kernels: A Vocal Separation Case Study (Delia Fano Yela - 11 April, 2017)
Musical source separation methods exploit source-specific spectral characteristics to facilitate the decomposition process
Link: https://arxiv.org/abs/1702.02130
====================================================
Non-colocated Time-Reversal MUSIC: High-SNR Distribution of Null Spectrum (D. Ciuonzo - 25 January, 2017)
We derive the asymptotic distribution of the null spectrum of the well-known Multiple Signal Classification (MUSIC) in its computational Time-Reversal (TR) form. The result pertains to a single-frequency non-colocated multistatic scenario and several TR-MUSIC variants are here investigated
Link: https://arxiv.org/abs/1701.07516
====================================================
Investigating the role of musical genre in human perception of music stretching resistance (Jun Chen - 12 January, 2017)
These findings lead to a new measurement on the similarity between different musical genres based on their music stretching resistance. In addition, the analysis of variance (ANOVA) also supports the findings in this paper by verifying the significance of musical genre in shaping music stretching resistance.
Link: https://arxiv.org/abs/1701.03274
====================================================
A hybrid approach to supervised machine learning for algorithmic melody composition (Rouven Bauer - 29 December, 2016)
An online listening test conducted shows that enhancing a pure Markov model with musically relevant context, like count and planed melody contour, improves the result significantly.
Link: https://arxiv.org/abs/1612.09212
====================================================
DOA Estimation of Coherent Signals on Coprime Arrays Exploiting Fourth-Order Cumulants (Yang Hu - 21 January, 2017)
From the smoothed FCM, the DOAs of both the coherent and independent signals can be successfully estimated on the pseudo-spectrum generated by the fourth-order MUSIC algorithm
Link: https://arxiv.org/abs/1612.06537
====================================================
A Phase Vocoder based on Nonstationary Gabor Frames (Emil SolsbÃ¦k Ottosen - 6 September, 2017)
We propose a new algorithm for time stretching music signals based on the theory of nonstationary Gabor frames (NSGFs). The proposed algorithm is tested on both synthetic and real world signals and compared with state of the art algorithms in a reproducible manner.
Link: https://arxiv.org/abs/1612.05156
====================================================
Towards End-to-End Audio-Sheet-Music Retrieval (Matthias Dorfer - 15 December, 2016)
This would be highly useful in many content-based musical retrieval scenarios. Initial experiments with relatively simple monophonic music show promising results.
Link: https://arxiv.org/abs/1612.05070
====================================================
Towards Score Following in Sheet Music Images (Matthias Dorfer - 15 December, 2016)
It learns to predict, for a given unseen audio snippet (covering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks -- which have proven to be powerful image processing models -- working with sheet music becomes feasible and a promising future research direction.
Link: https://arxiv.org/abs/1612.05050
====================================================
Music Generation with Deep Learning (Vasanth Kalingeri - 15 December, 2016)
Fully connected and convolutional layers are used along with LSTM's to capture rich features in the frequency domain and increase the quality of music generated. The work is focused on unconstrained music generation and uses no information about musical structure(notes or chords) to aid learning.The music generated from various architectures are compared using blind fold tests
Link: https://arxiv.org/abs/1612.04928
====================================================
Imposing higher-level Structure in Polyphonic Music Generation using Convolutional Restricted Boltzmann Machines and Constraints (Stefan Lattner - 14 April, 2018)
We introduce a method for imposing higher-level structure on generated, polyphonic music. Results show that with this approach it is possible to control the higher-level self-similarity structure, the meter, and the tonal properties of the resulting musical piece, while preserving its local musical coherence.
Link: https://arxiv.org/abs/1612.04742
====================================================
You Are What You Eat... Listen to, Watch, and Read (Mason Bretan - 13 December, 2016)
A personality specific topic model describing a person's favorite books, movies, shows, music, and food was generated using latent Dirichlet allocation (LDA). There were several significant findings, for example, intuitive thinking types preferred sci-fi/fantasy entertainment, extraversion correlated positively with upbeat dance music, and jazz, folk, and international cuisine correlated positively with those characterized by openness to experience
Link: https://arxiv.org/abs/1612.04403
====================================================
A Robotic Prosthesis for an Amputee Drummer (Mason Bretan - 13 December, 2016)
A secondary objective of the prosthesis is to explore the implications of musical expression and human-robotic interaction when a second, completely autonomous, stick is added to the prosthesis. This wearable robotic musician interacts with the user by listening to the music and responding with different rhythms and behaviors
Link: https://arxiv.org/abs/1612.04391
====================================================
A Unit Selection Methodology for Music Generation Using Deep Neural Networks (Mason Bretan - 12 December, 2016)
We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task
Link: https://arxiv.org/abs/1612.03789
====================================================
An Information-theoretic Approach to Machine-oriented Music Summarization (Francisco Raposo - 21 September, 2018)
Music summarization allows for higher efficiency in processing, storage, and sharing of datasets. We now generalize previous conclusions by evaluating the impact of generic summarization of music from a probabilistic perspective
Link: https://arxiv.org/abs/1612.02350
====================================================
Towards computer-assisted understanding of dynamics in symphonic music (Maarten Grachten - 13 December, 2016)
We describe a computational model that interprets dynamics---expressive loudness variations in performances---in terms of the musical score, highlighting differences between performances of the same piece. Although the present model is still in active development, it may pave the road for a consumer-oriented companion to interactive classical music understanding.
Link: https://arxiv.org/abs/1612.02198
====================================================
DeepBach: a Steerable Model for Bach Chorales Generation (GaÃ«tan Hadjeres - 17 June, 2017)
This is in contrast with many automatic music composition approaches which tend to compose music sequentially. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.
Link: https://arxiv.org/abs/1612.01010
====================================================
Computer Assisted Composition with Recurrent Neural Networks (Christian Walder - 29 September, 2017)
Sequence modeling with neural networks has lead to powerful models of symbolic music data. We address the problem of exploiting these models to reach creative musical goals, by combining with human input
Link: https://arxiv.org/abs/1612.00092
====================================================
Fusion of EEG and Musical Features in Continuous Music-emotion Recognition (Nattapong Thammasan - 30 November, 2016)
In this paper, we present a study of fusion of signals of electroencephalogram (EEG), a tool to capture brainwaves at a high-temporal resolution, and musical features at decision level in recognizing the time-varying binary classes of arousal and valence. Our empirical results showed that the fusion could outperform the performance of emotion recognition using only EEG modality that was suffered from inter-subject variability, and this suggested the promise of multimodal fusion in improving the accuracy of music-emotion recognition.
Link: https://arxiv.org/abs/1611.10120
====================================================
C-RNN-GAN: Continuous recurrent neural networks with adversarial training (Olof Mogren - 29 November, 2016)
We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.
Link: https://arxiv.org/abs/1611.09904
====================================================
Getting Closer to the Essence of Music: The Con Espressione Manifesto (Gerhard Widmer - 29 November, 2016)
This text offers a personal and very subjective view on the current situation of Music Information Research (MIR). Motivated by the desire to build systems with a somewhat deeper understanding of music than the ones we currently have, I try to sketch a number of challenges for the next decade of MIR research, grouped around six simple truths about music that are probably generally agreed on, but often ignored in everyday research.
Link: https://arxiv.org/abs/1611.09733
====================================================
SISO and SIMO Accompaniment Cancellation for Live Solo Recordings Based on Short-Time ERB-Band Wiener Filtering and Spectral Subtraction (Stanislaw Gorlow - 27 November, 2016)
Research in collaborative music learning is subject to unresolved problems demanding new technological solutions. Our findings underpin that adaptive filtering is inapt of dealing with music signals and that Wiener filtering in the short-time Fourier transform domain is a much more effective approach
Link: https://arxiv.org/abs/1611.08905
====================================================
MOMOS-MT: Mobile Monophonic System for Music Transcription (Munir Makhmutov - 22 November, 2016)
Using FFT, the system detects the pitch frequencies, also other methods detect note durations, tempo, time signatures and generates sheet music. The final system is able to be used in mobile platforms allowing the user to take recordings and produce sheet music in situ to a performance.
Link: https://arxiv.org/abs/1611.07351
====================================================
Composing Music with Grammar Argumented Neural Networks and Note-Level Encoding (Zheng Sun - 7 December, 2016)
This allows easy implementation of music theory grammars, as well as closer emulation of the thinking pattern of a musician. Although the GA rules are applied to the training data and never directly to the LSTM music generation, our machine still composes music that possess high incidences of diatonic scale notes, small pitch intervals and chords, in deference to music theory.
Link: https://arxiv.org/abs/1611.05416
====================================================
Detecting tala Computationally in Polyphonic Context - A Novel Approach (Susmita Bhaduri - 24 September, 2018)
The human auditory system uses perceptual grouping of musical-elements and easily filters the tabla component, thereby decoding prominent rhythmic features like tala, tempo from a polyphonic composition. For Western music, lots of work have been reported for automated drum analysis of polyphonic composition
Link: https://arxiv.org/abs/1611.05182
====================================================
Song From PI: A Musically Plausible Network for Pop Music Generation (Hang Chu - 10 November, 2016)
Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google
Link: https://arxiv.org/abs/1611.03477
====================================================
Noise reduction combining microphone and piezoelectric device (Naoya Takahashi - 9 November, 2016)
The pickup microphone attached on the specific musical instrument is often employed to obtain the sound exclusively from other instrumental sounds
Link: https://arxiv.org/abs/1611.03178
====================================================
The Machine Learning Algorithm as Creative Musical Tool (Rebecca Fiebrink - 1 November, 2016)
Such an approach offers a significant advantage in music scenarios in which musicians can teach the system to learn an idiosyncratic style, or can break the rules to explore the system's capacity in unexpected ways. In this chapter we draw on music, machine learning, and human-computer interaction to elucidate an understanding of machine learning algorithms as creative tools for music and the sonic arts
Link: https://arxiv.org/abs/1611.00379
====================================================
Collaboration Networks in the Music Industry (Pascal Budner - 1 November, 2016)
We found that the existing research on collaboration networks is corroborated by the particular collaboration network in the music industry. Furthermore, it has been found that the most important professions of the music industry in terms of connectivity were main artists and engineers.
Link: https://arxiv.org/abs/1611.00377
====================================================
MusicMood: Predicting the mood of music from song lyrics using machine learning (Sebastian Raschka - 1 November, 2016)
In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics.
Link: https://arxiv.org/abs/1611.00138
====================================================
Making Mainstream Synthesizers with Csound (Gleb G. Rogozinsky - 16 October, 2016)
For more than the past twenty years, Csound has been one of the leaders in the world of the computer music research, implementing innovative synthesis methods and making them available beyond the academic environments from which they often arise, and into the hands of musicians and sound designers throughout the world
Link: https://arxiv.org/abs/1610.04922
====================================================
Tonal consonance parameters link microscopic and macroscopic properties of music exposing a hidden order in melody (Jorge Useche - 23 April, 2017)
We apply this formalism to melody, showing that melodic lines in musical pieces can be described in terms of the physical properties of melodic intervals and the existence of an entropy extremalization principle subject to psychoacoustic macroscopic constraints with musical meaning. This result connects the human perception of consonance with the complexity of human creativity in music through the physical properties of the musical stimulus.
Link: https://arxiv.org/abs/1610.04551
====================================================
A Music-generating System Inspired by the Science of Complex Adaptive Systems (Shawn Bell - 7 October, 2016)
This paper presents NetWorks (NW), an interactive music generation system that uses a hierarchically clustered scale free network to generate music that ranges from orderly to chaotic. At the 'edge of chaos', NW generates patterns that exhibit emergent complexity through coherent development at low, mid, and high levels of musical organization, and often suggests goal seeking behaviour
Link: https://arxiv.org/abs/1610.02475
====================================================
On the Modeling of Musical Solos as Complex Networks (Stefano Ferretti - 3 October, 2016)
Then, we provide an analysis on a set of guitar solos performed by main musicians. Results of this study indicate that the presented model can have an impact on audio and multimedia applications such as music classification, identification, e-learning, automatic music generation, multimedia entertainment.
Link: https://arxiv.org/abs/1610.00468
====================================================
A Consumer BCI for Automated Music Evaluation Within a Popular On-Demand Music Streaming Service - Taking Listener's Brainwaves to Extremes (Fotis Kalaganis - 30 September, 2016)
We investigated the possibility of using a machine-learning scheme in conjunction with commercial wearable EEG-devices for translating listener's subjective experience of music into scores that can be used for the automated annotation of music in popular on-demand streaming services. Our research operated in two distinct stages: i) a generic feature engineering stage, in which features from signal-analytics were ranked and selected based on their ability to associate music induced perturbations in brainwaves with listener's appraisal of music
Link: https://arxiv.org/abs/1609.06374
====================================================
Interference Reduction in Music Recordings Combining Kernel Additive Modelling and Non-Negative Matrix Factorization (Delia Fano Yela - 8 February, 2017)
Further, we introduce a temporal context in the kernel, taking some musical structure into account. Our experiments show improved separation quality for our proposed method over a state-of-the-art approach for interference reduction.
Link: https://arxiv.org/abs/1609.06210
====================================================
Style Imitation and Chord Invention in Polyphonic Music with Exponential Families (GaÃ«tan Hadjeres - 16 September, 2016)
We propose a statistical model of polyphonic music, based on the maximum entropy principle. We discuss how the model enables the user to specify and enforce user-defined constraints, which makes it useful for style-based, interactive music generation.
Link: https://arxiv.org/abs/1609.05152
====================================================
Structured Dropout for Weak Label and Multi-Instance Learning and Its Application to Score-Informed Source Separation (Sebastian Ewert - 26 December, 2016)
We demonstrate the capabilities of our approach in the context of score-informed source separation of music.
Link: https://arxiv.org/abs/1609.04557
====================================================
Convolutional Recurrent Neural Networks for Music Classification (Keunwoo Choi - 21 December, 2016)
We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.
Link: https://arxiv.org/abs/1609.04243
====================================================
WaveNet: A Generative Model for Raw Audio (Aaron van den Oord - 19 September, 2016)
When trained to model music, we find that it generates novel and often highly realistic musical fragments
Link: https://arxiv.org/abs/1609.03499
====================================================
Towards Music Captioning: Generating Music Playlist Descriptions (Keunwoo Choi - 15 January, 2017)
Recommending automatically generated music playlists (e.g. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning
Link: https://arxiv.org/abs/1608.04868
====================================================
LÃ©vy NMF for robust nonnegative source separation (Paul Magron - 8 November, 2016)
Source separation, which consists in decomposing data into meaningful structured components, is an active research topic in many areas, such as music and image signal processing, applied physics and text mining. The analysis of two types of realistic signals is also considered: musical spectrograms and fluorescence spectra of chemical species
Link: https://arxiv.org/abs/1608.01844
====================================================
Meta-Prod2Vec - Product Embeddings Using Side-Information for Recommendation (Flavian Vasile - 25 July, 2016)
We show that the new item representa- tions lead to better performance on recommendation tasks on an open music dataset.
Link: https://arxiv.org/abs/1607.07326
====================================================
Inpainting of long audio segments with similarity graphs (Nathanael Perraudin - 23 February, 2018)
We present a novel method for the compensation of long duration data loss in audio signals, in particular music. Extensive listening tests show that the proposed algorithm provides highly promising results when applied to a variety of real-world music signals.
Link: https://arxiv.org/abs/1607.06667
====================================================
Computational Complexity of Arranging Music (William S. Moses - 14 July, 2016)
This paper proves that arrangement of music is NP-hard when subject to various constraints: avoiding musical dissonance, limiting how many notes can be played simultaneously, and limiting transition speed between chords. These results imply the computational complexity of related musical problems, including musical choreography and rhythm games.
Link: https://arxiv.org/abs/1607.04220
====================================================
Minimum-latency Time-frequency Analysis Using Asymmetric Window Functions (Li Su - 29 June, 2016)
Finally, the music onset detection problem is studied to show the strength of the proposed algorithm.
Link: https://arxiv.org/abs/1606.09047
====================================================
Polymetric Rhythmic Feel for a Cognitive Drum Computer (Oliver Weede - 21 June, 2016)
This paper addresses a question about music cognition: how do we derive polymetric structures. By analyzing the micro-timing of West African percussion music a timing pattern consisting of six pulses was discovered
Link: https://arxiv.org/abs/1606.06197
====================================================
Deep Learning for Music (Allen Huang - 15 June, 2016)
Previous work in music generation has mainly been focused on creating a single melody. More recent work on polyphonic music modeling, centered around time series probability density estimation, has met some partial success
Link: https://arxiv.org/abs/1606.04930
====================================================
Efficient data streaming multiway aggregation through concurrent algorithmic designs and new abstract data types (Vincenzo Gulisano - 15 June, 2016)
The paper includes an extensive experimental study, based on a variety of aggregation continuous queries on two large datasets extracted from SoundCloud, a music social network, and from a Smart Grid network
Link: https://arxiv.org/abs/1606.04746
====================================================
Symbolic Music Data Version 1.0 (Christian Walder - 8 June, 2016)
In this document, we introduce a new dataset designed for training machine learning models of symbolic music data
Link: https://arxiv.org/abs/1606.02542
====================================================
Towards Playlist Generation Algorithms Using RNNs Trained on Within-Track Transitions (Keunwoo Choi - 7 June, 2016)
We introduce modelling sequences of high-level music descriptors using RNNs and discuss an experiment involving different similarity functions, where the sequences are provided by a musical structural analysis algorithm. Qualitative observations show that the proposed approach can effectively model transitions of music tracks in playlists.
Link: https://arxiv.org/abs/1606.02096
====================================================
Piano Transcription in the Studio Using an Extensible Alternating Directions Framework (Sebastian Ewert - 27 July, 2016)
Given a musical audio recording, the goal of automatic music transcription is to determine a score-like representation of the piece underlying the recording
Link: https://arxiv.org/abs/1606.00785
====================================================
Automatic tagging using deep convolutional neural networks (Keunwoo Choi - 1 June, 2016)
We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input
Link: https://arxiv.org/abs/1606.00298
====================================================
Nonnegative tensor factorization with frequency modulation cues for blind audio source separation (Elliot Creager - 31 May, 2016)
We present Vibrato Nonnegative Tensor Factorization, an algorithm for single-channel unsupervised audio source separation with an application to separating instrumental or vocal sources with nonstationary pitch from music recordings. This permits the modeling and unsupervised separation of vibrato or glissando musical sources, which is not possible with the basic matrix factorization formulation.
Link: https://arxiv.org/abs/1606.00037
====================================================
Mixtape Application: Music Map Methodology and Evaluation (Pedro H. F. Holanda - 30 May, 2016)
This report discusses dimensionality reduction techniques used to create a music map - a map where the distances between songs represent their similarity and that can be used to recommend songs
Link: https://arxiv.org/abs/1605.08494
====================================================
madmom: a new Python Audio and Music Signal Processing Library (Sebastian BÃ¶ck - 23 May, 2016)
Apart from low-level audio processing, madmom puts emphasis on musically meaningful high-level features. Additionally, madmom comes with several state-of-the-art MIR algorithms for onset detection, beat, downbeat and meter tracking, tempo estimation, and piano transcription
Link: https://arxiv.org/abs/1605.07008
====================================================
Deep convolutional networks on the pitch spiral for musical instrument recognition (Vincent Lostanlen - 10 January, 2017)
Audio-based classification of musical instruments thus requires to build signal representations that are invariant to such transformations. We provide an acoustical interpretation of these strategies within the source-filter framework of quasi-harmonic sounds with a fixed spectral envelope, which are archetypal of musical notes
Link: https://arxiv.org/abs/1605.06644
====================================================
Subgraph Isomorphism in Temporal Networks (Ursula Redmond - 7 May, 2016)
Examples include the dissemination of information through a social network, the propagation of musical ideas in a music sampling network, and the spread of a disease via contacts between infected and susceptible individuals
Link: https://arxiv.org/abs/1605.02174
====================================================
Diagonal Unloading Beamforming for Source Localization (Daniele Salvati - 3 May, 2016)
We show how to calculate precisely the unloading parameter, and we present an eigenvalue analysis for comparing the proposed DU beamforming, the minimum variance distortionless response (MVDR) filter and the multiple signal classification (MUSIC) method. Theoretical analysis and experiments with acoustic sources demonstrate that the DU beamformer localization performance is comparable to that of MVDR and MUSIC
Link: https://arxiv.org/abs/1605.00810
====================================================
Text-based LSTM networks for Automatic Music Composition (Keunwoo Choi - 18 April, 2016)
In this paper, we introduce new methods and discuss results of text-based LSTM (Long Short-Term Memory) networks for automatic music composition. The proposed system can be used for fully automatic composition or as semi-automatic systems that help humans to compose music by controlling a diversity parameter of the model.
Link: https://arxiv.org/abs/1604.05358
====================================================
